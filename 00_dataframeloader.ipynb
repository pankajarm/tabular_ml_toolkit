{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataframeloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Loader\n",
    "\n",
    "> An API to create training, validation and test dataset for Machine Learning models based on tabluarl or strucuture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "#settings for modin\n",
    "import ray\n",
    "ray.init()\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tabular_ml_toolkit.logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class DataFrameLoader:\n",
    "    \"\"\"\n",
    "    Represent DataFrameLoader class\n",
    "    \n",
    "    Attributes:\n",
    "    X_full: full dataframe load from raw input\n",
    "    X_test: full test dataframe load from raw input\n",
    "    X: features\n",
    "    y: target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "        self.shape_X_full = None\n",
    "        self.X_full = None\n",
    "        self.X_test = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_valid = None\n",
    "        self.y_train = None\n",
    "        self.y_valid = None\n",
    "        self.use_num_cols = None\n",
    "        self.use_cat_cols = None\n",
    "        self.categorical_cols = None\n",
    "        self.numerical_cols = None\n",
    "        self.low_card_cat_cols = None\n",
    "        self.high_card_cat_cols = None\n",
    "        self.final_cols = None\n",
    "        self.target = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Returns human readable string reprsentation\"\"\"\n",
    "        return \"DataFrameLoader object with attributes: X_full, X_test, X(features), y(target), X_train, X_valid, y_train and y_valid\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    # utility method\n",
    "    # Idea taken from https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65/comments\n",
    "    # Author ArjenGroen https://www.kaggle.com/arjanso\n",
    "    def reduce_num_dtype_mem_usage(self, df, verbose=True):\n",
    "        start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in self.numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == \"int\":\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                \"DataFrame Memory usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                    end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "                )\n",
    "            )\n",
    "        return df\n",
    "    \n",
    "    # CORE METHODS\n",
    "    # load data from csv\n",
    "    def read_csv(self,train_file_path:str,test_file_path:str, idx_col:str, nrows:int):\n",
    "        # Read the csv files using pandas\n",
    "        if train_file_path is not None:\n",
    "            self.X_full = pd.read_csv(train_file_path, index_col=idx_col, nrows=nrows)\n",
    "            self.shape_X_full = self.X_full.shape\n",
    "            self.X_full = self.reduce_num_dtype_mem_usage(self.X_full, verbose=True)\n",
    "        else:\n",
    "            logger.info(f\"Not valid train_file_path, input provided: {train_file_path}\")\n",
    "        if test_file_path is not None:\n",
    "            self.X_test = pd.read_csv(test_file_path, index_col=idx_col, nrows=nrows)\n",
    "            self.shape_X_test = self.X_test.shape\n",
    "            self.X_test = self.reduce_num_dtype_mem_usage(self.X_test, verbose=True)\n",
    "            \n",
    "        else:\n",
    "            logger.info(f\"No test_file_path given, so training will continue without it!\")\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # prepare X and y\n",
    "    def prepare_X_y(self,input_df:object, target:str):\n",
    "        # Remove rows with missing target\n",
    "        self.X = input_df.dropna(axis=0, subset=[target])\n",
    "        # separate target from predictors\n",
    "        self.y = self.X[target].values\n",
    "        self.target = target\n",
    "        # drop target\n",
    "        self.X = input_df.drop([target], axis=1)\n",
    "        return self\n",
    "    \n",
    "    # select categorical columns\n",
    "    def select_categorical_cols(self):\n",
    "        # for low cardinality columns\n",
    "        self.low_card_cat_cols = [cname for cname in self.X.columns if\n",
    "                    self.X[cname].nunique() < 10 and \n",
    "                    self.X[cname].dtype == \"object\"]\n",
    "        # for high cardinality columns\n",
    "        self.high_card_cat_cols = [cname for cname in self.X.columns if\n",
    "                    self.X[cname].nunique() > 10 and \n",
    "                    self.X[cname].dtype == \"object\"]    \n",
    "        # for all categorical columns\n",
    "        self.categorical_cols = self.low_card_cat_cols + self.high_card_cat_cols\n",
    "    \n",
    "    # select numerical columns\n",
    "    def select_numerical_cols(self):\n",
    "        self.numerical_cols = [cname for cname in self.X.columns if \n",
    "                self.X[cname].dtype in self.numerics]\n",
    "    \n",
    "    # prepare final columns by data type\n",
    "    def prepare_final_cols(self, use_num_cols:bool, use_cat_cols:bool):\n",
    "        self.use_num_cols = use_num_cols\n",
    "        self.use_cat_cols = use_cat_cols\n",
    "        \n",
    "        if self.use_num_cols:\n",
    "            self.select_categorical_cols()\n",
    "        if self.use_cat_cols:\n",
    "            self.select_numerical_cols()\n",
    "        \n",
    "        if (self.numerical_cols is not None) and (self.categorical_cols is not None):\n",
    "            self.final_cols = self.numerical_cols + self.categorical_cols\n",
    "\n",
    "        elif (self.numerical_cols is not None) and (self.categorical_cols is None):\n",
    "            self.final_cols = self.numerical_cols\n",
    "\n",
    "        elif (self.numerical_cols is None) and (self.categorical_cols is not None):\n",
    "            self.final_cols = self.categorical_cols\n",
    "    \n",
    "    \n",
    "    # prepare X_train, X_valid from selected columns\n",
    "    def update_X_train_X_valid_X_test_with_final_cols(self, final_cols:object):\n",
    "        self.X_train = self.X_train[final_cols]\n",
    "        self.X_valid = self.X_valid[final_cols]\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.X_test[final_cols]\n",
    "\n",
    "        \n",
    "    def update_X_y_with_final_cols(self,final_cols:object):\n",
    "        self.X = self.X[final_cols]\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.X_test[final_cols]\n",
    "        \n",
    "    # split X and y into X_train, y_train, X_valid & y_valid dataframes    \n",
    "    def create_train_valid(self, valid_size:float, X=None, y=None, random_state=42):\n",
    "        \n",
    "        if X and y:\n",
    "            self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(\n",
    "                X, y, train_size=(1-valid_size), test_size=valid_size, random_state=random_state)\n",
    "            \n",
    "        else:\n",
    "            self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(\n",
    "                self.X, self.y, train_size=(1-valid_size), test_size=valid_size, random_state=random_state)\n",
    "        \n",
    "        self.update_X_train_X_valid_X_test_with_final_cols(self.final_cols)\n",
    "    \n",
    "    # get train and valid dataframe\n",
    "    def from_csv(self, train_file_path:str,\n",
    "                 idx_col:str, target:str,\n",
    "                 nrows:int=None,\n",
    "                 test_file_path:str=None,\n",
    "                 use_num_cols:bool=True,\n",
    "                 use_cat_cols:bool=True,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # read csv and load dataframes using pandas\n",
    "        self.read_csv(train_file_path,test_file_path, idx_col, nrows)\n",
    "        if self.X_full is not None:\n",
    "            self.prepare_X_y(self.X_full, target)\n",
    "        # create final columns based upon type of columns\n",
    "        self.prepare_final_cols(use_num_cols=use_num_cols, use_cat_cols=use_cat_cols)\n",
    "        if self.final_cols is not None:\n",
    "            self.update_X_y_with_final_cols(self.final_cols)\n",
    "        \n",
    "        # clean up unused dataframes\n",
    "        unused_df_lst = [self.X_full]\n",
    "        del unused_df_lst\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DataFrameLoader.from_csv\" class=\"doc_header\"><code>DataFrameLoader.from_csv</code><a href=\"__main__.py#L173\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DataFrameLoader.from_csv</code>(**`train_file_path`**:`str`, **`idx_col`**:`str`, **`target`**:`str`, **`nrows`**:`int`=*`None`*, **`test_file_path`**:`str`=*`None`*, **`use_num_cols`**:`bool`=*`True`*, **`use_cat_cols`**:`bool`=*`True`*, **`random_state`**=*`42`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DataFrameLoader.from_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's load [Melbourne Home Sale price raw data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n",
      "UserWarning: Distributing <class 'int'> object. This may take some time.\n",
      "2021-11-22 17:27:58,683 INFO DataFrame Memory usage decreased to 0.58 Mb (35.5% reduction)\n",
      "2021-11-22 17:28:03,337 INFO DataFrame Memory usage decreased to 0.58 Mb (34.8% reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(compute_sliced_len pid=80042)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrameLoader object with attributes: X_full, X_test, X(features), y(target), X_train, X_valid, y_train and y_valid"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl = DataFrameLoader().from_csv(\n",
    "    train_file_path=\"input/home_data/train.csv\",\n",
    "    test_file_path=\"input/home_data/test.csv\",\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\")\n",
    "dfl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for X_full and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homedata X shape: (1460, 79)\n",
      "homedata y shape (1460,)\n",
      "homedata X_test shape (1459, 79)\n"
     ]
    }
   ],
   "source": [
    "# show shape of X and y \n",
    "print(\"homedata X shape:\", dfl.X.shape)\n",
    "print(\"homedata y shape\", dfl.y.shape)\n",
    "print(\"homedata X_test shape\", dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prepare_X_y()\n",
    "assert dfl.X.shape == (1460,79)\n",
    "assert dfl.y.shape == (1460,)\n",
    "assert dfl.X_test.shape == (1459,79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create train, valid split from X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.create_train_valid(valid_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homedata X_train shape: (1168, 79)\n",
      "homedata y_train shape (1168,)\n",
      "homedata X_valid shape: (292, 79)\n",
      "homedata y_valid shape (292,)\n",
      "homedata X_test shape (1459, 79)\n"
     ]
    }
   ],
   "source": [
    "# show shape of X_train, X_valid, y_train and y_valid\n",
    "print(\"homedata X_train shape:\", dfl.X_train.shape)\n",
    "print(\"homedata y_train shape\", dfl.y_train.shape)\n",
    "print(\"homedata X_valid shape:\", dfl.X_valid.shape)\n",
    "print(\"homedata y_valid shape\", dfl.y_valid.shape)\n",
    "print(\"homedata X_test shape\", dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for X_train, y_train, X_valid and y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfl.X_train.shape == (1168,79)\n",
    "assert dfl.y_train.shape == (1168,)\n",
    "assert dfl.X_valid.shape == (292,79)\n",
    "assert dfl.y_valid.shape == (292,)\n",
    "assert dfl.X_test.shape == (1459,79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataframeloader.ipynb.\n",
      "Converted 01_preprocessor.ipynb.\n",
      "Converted 02_tmlt.ipynb.\n",
      "Converted 04_optuna_objective.ipynb.\n",
      "Converted 10_Tutorial.ipynb.\n",
      "Converted 13_Kaggle_TPS_Tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted logger.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbdev_env]",
   "language": "python",
   "name": "conda-env-nbdev_env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
