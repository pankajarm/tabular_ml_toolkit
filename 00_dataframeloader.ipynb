{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataframeloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Loader\n",
    "\n",
    "> An API to create training, validation and test dataset for Machine Learning models based on tabluarl or strucuture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tabular_ml_toolkit.logger import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "# make sure to pip install modin[ray]>=0.11.3\n",
    "# #settings for modin\n",
    "# import ray\n",
    "# ray.init()\n",
    "# import os\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "# import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class DataFrameLoader:\n",
    "    \"\"\"\n",
    "    Represent DataFrameLoader class\n",
    "    \n",
    "    Attributes:\n",
    "    X_test: test dataframe\n",
    "    X: features dataframe\n",
    "    y: target series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "        self.shape_X_full = None\n",
    "        self.X_full = None\n",
    "        self.X_test = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_valid = None\n",
    "        self.y_train = None\n",
    "        self.y_valid = None\n",
    "        self.use_num_cols = None\n",
    "        self.use_cat_cols = None\n",
    "        self.categorical_cols = None\n",
    "        self.numerical_cols = None\n",
    "        self.low_card_cat_cols = None\n",
    "        self.high_card_cat_cols = None\n",
    "        self.final_cols = None\n",
    "        self.target = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Returns human readable string reprsentation\"\"\"\n",
    "        return \"DataFrameLoader object with attributes: X_full, X_test, X(features), y(target), X_train, X_valid, y_train and y_valid\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    # utility method\n",
    "    # Idea taken from https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65/comments\n",
    "    # Author ArjenGroen https://www.kaggle.com/arjanso\n",
    "    def reduce_num_dtype_mem_usage(self, df, verbose=True):\n",
    "        start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in self.numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == \"int\":\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                \"DataFrame Memory usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                    end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "                )\n",
    "            )\n",
    "        return df\n",
    "    \n",
    "    # CORE METHODS\n",
    "    # load data from csv\n",
    "    def read_csv(self,train_file_path:str,test_file_path:str, idx_col:str, nrows:int):\n",
    "        # Read the csv files using pandas\n",
    "        if train_file_path is not None:\n",
    "            self.X_full = pd.read_csv(train_file_path, index_col=idx_col, nrows=nrows)\n",
    "            self.shape_X_full = self.X_full.shape\n",
    "            self.X_full = self.reduce_num_dtype_mem_usage(self.X_full, verbose=True)\n",
    "        else:\n",
    "            logger.warn(f\"No valid train_file_path provided: {train_file_path}\")\n",
    "        \n",
    "        if test_file_path is not None:\n",
    "            self.X_test = pd.read_csv(test_file_path, index_col=idx_col, nrows=nrows)\n",
    "            self.shape_X_test = self.X_test.shape\n",
    "            self.X_test = self.reduce_num_dtype_mem_usage(self.X_test, verbose=True)\n",
    "            \n",
    "        else:\n",
    "            logger.info(f\"No test_file_path given, so training will continue without it!\")\n",
    "        return self\n",
    "    \n",
    "    # fixes least class having only 1 value hence breaking logic for train, val split\n",
    "    def fix_least_class(self, X, target):\n",
    "        y = X[target].values\n",
    "        # now check for value count and fix least class value\n",
    "        val_cnt = X[target].value_counts().to_frame()\n",
    "        least_class_label = val_cnt.index[-1:].to_list()[0]\n",
    "        least_class_val = val_cnt.values[-1:][0][0]\n",
    "        # TODO see if you want to change 2 to other number and then use for loop to add it\n",
    "        if least_class_val < 2:\n",
    "            logger.info(f\"The least class label is :{least_class_label} and value count is: {least_class_val}\")\n",
    "            #  just copying more least value\n",
    "            lowest_val_cnt_row = X[X[target] == least_class_label]\n",
    "            # duplicate lowest value count class bu +12 because of able to do alteast 10 k-fold\n",
    "            #start_time = time.time()\n",
    "            lowest_val_cnt_df = pd.concat([lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row,\n",
    "                                           lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row,\n",
    "                                          lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row,\n",
    "                                          lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row,\n",
    "                                          lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row,\n",
    "                                          lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row,\n",
    "                                          lowest_val_cnt_row, lowest_val_cnt_row, lowest_val_cnt_row],\n",
    "                                          axis=0, ignore_index=True)\n",
    "            #end_time = time.time()\n",
    "            #logger.info(f\"The time took to concat 12 rows: {end_time - start_time}\")\n",
    "            #del [start_time, end_time]\n",
    "            # now just copy paste lowest_val_cnt_row\n",
    "            # can use for loop here to add multiple times same row but performance will impact\n",
    "            logger.info(f\"The Original X shape is: {X.shape}\")\n",
    "            #start_time = time.time()\n",
    "            #X = X.append(lowest_val_cnt_df, ignore_index = True)\n",
    "            X = pd.concat([X, lowest_val_cnt_df], axis=0, ignore_index=True)\n",
    "            #end_time = time.time()\n",
    "            #logger.info(f\"The time took to append 1 dataframe to existing one!: {end_time - start_time}\")\n",
    "            #del [start_time, end_time]\n",
    "            logger.info(f\"The X shape after least class duplicates appends is: {X.shape}\")\n",
    "            y = X[target].values\n",
    "        #trigger gc to clearn old X df from memory\n",
    "        gc.collect()\n",
    "        return y, X\n",
    "    \n",
    "    \n",
    "    # prepare X and y\n",
    "    def prepare_X_y(self,input_df:object, target:str, problem_type:str):\n",
    "        # Remove rows with missing target\n",
    "        self.X = input_df.dropna(axis=0, subset=[target])\n",
    "        # set target in dfl\n",
    "        self.target = target\n",
    "        # separate target from predictors\n",
    "        #TODO: check value_counts() to fix least class having only 1 value\n",
    "        # TODO: breaking logic for train, val split\n",
    "        if \"classification\" in problem_type:\n",
    "            self.y, self.X = self.fix_least_class(self.X, target)\n",
    "        else:\n",
    "            self.y = self.X[target].values\n",
    "        # drop target\n",
    "        self.X = self.X.drop([target], axis=1)\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    # select categorical columns\n",
    "    def select_categorical_cols(self, X:object):\n",
    "        # for low cardinality columns\n",
    "        self.low_card_cat_cols = [cname for cname in X.columns if\n",
    "                    X[cname].nunique() < 10 and \n",
    "                    X[cname].dtype == \"object\"]\n",
    "        # for high cardinality columns\n",
    "        self.high_card_cat_cols = [cname for cname in X.columns if\n",
    "                    X[cname].nunique() > 10 and \n",
    "                    X[cname].dtype == \"object\"]    \n",
    "        # for all categorical columns\n",
    "        self.categorical_cols = self.low_card_cat_cols + self.high_card_cat_cols\n",
    "    \n",
    "    # select numerical columns\n",
    "    def select_numerical_cols(self, X:object):\n",
    "        self.numerical_cols = [cname for cname in X.columns if \n",
    "                X[cname].dtype in self.numerics]\n",
    "    \n",
    "    # prepare final columns by data type\n",
    "    def prepare_final_cols(self, X):   \n",
    "        self.select_categorical_cols(X)\n",
    "        self.select_numerical_cols(X)\n",
    "        \n",
    "        if (self.numerical_cols is not None) and (self.categorical_cols is not None):\n",
    "            self.final_cols = self.numerical_cols + self.categorical_cols\n",
    "\n",
    "        elif (self.numerical_cols is not None) and (self.categorical_cols is None):\n",
    "            self.final_cols = self.numerical_cols\n",
    "\n",
    "        elif (self.numerical_cols is None) and (self.categorical_cols is not None):\n",
    "            self.final_cols = self.categorical_cols\n",
    "    \n",
    "    \n",
    "    # prepare X_train, X_valid from selected columns\n",
    "    def update_X_train_X_valid_X_test_with_final_cols(self, final_cols:object):\n",
    "        self.X_train = self.X_train[final_cols]\n",
    "        self.X_valid = self.X_valid[final_cols]\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.X_test[final_cols]\n",
    "\n",
    "        \n",
    "    def update_X_y_with_final_cols(self,final_cols:object):\n",
    "        self.X = self.X[final_cols]\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.X_test[final_cols]\n",
    "        \n",
    "    # split X and y into X_train, y_train, X_valid & y_valid dataframes    \n",
    "    def create_train_valid(self, X:object, y:object, valid_size:float, random_state=42):\n",
    "        \n",
    "        self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(\n",
    "            X, y, train_size=(1-valid_size), test_size=valid_size, random_state=random_state)\n",
    "        \n",
    "        #self.update_X_train_X_valid_X_test_with_final_cols(self.final_cols)\n",
    "        \n",
    "        return self.X_train, self.X_valid, self.y_train, self.y_valid\n",
    "    \n",
    "#     # split X and y into X_train, y_train, X_valid & y_valid dataframes    \n",
    "#     def create_train_valid(self, valid_size:float, X=None, y=None, random_state=42):\n",
    "        \n",
    "#         if X and y:\n",
    "#             self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(\n",
    "#                 X, y, train_size=(1-valid_size), test_size=valid_size, random_state=random_state)\n",
    "            \n",
    "#         else:\n",
    "#             self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(\n",
    "#                 self.X, self.y, train_size=(1-valid_size), test_size=valid_size, random_state=random_state)\n",
    "        \n",
    "#         self.update_X_train_X_valid_X_test_with_final_cols(self.final_cols)\n",
    "        \n",
    "#         return self.X_train, self.X_valid, self.y_train, self.y_valid\n",
    "    \n",
    "    # get train and valid dataframe\n",
    "    def from_csv(self, train_file_path:str,\n",
    "                 idx_col:str, target:str,\n",
    "                 problem_type:str,\n",
    "                 nrows:int=None,\n",
    "                 test_file_path:str=None,\n",
    "                 use_num_cols:bool=True,\n",
    "                 use_cat_cols:bool=True,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # read csv and load dataframes using pandas\n",
    "        self.read_csv(train_file_path,test_file_path, idx_col, nrows)\n",
    "        if self.X_full is not None:\n",
    "            self.prepare_X_y(self.X_full, target, problem_type)\n",
    "        # create final columns based upon type of columns\n",
    "        self.prepare_final_cols(self.X)\n",
    "        if self.final_cols is not None:\n",
    "            self.update_X_y_with_final_cols(self.final_cols)\n",
    "        \n",
    "        # clean up unused dataframes\n",
    "        del [self.X_full]\n",
    "        gc.collect()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "        # get train and valid dataframe\n",
    "    def regenerate_dfl(self, X:object, y:object, X_test:object, random_state=42):\n",
    "        # assign X,y and X_test\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_test = X_test\n",
    "        \n",
    "        # create final columns based upon dtype of columns\n",
    "        self.prepare_final_cols(X)\n",
    "        \n",
    "        if self.final_cols is not None:\n",
    "            self.update_X_y_with_final_cols(self.final_cols)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DataFrameLoader.from_csv\" class=\"doc_header\"><code>DataFrameLoader.from_csv</code><a href=\"__main__.py#L227\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DataFrameLoader.from_csv</code>(**`train_file_path`**:`str`, **`idx_col`**:`str`, **`target`**:`str`, **`problem_type`**:`str`, **`nrows`**:`int`=*`None`*, **`test_file_path`**:`str`=*`None`*, **`use_num_cols`**:`bool`=*`True`*, **`use_cat_cols`**:`bool`=*`True`*, **`random_state`**=*`42`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DataFrameLoader.from_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's load [Melbourne Home Sale price raw data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-25 21:54:06,909 INFO DataFrame Memory usage decreased to 0.58 Mb (35.5% reduction)\n",
      "2021-12-25 21:54:06,939 INFO DataFrame Memory usage decreased to 0.58 Mb (34.8% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrameLoader object with attributes: X_full, X_test, X(features), y(target), X_train, X_valid, y_train and y_valid"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl = DataFrameLoader().from_csv(\n",
    "    train_file_path=\"input/home_data/train.csv\",\n",
    "    test_file_path=\"input/home_data/test.csv\",\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    problem_type=\"regression\")\n",
    "dfl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for X_full and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homedata X shape: (1460, 79)\n",
      "homedata y shape (1460,)\n",
      "homedata X_test shape (1459, 79)\n"
     ]
    }
   ],
   "source": [
    "# show shape of X and y \n",
    "print(\"homedata X shape:\", dfl.X.shape)\n",
    "print(\"homedata y shape\", dfl.y.shape)\n",
    "print(\"homedata X_test shape\", dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prepare_X_y()\n",
    "assert dfl.X.shape == (1460,79)\n",
    "assert dfl.y.shape == (1460,)\n",
    "assert dfl.X_test.shape == (1459,79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create train, valid split from X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = dfl.create_train_valid(dfl.X, dfl.y, valid_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homedata X_train shape: (1168, 79)\n",
      "homedata y_train shape (1168,)\n",
      "homedata X_valid shape: (292, 79)\n",
      "homedata y_valid shape (292,)\n",
      "homedata X_test shape (1459, 79)\n"
     ]
    }
   ],
   "source": [
    "# show shape of X_train, X_valid, y_train and y_valid\n",
    "print(\"homedata X_train shape:\", dfl.X_train.shape)\n",
    "print(\"homedata y_train shape\", dfl.y_train.shape)\n",
    "print(\"homedata X_valid shape:\", dfl.X_valid.shape)\n",
    "print(\"homedata y_valid shape\", dfl.y_valid.shape)\n",
    "print(\"homedata X_test shape\", dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for X_train, y_train, X_valid and y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfl.X_train.shape == (1168,79)\n",
    "assert dfl.y_train.shape == (1168,)\n",
    "assert dfl.X_valid.shape == (292,79)\n",
    "assert dfl.y_valid.shape == (292,)\n",
    "assert dfl.X_test.shape == (1459,79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataframeloader.ipynb.\n",
      "Converted 01_preprocessor.ipynb.\n",
      "Converted 02_tmlt.ipynb.\n",
      "Converted 04_xgb_optuna_objective.ipynb.\n",
      "Converted Kaggle_TPS_Dec_Meta_Clean.ipynb.\n",
      "Converted Kaggle_TPS_Dec_TabNet.ipynb.\n",
      "Converted Kaggle_TPS_Dec_Tutorial-Meta(Tabnet+XGB).ipynb.\n",
      "Converted Kaggle_TPS_Dec_Tutorial-Meta.ipynb.\n",
      "Converted Kaggle_TPS_Dec_Tutorial_XGB.ipynb.\n",
      "Converted Kaggle_TPS_Nov_Tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted logger.ipynb.\n",
      "Converted utility.ipynb.\n",
      "Converted xgb_tabular_ml_toolkit.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03d77e6394b78866746a93e431cb2242fb65951a366e3e86e053dc831f6c9ae0"
  },
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
