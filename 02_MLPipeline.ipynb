{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp MLPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "> An API to create training pipeline for machine learning models on tabular or strucuture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tabular_ml_toolkit.DataFrameLoader import *\n",
    "from tabular_ml_toolkit.PreProcessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"\n",
    "    Represent MLPipeline class\n",
    "    \n",
    "    Attributes:\\n\n",
    "    pipeline: An MLPipeline instance \\n\n",
    "    dataframeloader: A DataFrameLoader instance \\n\n",
    "    preprocessor: A PreProcessor Instance \\n\n",
    "    model: The given Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pipeline = None\n",
    "        self.dataframeloader = None\n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        self.scikit_pipeline = None\n",
    "        self.transformer_type = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Returns human readable string reprsentation\"\"\"\n",
    "        attr_str = (\"pipeline, dataframeloader, preprocessor, model\")\n",
    "        return (\"Training Pipeline object with attributes:\"+attr_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    \n",
    "    # core methods\n",
    "    \n",
    "    # Bundle preprocessing and modeling code in a training pipeline\n",
    "    def bundle_preproessor_model(self, transformer_type, model):\n",
    "        self.scikit_pipeline = Pipeline(\n",
    "            steps=[('preprocessor', transformer_type),\n",
    "                   ('model', model)])\n",
    "    \n",
    "    # Core methods for Simple Training\n",
    "    def prepare_data_for_training(self, train_file_path:str,\n",
    "                                  test_file_path:str,\n",
    "                                  idx_col:str, target:str,\n",
    "                                  random_state:int,\n",
    "                                  model:object,\n",
    "                                  valid_size:float=None):\n",
    "        self.model = model\n",
    "        \n",
    "        # call DataFrameLoader module\n",
    "        self.dataframeloader = DataFrameLoader().from_csv(\n",
    "            train_file_path=train_file_path,\n",
    "            test_file_path=test_file_path,\n",
    "            idx_col=idx_col,\n",
    "            target=target,\n",
    "            random_state=random_state,\n",
    "            valid_size=valid_size)\n",
    "        \n",
    "        # call PreProcessor module\n",
    "        self.preprocessor = PreProcessor().preprocess_all_cols(\n",
    "            dataframeloader=self.dataframeloader)\n",
    "        \n",
    "        # call bundle method\n",
    "        self.bundle_preproessor_model(transformer_type=self.preprocessor.transformer_type,\n",
    "                                     model = model)\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    \n",
    "    def do_cross_validation(self, cv:int, scoring:str):\n",
    "        scores = cross_val_score(\n",
    "            estimator=self.scikit_pipeline,\n",
    "            X=self.dataframeloader.X,\n",
    "            y=self.dataframeloader.y,\n",
    "            scoring=scoring,\n",
    "            cv=cv)\n",
    "        # Multiply by -1 since sklearn calculates *negative* scoring for some of the metrics\n",
    "        if \"neg_\" in scoring:\n",
    "            scores = -1 * scores\n",
    "        return scores\n",
    "        \n",
    "    # Core methods for GridSearch\n",
    "    def do_grid_search(self, param_grid:object, cv:int, scoring:str):\n",
    "        \n",
    "        # create GridSeachCV instance\n",
    "        grid_search = GridSearchCV(estimator=self.scikit_pipeline,\n",
    "                                   param_grid=param_grid,\n",
    "                                   cv=cv,\n",
    "                                   scoring=scoring)\n",
    "        # now call fit\n",
    "        grid_search.fit(self.dataframeloader.X, self.dataframeloader.y)\n",
    "        return grid_search\n",
    "\n",
    "    \n",
    "    # do k-fold training\n",
    "    def do_k_fold_training(self, n_splits:int, metrics:object):\n",
    "        \n",
    "        #create stratified K Folds instance\n",
    "        k_fold = StratifiedKFold(n_splits=n_splits,\n",
    "                             random_state=48,\n",
    "                             shuffle=True)\n",
    "        \n",
    "        # list contains metrics score for each fold\n",
    "        metrics_score = []\n",
    "        n=0\n",
    "        for train_idx, valid_idx in k_fold.split(self.dataframeloader.X_cv, self.dataframeloader.y):\n",
    "            # create X_train\n",
    "            self.dataframeloader.X_train = self.dataframeloader.X_cv.iloc[train_idx]\n",
    "            # create X_valid\n",
    "            self.dataframeloader.X_valid = self.dataframeloader.X_cv.iloc[valid_idx] \n",
    "            # create y_train\n",
    "            self.dataframeloader.y_train = self.dataframeloader.y.iloc[train_idx]\n",
    "            # create y_valid\n",
    "            self.dataframeloader.y_valid = self.dataframeloader.y.iloc[valid_idx]\n",
    "            \n",
    "            # fit\n",
    "            self.scikit_pipeline.fit(self.dataframeloader.X_train, self.dataframeloader.y_train)\n",
    "            \n",
    "            #evaluate metrics based upon input\n",
    "            if \"proba\" in metrics.__globals__:\n",
    "                metrics_score.append(metrics(self.dataframeloader.y_valid,\n",
    "                                               self.scikit_pipeline.predict_proba(self.dataframeloader.X_valid)[:,1]))\n",
    "            else:\n",
    "                metrics_score.append(metrics(self.dataframeloader.y_valid,\n",
    "                                               self.scikit_pipeline.predict(self.dataframeloader.X_valid)))\n",
    "            \n",
    "            print(f\"fold: {n+1} , {str(metrics.__name__)}: {metrics_score[n]}\")\n",
    "            # increment fold counter label\n",
    "            n += 1\n",
    "        return k_fold, metrics_score\n",
    "            \n",
    "    def do_k_fold_prediction(self, k_fold:object):\n",
    "        # create preds dataframe\n",
    "        preds = np.zeros(self.dataframeloader.X_test_cv.shape[0])\n",
    "        for _ in range(k_fold.n_splits):\n",
    "            # predict\n",
    "            preds += self.scikit_pipeline.predict(self.dataframeloader.X_test_cv) / k_fold.n_splits\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"MLPipeline\" class=\"doc_header\"><code>class</code> <code>MLPipeline</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>MLPipeline</code>()\n",
       "\n",
       "Represent MLPipeline class\n",
       "\n",
       "Attributes:\n",
       "\n",
       "pipeline: An MLPipeline instance \n",
       "\n",
       "dataframeloader: A DataFrameLoader instance \n",
       "\n",
       "preprocessor: A PreProcessor Instance \n",
       "\n",
       "model: The given Model"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MLPipeline.prepare_data_for_training\" class=\"doc_header\"><code>MLPipeline.prepare_data_for_training</code><a href=\"__main__.py#L40\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>MLPipeline.prepare_data_for_training</code>(**`train_file_path`**:`str`, **`test_file_path`**:`str`, **`idx_col`**:`str`, **`target`**:`str`, **`random_state`**:`int`, **`model`**:`object`, **`valid_size`**:`float`=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLPipeline.prepare_data_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MLPipeline Class with House Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You can use MLPipeline to train any model. Here we are using RandomForestRegressor from Scikit-Learn, on  [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_DataFrameLoader.ipynb.\n",
      "Converted 01_PreProcessor.ipynb.\n",
      "Converted 02_MLPipeline.ipynb.\n",
      "Converted 03_Tutorial.ipynb.\n",
      "Converted 04_Kaggle_TPS_Challenge_Nov_2021_XGB.ipynb.\n",
      "Converted 05_Kaggle_TPS_Challenge_Nov_2021_SCIKIT_LEARN.ipynb.\n",
      "Converted 06_Getting_Started_Kaggle_TPS_Challenge.ipynb.\n",
      "Converted 07_Kaggle_TPS_Tutorial.ipynb.\n",
      "Converted 08_Optuna_Tutorial.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted automl_in_sklearn_pipeline.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
