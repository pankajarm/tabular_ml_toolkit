{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp MLPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "> An API to create super fast training pipeline for machine learning models based on tabular or strucuture data\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter tuning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabular_ml_toolkit.DataFrameLoader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yd/85jdd5bx59v81j8xnbhpkb3h0000gq/T/ipykernel_7762/3767311702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabular_ml_toolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrameLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabular_ml_toolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreProcessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabular_ml_toolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabular_ml_toolkit.DataFrameLoader'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from tabular_ml_toolkit.DataFrameLoader import *\n",
    "from tabular_ml_toolkit.PreProcessor import *\n",
    "from tabular_ml_toolkit.Logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "\n",
    "# for displaying diagram of pipelines \n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "# for finding n_jobs in all sklearn estimators\n",
    "from sklearn.utils import all_estimators\n",
    "import inspect\n",
    "\n",
    "# Just to compare fit times\n",
    "import time\n",
    "\n",
    "# for os specific settings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"\n",
    "    Represent MLPipeline class\n",
    "    \n",
    "    Attributes:\\n\n",
    "    pipeline: An MLPipeline instance \\n\n",
    "    dfl: A DataFrameLoader instance \\n\n",
    "    pp: A PreProcessor Instance \\n\n",
    "    model: The given Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dfl = None\n",
    "        self.pp = None\n",
    "        self.model = None\n",
    "        self.spl = None\n",
    "        self.transformer_type = None\n",
    "        self.has_n_jobs = self.create_has_n_jobs()\n",
    "        self.IDEAL_CPU_CORES = self.find_ideal_cpu_cores()\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Returns human readable string reprsentation\"\"\"\n",
    "        attr_str = (\"spl, dfl, pp, model\")\n",
    "        return (\"Training Pipeline object with attributes:\"+attr_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    #helper method to find ideal cpu cores\n",
    "    def find_ideal_cpu_cores(self):\n",
    "        if os.cpu_count() > 2:\n",
    "            ideal_cpu_cores = os.cpu_count()-1\n",
    "            logger.info(f\"{os.cpu_count()} cores found, parallel processing is enabled!\")\n",
    "        else:\n",
    "            ideal_cpu_cores = None\n",
    "            logger.info(f\"{os.cpu_count()} cores found, parallel processing NOT enabled!\")\n",
    "        return ideal_cpu_cores\n",
    "    \n",
    "    #Helper method to find all sklearn estimators with support for parallelism aka n_jobs\n",
    "    def create_has_n_jobs(self):\n",
    "        self.has_n_jobs = ['XGBRegressor', 'XGBClassifier']\n",
    "        for est in all_estimators():\n",
    "            s = inspect.signature(est[1])\n",
    "            if 'n_jobs' in s.parameters:\n",
    "                self.has_n_jobs.append(est[0])\n",
    "        return self.has_n_jobs\n",
    "                \n",
    "    # core methods\n",
    "    \n",
    "    # Bundle preprocessing and modeling code in a training pipeline\n",
    "    def create_final_sklearn_pipeline(self, transformer_type, model):\n",
    "        self.spl = Pipeline(\n",
    "            steps=[('preprocessor', transformer_type),\n",
    "                   ('model', model)])\n",
    "        return self.spl\n",
    "    \n",
    "    # Core methods for Simple Training\n",
    "    def prepare_data_for_training(self, train_file_path:str,\n",
    "                                  test_file_path:str,\n",
    "                                  idx_col:str, target:str,\n",
    "                                  random_state:int,\n",
    "                                  model:object):\n",
    "        \n",
    "        # check if given model supports n_jobs aka cpu core based Parallelism\n",
    "        estimator_name = model.__class__.__name__\n",
    "        # print(estimator_name)\n",
    "        # print(self.has_n_jobs)\n",
    "        if estimator_name in self.has_n_jobs :\n",
    "            # In order to OS not to kill the job, leave one processor out \n",
    "            model.n_jobs = self.IDEAL_CPU_CORES\n",
    "            self.model = model\n",
    "        else:\n",
    "            print(f\"{estimator_name} doesn't support parallelism yet! Training will continue on a single thread.\")\n",
    "            self.model = model\n",
    "        \n",
    "        # call DataFrameLoader module\n",
    "        self.dfl = DataFrameLoader().from_csv(\n",
    "            train_file_path=train_file_path,\n",
    "            test_file_path=test_file_path,\n",
    "            idx_col=idx_col,\n",
    "            target=target,\n",
    "            random_state=random_state)\n",
    "        \n",
    "        # call PreProcessor module\n",
    "        self.pp = PreProcessor().preprocess_all_cols(dataframeloader=self.dfl)\n",
    "        \n",
    "        # call create final sklearn pipelien method\n",
    "        self.spl = self.create_final_sklearn_pipeline(transformer_type=self.pp.transformer_type,\n",
    "                                     model = model)\n",
    "        # return MLPipeline\n",
    "        return self\n",
    "    \n",
    "    # Force to update the preprocessor in pipeline\n",
    "    def update_preprocessor(self,\n",
    "                            num_cols__imputer=SimpleImputer(strategy='median'),\n",
    "                            num_cols__scaler=StandardScaler(),\n",
    "                            cat_cols__imputer=SimpleImputer(strategy='constant'),\n",
    "                            cat_cols__encoder=OneHotEncoder(handle_unknown='ignore')):\n",
    "        # change preprocessor\n",
    "        self.pp = PreProcessor().preprocess_all_cols(self.dfl,\n",
    "                                                     num_cols__imputer=num_cols__imputer,\n",
    "                                                     num_cols__scaler=num_cols__scaler,\n",
    "                                                     cat_cols__imputer=cat_cols__imputer,\n",
    "                                                     cat_cols__encoder=cat_cols__encoder)\n",
    "        # recall create final sklearn pipelien method\n",
    "        self.spl = self.create_final_sklearn_pipeline(transformer_type=self.pp.transformer_type,\n",
    "                                     model = self.model)\n",
    "        \n",
    "    \n",
    "    # Force to update the model in pipeline\n",
    "    def update_model(self, model:object):\n",
    "        #change model\n",
    "        self.model = model\n",
    "        # recall create final sklearn pipelien method\n",
    "        self.spl = self.create_final_sklearn_pipeline(transformer_type=self.pp.transformer_type,\n",
    "                                     model = self.model)\n",
    "    \n",
    "    # HELPER METHODS\n",
    "    # cross validation\n",
    "    def do_cross_validation(self, cv:int, scoring:str):\n",
    "        scores = cross_val_score(\n",
    "            estimator=self.spl,\n",
    "            X=self.dfl.X,\n",
    "            y=self.dfl.y,\n",
    "            scoring=scoring,\n",
    "            cv=cv)\n",
    "        # Multiply by -1 since sklearn calculates *negative* scoring for some of the metrics\n",
    "        if \"neg_\" in scoring:\n",
    "            scores = -1 * scores\n",
    "        return scores\n",
    "        \n",
    "    # Core methods for GridSearch\n",
    "    def do_grid_search(self, param_grid:object, cv:int,\n",
    "                       scoring:str, n_jobs=None):\n",
    "        \n",
    "        if n_jobs is None:\n",
    "            n_jobs = self.IDEAL_CPU_CORES\n",
    "        \n",
    "        # create GridSeachCV instance\n",
    "        grid_search = GridSearchCV(estimator=self.spl,\n",
    "                                   param_grid=param_grid,\n",
    "                                   cv=cv,\n",
    "                                   scoring=scoring,\n",
    "                                   n_jobs=n_jobs)\n",
    "        # now call fit\n",
    "        grid_search.fit(self.dfl.X, self.dfl.y)\n",
    "        return grid_search\n",
    "    \n",
    "    # Core methods for Tune SK-Learn GridSearch\n",
    "    def do_tune_grid_search(self,\n",
    "                            param_grid:object,\n",
    "                            scoring:str=None,\n",
    "                            mode:str='max',\n",
    "                            cv:int=5,\n",
    "                            early_stopping=True,\n",
    "                            time_budget_s:int=None,\n",
    "                            name:str=None,\n",
    "                            use_gpu:bool=False,\n",
    "                            stopper:object=None,\n",
    "                            max_iters:int=10,\n",
    "                            n_jobs=None):\n",
    "        \n",
    "        if n_jobs is None:\n",
    "            n_jobs = self.IDEAL_CPU_CORES\n",
    "        \n",
    "        # create GridSeachCV instance\n",
    "        tune_search = TuneGridSearchCV(\n",
    "            estimator=self.spl,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            mode=mode,\n",
    "            cv=cv,\n",
    "            time_budget_s=time_budget_s,\n",
    "            name=name,\n",
    "            use_gpu=use_gpu,\n",
    "            early_stopping=early_stopping,\n",
    "            stopper=stopper,\n",
    "            max_iters=max_iters,\n",
    "            n_jobs=n_jobs,\n",
    "            pipeline_auto_early_stop=True)\n",
    "        \n",
    "        # now call fit\n",
    "        tune_search.fit(self.dfl.X, self.dfl.y)\n",
    "        return tune_search\n",
    "\n",
    "    \n",
    "    # do k-fold training\n",
    "    # metrics has to be sklearn metrics object type mean_absoulte_error, acccuracy\n",
    "    def do_k_fold_training(self, n_splits:int, metrics:object, random_state=42):\n",
    "        \n",
    "        #create stratified K Folds instance\n",
    "        k_fold = StratifiedKFold(n_splits=n_splits,\n",
    "                             random_state=random_state,\n",
    "                             shuffle=True)\n",
    "        \n",
    "        # check for test dataset before prediction\n",
    "        if self.dfl.X_test is not None:\n",
    "            test_preds = np.zeros(self.dfl.X_test.shape[0])\n",
    "        # list contains metrics score for each fold\n",
    "        metrics_score = []\n",
    "        n=0\n",
    "        for train_idx, valid_idx in k_fold.split(self.dfl.X, self.dfl.y):\n",
    "            # create X_train\n",
    "            self.dfl.X_train = self.dfl.X.iloc[train_idx]\n",
    "            # create X_valid\n",
    "            self.dfl.X_valid = self.dfl.X.iloc[valid_idx] \n",
    "            # create y_train\n",
    "            self.dfl.y_train = self.dfl.y.iloc[train_idx]\n",
    "            # create y_valid\n",
    "            self.dfl.y_valid = self.dfl.y.iloc[valid_idx]\n",
    "            \n",
    "            # fit\n",
    "            self.spl.fit(self.dfl.X_train, self.dfl.y_train)\n",
    "            \n",
    "            #evaluate metrics based upon input\n",
    "            if \"proba\" in metrics.__globals__:\n",
    "                # predictions on valid dataset\n",
    "                metrics_score.append(metrics(self.dfl.y_valid,\n",
    "                                               self.spl.predict_proba(self.dfl.X_valid)[:,1]))\n",
    "                if self.dfl.X_test is not None: \n",
    "                    # prediction probabs on test dataset\n",
    "                    test_preds += self.spl.predict_proba(self.dfl.X_test)[:,1] / k_fold.n_splits\n",
    "            else:\n",
    "                metrics_score.append(metrics(self.dfl.y_valid,\n",
    "                                               self.spl.predict(self.dfl.X_valid)))\n",
    "                if self.dfl.X_test is not None: \n",
    "                    # predictions on test dataset\n",
    "                    test_preds += self.spl.predict(self.dfl.X_test) / k_fold.n_splits          \n",
    "            \n",
    "            logger.info(f\"fold: {n+1} , {str(metrics.__name__)}: {metrics_score[n]}\")\n",
    "            # increment fold counter label\n",
    "            n += 1\n",
    "        \n",
    "        \n",
    "        return metrics_score, test_preds\n",
    "    \n",
    "    # do optuna bases study optimization for hyperparmaeter search\n",
    "    # task could be only \"classification\" or \"regression\"\n",
    "    # xgb_eval_metric string reprsenting \"mae\", \"rmse\", \"logloss\"\n",
    "    # kfold_metrics need to be sklearn metrics object type some of them are:\n",
    "    # from sklearn.metrics import mean_absolute_error, roc_auc_score,accuracy_score\n",
    "    # kfold_splits should be int, default is 5\n",
    "    def do_xgb_optuna_optimization(self, task:str, xgb_eval_metric:str, kfold_metrics:str,\n",
    "                                   kfold_splits=5, use_gpu=False, opt_trials=100, opt_timeout=360):\n",
    "        \n",
    "        #get params\n",
    "        def get_params(trial, use_gpu=False):\n",
    "            params = {\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n",
    "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9),\n",
    "                \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 100, 500),\n",
    "                \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [7000, 15000, 20000]),\n",
    "            }\n",
    "            if use_gpu:\n",
    "                params[\"tree_method\"] = \"gpu_hist\"\n",
    "                params[\"gpu_id\"] = 0\n",
    "                params[\"predictor\"] = \"gpu_predictor\"\n",
    "            else:\n",
    "                params[\"tree_method\"] = trial.suggest_categorical(\"tree_method\", [\"exact\", \"approx\", \"hist\"])\n",
    "                params[\"booster\"] = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\"])\n",
    "                if params[\"booster\"] == \"gbtree\":\n",
    "                    params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "                    params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "            return params\n",
    "        \n",
    "        # Creatre Optuna Objective Class         \n",
    "        class Objective(object):\n",
    "            def __init__(self, dfl):\n",
    "                self.X = dfl.X\n",
    "                self.y = dfl.y\n",
    "\n",
    "            def __call__(self, trial):\n",
    "                x, y = self.X, self.y\n",
    "                \n",
    "                #get_params here\n",
    "                params = get_params(trial, use_gpu=False)\n",
    "                early_stopping_rounds = params[\"early_stopping_rounds\"]\n",
    "                del params[\"early_stopping_rounds\"]\n",
    "                \n",
    "                # get xgb model based on task type\n",
    "                if task == \"regression\":\n",
    "                    xgb_model = XGBRegressor(eval_metric=xgb_eval_metric,random_state=42, use_label_encoder=False,**params)\n",
    "                if task == \"classification\":\n",
    "                    xgb_model = XGBClassifier(eval_metric=xgb_eval_metric,random_state=42, use_label_encoder=False,**params)\n",
    "                \n",
    "                # update the model here on tmlt pipeline\n",
    "                self.update_model(xgb_model)\n",
    "                \n",
    "                #rest remains same\n",
    "                score, _ = tmlt.do_k_fold_training(n_splits=kfold_splits, metrics=kfold_metrics)\n",
    "                metrics_mean_score = np.mean(score)\n",
    "                return metrics_mean_score\n",
    "                \n",
    "            \n",
    "        # now call objective instance\n",
    "        # Load the dataset in advance for reusing it each trial execution.\n",
    "        objective = Objective(tmlt.dfl)\n",
    "        # create sql db in output directory path\n",
    "        db_path = os.path.join(OUTPUT_DIR_PATH, \"params.db\")\n",
    "\n",
    "        # choose direction based upon metrics type\n",
    "        if \"proba\" in metrics.__globals__:\n",
    "            metrics_direction = \"maximize\"\n",
    "        else:\n",
    "            metrics_direction = \"minimize\"\n",
    "        \n",
    "        # now create study\n",
    "        study = optuna.create_study(\n",
    "            direction=metrics_direction,\n",
    "            study_name=\"tmlt_autoxgb\",\n",
    "            storage=f\"sqlite:///{db_path}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(objective, n_trials=opt_trials, timeout=opt_timeout)\n",
    "        return study\n",
    "        \n",
    "    \n",
    "    # helper method for update_preprocessor\n",
    "    # to create params value dict from grid_search object\n",
    "    def get_preprocessor_best_params(self, grid_search_object:object):\n",
    "        pp_best_params = {}\n",
    "        for k in grid_search_object.best_params_:\n",
    "            #print(k)\n",
    "            if 'preprocessor' in k:\n",
    "                key = k.split('__')[1] + \"__\" + k.split('__')[2] \n",
    "                pp_best_params[key] = grid_search_object.best_params_[k]\n",
    "        return pp_best_params\n",
    "    \n",
    "    # helper method for update_model\n",
    "    # to create params value dict from grid_search object\n",
    "    def get_model_best_params(self, grid_search_object:object):\n",
    "        model_best_params = {}\n",
    "        for k in grid_search_object.best_params_:\n",
    "            #print(k)\n",
    "            if 'model' in k:\n",
    "                key = k.split('__')[1]\n",
    "                model_best_params[key] = grid_search_object.best_params_[k]\n",
    "        return model_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLPipeline.prepare_data_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
