{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tmlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "> An API to create super fast training pipeline for machine learning models based on tabular or strucuture data\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter tuning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tabular_ml_toolkit.dataframeloader import *\n",
    "from tabular_ml_toolkit.preprocessor import *\n",
    "from tabular_ml_toolkit.logger import *\n",
    "from tabular_ml_toolkit.xgb_optuna_objective import *\n",
    "from tabular_ml_toolkit.utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "# for Optuna\n",
    "import optuna\n",
    "#for XGB\n",
    "import xgboost\n",
    "# for imbalance learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# for finding n_jobs in all sklearn estimators\n",
    "from sklearn.utils import all_estimators\n",
    "import inspect\n",
    "\n",
    "# Just to compare fit times\n",
    "import time\n",
    "\n",
    "# for os specific settings\n",
    "import gc\n",
    "import os\n",
    "from shutil import rmtree\n",
    "\n",
    "#for clearning torch cache\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TMLT:\n",
    "    \"\"\"\n",
    "    Represent Tabular ML Toolkit class\n",
    "    \n",
    "    Attributes:\\n\n",
    "    dfl: A DataFrameLoader instance \\n\n",
    "    pp: A PreProcessor instance \\n\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dfl = None\n",
    "        self.pp = None\n",
    "        self.transformer_type = None\n",
    "        self.problem_type = None\n",
    "        self.has_n_jobs = check_has_n_jobs()\n",
    "        self.IDEAL_CPU_CORES = find_ideal_cpu_cores()\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"Returns human readable string reprsentation\"\"\"\n",
    "        attr_str = (\"dfl, pp\")\n",
    "        return (\"Training Pipeline object with attributes:\"+attr_str)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "                \n",
    "    ## All Core Methods ##\n",
    "    \n",
    "    # Main Method to create, load, preprocess ready data based upon problem type\n",
    "    def prepare_data(self, train_file_path:str,\n",
    "                                  problem_type:str,\n",
    "                                  idx_col:str, target:str,\n",
    "                                  random_state:int,\n",
    "                                  test_file_path:str=None,\n",
    "                                  nrows=None):\n",
    "        #set problem type\n",
    "        self.problem_type = problem_type\n",
    "        \n",
    "        # call DataFrameLoader module\n",
    "        self.dfl = DataFrameLoader().from_csv(\n",
    "            train_file_path=train_file_path,\n",
    "            test_file_path=test_file_path,\n",
    "            idx_col=idx_col,\n",
    "            target=target,\n",
    "            random_state=random_state,\n",
    "            nrows=nrows,\n",
    "            problem_type=self.problem_type)\n",
    "        \n",
    "        # call PreProcessor module\n",
    "        self.pp = PreProcessor().preprocess_all_cols(dataframeloader=self.dfl, problem_type=self.problem_type)\n",
    "\n",
    "        # return tmlt\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    #fit-tranform preprocessor\n",
    "    def pp_fit_transform(self, X:object, y:object, X_test:object=None, resample_approach:str=None,\n",
    "                         random_state:int=42):\n",
    "        X_np = None\n",
    "        X_test_np = None\n",
    "        y_np = None\n",
    "        \n",
    "        #fit-transform for X and X_test\n",
    "        X_np = self.pp.columns_transfomer.fit_transform(X)\n",
    "        if X_test is not None:\n",
    "            X_test_np = self.pp.columns_transfomer.transform(X_test)\n",
    "        # fit-tranform for y\n",
    "        if \"classification\" in self.problem_type:\n",
    "            y_np = self.pp.target_cols__encoder.fit_transform(y)\n",
    "            # logger.info(f\"Encoded y_np classes values are: {dict(pd.Series(y_np).value_counts())}\")\n",
    "        else:\n",
    "            y_np = y\n",
    "        \n",
    "        # resample can be done by many approaches, here we are using only 3\n",
    "        if resample_approach is not None:\n",
    "            if resample_approach == 'oversample':\n",
    "                resample = SMOTE()\n",
    "            elif resample_approach == 'undersample':\n",
    "                resample = RandomUnderSampler(random_state=random_state)\n",
    "            elif resample_approach == 'combine':\n",
    "                resample = SMOTEENN(random_state=random_state)\n",
    "            # now fit and tranform\n",
    "            X_np, y_np = resample.fit_resample(X_np, y)\n",
    "        \n",
    "        return X_np, y_np, X_test_np\n",
    "    \n",
    "    # Force to update the dataframeloader in pipeline\n",
    "    def update_dfl(self, X:object, y:object, X_test:object,problem_type:str,\n",
    "                   num_cols__imputer=SimpleImputer(strategy='median'),\n",
    "                   num_cols__scaler=StandardScaler(),\n",
    "                   cat_cols__imputer=SimpleImputer(strategy='constant'),\n",
    "                   cat_cols__encoder=OneHotEncoder(handle_unknown='ignore'),\n",
    "                   target_cols__encoder=LabelEncoder()):\n",
    "        \n",
    "        # remove the old pipeline_cach directory\n",
    "        if os.path.isdir(\"pipeline_cache_dir\"):\n",
    "            rmtree(\"pipeline_cache_dir\")\n",
    "        \n",
    "        # regenerate dfl\n",
    "        self.dfl = DataFrameLoader().regenerate_dfl(X,y,X_test)\n",
    "        \n",
    "        # change preprocessor\n",
    "        self.pp = PreProcessor().preprocess_all_cols(self.dfl,problem_type=problem_type,\n",
    "                                                     num_cols__imputer=num_cols__imputer,\n",
    "                                                     num_cols__scaler=num_cols__scaler,\n",
    "                                                     cat_cols__imputer=cat_cols__imputer,\n",
    "                                                     cat_cols__encoder=cat_cols__encoder,\n",
    "                                                     target_cols__encoder=LabelEncoder())\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Force to update the preprocessor in pipeline\n",
    "    def update_preprocessor(self,problem_type:str,\n",
    "                            num_cols__imputer=SimpleImputer(strategy='median'),\n",
    "                            num_cols__scaler=StandardScaler(),\n",
    "                            cat_cols__imputer=SimpleImputer(strategy='constant'),\n",
    "                            cat_cols__encoder=OneHotEncoder(handle_unknown='ignore'),\n",
    "                            target_cols__encoder=LabelEncoder()):\n",
    "        \n",
    "        # remove the old pipeline_cach directory\n",
    "        if os.path.isdir(\"pipeline_cache_dir\"):\n",
    "            rmtree(\"pipeline_cache_dir\")\n",
    "        \n",
    "        # change preprocessor\n",
    "        self.pp = PreProcessor().preprocess_all_cols(self.dfl,problem_type=problem_type,\n",
    "                                                     num_cols__imputer=num_cols__imputer,\n",
    "                                                     num_cols__scaler=num_cols__scaler,\n",
    "                                                     cat_cols__imputer=cat_cols__imputer,\n",
    "                                                     cat_cols__encoder=cat_cols__encoder,\n",
    "                                                     target_cols__encoder=LabelEncoder())\n",
    "        gc.collect()\n",
    "        return self\n",
    "    \n",
    "    # cross validation\n",
    "    def do_cross_validation(self, X:object, y:object, model:object, scoring:str, cv:int=5):\n",
    "        \"\"\"\n",
    "        X, y\n",
    "        model: takes any sklearn compatible model/estimator\n",
    "        scoring: take str which are predefined here from https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "        cv: takes int by default 5\n",
    "        \"\"\"\n",
    "        scores = cross_val_score(\n",
    "            estimator=model,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            n_jobs=self.IDEAL_CPU_CORES)\n",
    "        \n",
    "        # Multiply by -1 since sklearn calculates *negative* scoring for some of the metrics\n",
    "        if \"neg_\" in scoring:\n",
    "            scores = -1 * scores\n",
    "        gc.collect()\n",
    "        return scores\n",
    "        \n",
    "    # GridSearch\n",
    "    def do_grid_search(self, X:object, y:object, param_grid:object, cv:int,\n",
    "                       scoring:str, n_jobs=None):\n",
    "        \n",
    "        if n_jobs is None:\n",
    "            n_jobs = self.IDEAL_CPU_CORES\n",
    "        \n",
    "        # create GridSeachCV instance\n",
    "        grid_search = GridSearchCV(estimator=self.spl,\n",
    "                                   param_grid=param_grid,\n",
    "                                   cv=cv,\n",
    "                                   scoring=scoring,\n",
    "                                   n_jobs=n_jobs)\n",
    "        # now call fit\n",
    "        grid_search.fit(X, y)\n",
    "        gc.collect()\n",
    "        return grid_search\n",
    "    \n",
    "   # do k-fold training, test and oof predictions if enabled\n",
    "    def do_kfold_train_preds(self, X:object, y:object, n_splits:int, model:object, val_metric:object, eval_metric:str, oof:bool=False, X_test:object=None,tabnet_fit_params:dict=None, random_state=42):\n",
    "        \"\"\"\n",
    "            This methods returns oof_preds and test_preds by doing kfold training on sklearn pipeline\n",
    "            n_splits=5 by default, takes only int value\n",
    "            random_sate=42, takes only int value\n",
    "        \"\"\"\n",
    "        #create stratified K Folds instance\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "        \n",
    "        # for OOF Predictions\n",
    "        oof_preds = np.zeros(X.shape[0]) if oof else None\n",
    "        \n",
    "        # check whether test dataset exist before test preds\n",
    "        test_preds = np.zeros(X_test.shape[0]) if X_test is not None else None\n",
    "\n",
    "        # list contains metrics results for each fold\n",
    "        kfold_metrics_results = []\n",
    "        n=0\n",
    "        for train_idx, valid_idx in kfold.split(X, y):\n",
    "            if isinstance(X, np.ndarray):\n",
    "                # create NUMPY based X_train, X_valid, y_train, y_valid\n",
    "                # fix the stratification problem\n",
    "                #train_idx,valid_idx = clip_splits(train_idx,valid_idx,self.dfl.X_full)\n",
    "                X_train , X_valid, y_train, y_valid = X[train_idx], X[valid_idx], y[train_idx], y[valid_idx]\n",
    "            else:\n",
    "                # create PANDAS based X_train, X_valid, y_train, y_valid\n",
    "                # fix the stratification problem\n",
    "                #train_idx,valid_idx = clip_splits(train_idx,valid_idx,self.dfl.X_full)\n",
    "                X_train , X_valid, y_train, y_valid = X.iloc[train_idx], X.iloc[valid_idx], y[train_idx], y[valid_idx]\n",
    "                \n",
    "            #TRAINING\n",
    "            logger.info(f\"Training Started!\")\n",
    "            #should be better way without string matching\n",
    "            #change fit for tabnet\n",
    "            if \"tabnet\" in str(model.__class__):\n",
    "                if  tabnet_fit_params:\n",
    "                    model.fit(X_train, y_train,\n",
    "                                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                                eval_metric=[eval_metric],\n",
    "                                **tabnet_fit_params)\n",
    "                else:\n",
    "                    raise ValueError(\"tabnet model requires tabnet_fit_params!\")     \n",
    "\n",
    "            #change fit for xgb\n",
    "            elif \"xgb\" in str(model.__class__):\n",
    "                model.fit(X_train, y_train,\n",
    "                            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                            eval_metric=eval_metric,\n",
    "                            verbose=False)\n",
    "            \n",
    "            # assumes model will follow standard sklearn fit API\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "            \n",
    "            logger.info(f\"Training Finished!\")\n",
    "\n",
    "            #VAL PREDICTIONS\n",
    "            metric = val_metric\n",
    "            metric_result = {}\n",
    "            preds = None\n",
    "            # getting either hyperplane distance or probablities from predictions\n",
    "            if \"svm\" in str(model.__class__):\n",
    "                logger.info(f\"Predicting Val Decision!\")\n",
    "                preds = model.decision_function(X_valid)\n",
    "            elif needs_predict_proba(metric):\n",
    "                logger.info(\"Predicting Val Probablities!\")\n",
    "                preds = model.predict_proba(X_valid)[:, 1]\n",
    "            else:\n",
    "                logger.info(\"Predicting Val Score!\")\n",
    "                preds = model.predict(X_valid)\n",
    "            \n",
    "            if oof:\n",
    "                #now add back out of fold predictions to that val set index \n",
    "                oof_preds[valid_idx] = preds\n",
    "                    \n",
    "            #METRICS\n",
    "            metric_result[str(metric.__name__)] = metric(y_valid, preds)\n",
    "            \n",
    "            #now display value of all the given metrics\n",
    "            for metric_name, metric_value in metric_result.items():\n",
    "                logger.info(f\"fold: {n+1} {metric_name} : {metric_value}\")\n",
    "            \n",
    "            #now append each kfold metric_result dict to list\n",
    "            kfold_metrics_results.append(metric_result)\n",
    "\n",
    "            #TEST PREDICTIONS\n",
    "            if X_test is not None:\n",
    "                if \"svm\" in str(model.__class__):\n",
    "                    test_preds += model.decision_function(X_test) / kfold.n_splits\n",
    "                else:\n",
    "                    if \"classification\" in self.problem_type:\n",
    "                        logger.info(f\"Predicting Test Set Probabilities!\")\n",
    "                        temp_preds_prob = model.predict_proba(X_test).argmax(axis=1)\n",
    "                        logger.info(f\"Fold: {n+1} Test Predictions Class Count is: {dict(pd.Series(temp_preds_prob).value_counts())}\")\n",
    "                        test_preds += temp_preds_prob / kfold.n_splits\n",
    "                    else:\n",
    "                        logger.info(\"Predicting Test Set Scores!\")\n",
    "                        test_preds += model.predict(X_test) / kfold.n_splits\n",
    "            else:\n",
    "                logger.warn(f\"Trying to do OOF Test Predictions but No Test Dataset Provided!\")\n",
    "\n",
    "            # In order to better GC, del X_train, X_valid, y_train, y_valid df after each fold is done,\n",
    "            # they will recreate again next time k-fold is called\n",
    "            del [X_train, X_valid, y_train, y_valid]\n",
    "            gc.collect()\n",
    "            #trigger cuda cache clean\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # increment fold counter label\n",
    "            n += 1\n",
    "\n",
    "        # inverse tranform labels predicted in test preds\n",
    "        if \"classification\" in self.problem_type:\n",
    "            test_preds = np.around(test_preds).astype(int) #because test_preds are divided by k-fold \n",
    "            test_preds = self.pp.target_cols__encoder.inverse_transform(test_preds)\n",
    "        \n",
    "        # For mean k-fold score\n",
    "        #logger.info(f\"kfold_metrics_results: {kfold_metrics_results} \")\n",
    "        mean_metrics_results = kfold_dict_mean(kfold_metrics_results)\n",
    "        logger.info(f\" Mean {str(metric.__name__)} from all Folds are: {mean_metrics_results}\")\n",
    "        \n",
    "        # for explicit memory clean\n",
    "        del [kfold, model]\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return mean_metrics_results, test_preds, oof_preds\n",
    "    \n",
    "    # Do optuna bases study optimization for hyperparmaeter search\n",
    "    def do_xgb_optuna_optimization(self, X_train_np, y_train_np, X_valid_np, y_valid_np, optuna_db_path:str,\n",
    "                                   use_gpu=False, opt_trials=100, opt_timeout=360, verbose=False):\n",
    "        \"\"\"\n",
    "            This methods returns and do optuna bases study optimization for hyperparmaeter search\n",
    "            optuna_db_path is output directory you want to use for storing sql db used for optuna\n",
    "            use_gpu=False by default, make it True if running on gpu machine\n",
    "            opt_trials=100 by default, change it based upon need\n",
    "            opt_timeout=360 by default, timeout value in seconds\n",
    "\n",
    "        \"\"\"       \n",
    "            \n",
    "        # get params based on problem type\n",
    "        xgb_model_type, val_preds_metrics, eval_metric, direction, _ = fetch_xgb_params_for_problem_type(self.problem_type)\n",
    "        \n",
    "        # Load the dataset in advance for reusing it each trial execution.\n",
    "        objective = XGB_Optuna_Objective(X_train_np, y_train_np, X_valid_np, y_valid_np,\n",
    "                                         val_preds_metrics=val_preds_metrics,\n",
    "                                         xgb_model_type=xgb_model_type, xgb_eval_metric=eval_metric,\n",
    "                                         use_gpu=use_gpu, verbose=verbose)\n",
    "        # create sql db in optuna db path\n",
    "        db_path = os.path.join(optuna_db_path, \"params.db\")\n",
    "        \n",
    "        # now create study\n",
    "        logger.info(f\"Optimization Direction is: {direction}\")\n",
    "        study = optuna.create_study(\n",
    "            direction=direction,\n",
    "            study_name=\"tmlt_autoxgb\",\n",
    "            storage=f\"sqlite:///{db_path}\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(objective, n_trials=opt_trials, timeout=opt_timeout)\n",
    "        gc.collect()\n",
    "        return study\n",
    "\n",
    "    \n",
    "    # helper methods for users before updating preprocessor in pipeline\n",
    "    def get_preprocessor_best_params_from_grid_search(self, grid_search_object:object):\n",
    "        pp_best_params = {}\n",
    "        for k in grid_search_object.best_params_:\n",
    "            #print(k)\n",
    "            if 'preprocessor' in k:\n",
    "                key = k.split('__')[1] + \"__\" + k.split('__')[2] \n",
    "                pp_best_params[key] = grid_search_object.best_params_[k]\n",
    "        return pp_best_params\n",
    "    \n",
    "    # helper methods for users before updating model in pipeline\n",
    "    def get_model_best_params_from_grid_search(self, grid_search_object:object):\n",
    "        model_best_params = {}\n",
    "        for k in grid_search_object.best_params_:\n",
    "            #print(k)\n",
    "            if 'model' in k:\n",
    "                key = k.split('__')[1]\n",
    "                model_best_params[key] = grid_search_object.best_params_[k]\n",
    "        return model_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"TMLT\" class=\"doc_header\"><code>class</code> <code>TMLT</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>TMLT</code>()\n",
       "\n",
       "Represent Tabular ML Toolkit class\n",
       "\n",
       "Attributes:\n",
       "\n",
       "dfl: A DataFrameLoader instance \n",
       "\n",
       "pp: A PreProcessor instance "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TMLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TMLT.prepare_data\" class=\"doc_header\"><code>TMLT.prepare_data</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TMLT.prepare_data</code>(**`train_file_path`**:`str`, **`problem_type`**:`str`, **`idx_col`**:`str`, **`target`**:`str`, **`random_state`**:`int`, **`test_file_path`**:`str`=*`None`*, **`nrows`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TMLT.prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TMLT.do_xgb_optuna_optimization\" class=\"doc_header\"><code>TMLT.do_xgb_optuna_optimization</code><a href=\"__main__.py#L307\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TMLT.do_xgb_optuna_optimization</code>(**`X_train_np`**, **`y_train_np`**, **`X_valid_np`**, **`y_valid_np`**, **`optuna_db_path`**:`str`, **`use_gpu`**=*`False`*, **`opt_trials`**=*`100`*, **`opt_timeout`**=*`360`*, **`verbose`**=*`False`*)\n",
       "\n",
       "This methods returns and do optuna bases study optimization for hyperparmaeter search\n",
       "optuna_db_path is output directory you want to use for storing sql db used for optuna\n",
       "use_gpu=False by default, make it True if running on gpu machine\n",
       "opt_trials=100 by default, change it based upon need\n",
       "opt_timeout=360 by default, timeout value in seconds"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TMLT.do_xgb_optuna_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataframeloader.ipynb.\n",
      "Converted 01_preprocessor.ipynb.\n",
      "Converted 02_tmlt.ipynb.\n",
      "Converted 04_xgb_optuna_objective.ipynb.\n",
      "Converted Kaggle_TPS_Dec_Meta_Clean.ipynb.\n",
      "Converted Kaggle_TPS_Dec_TabNet.ipynb.\n",
      "Converted Kaggle_TPS_Dec_Tutorial-Meta(Tabnet+XGB).ipynb.\n",
      "Converted Kaggle_TPS_Dec_Tutorial-Meta.ipynb.\n",
      "Converted Kaggle_TPS_Dec_Tutorial_XGB.ipynb.\n",
      "Converted Kaggle_TPS_Nov_Tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted logger.ipynb.\n",
      "Converted utility.ipynb.\n",
      "Converted xgb_tabular_ml_toolkit.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "run script to update all files\n",
    "\n",
    "make pypi\n",
    "\n",
    "reconnect and reinstall tmlt on google colab, restart colab\n",
    "use ensembled oof model code from this notebook and add it to google colab\n",
    "make sure to comment out nrows=4000\n",
    "let it run and wait\n",
    "get submissions and download it\n",
    "upload it to kaggle to see results\n",
    " -->"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03d77e6394b78866746a93e431cb2242fb65951a366e3e86e053dc831f6c9ae0"
  },
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
