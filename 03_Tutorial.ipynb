{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Tutorial with Tabular ML Toolkit\n",
    "\n",
    "> A tutorial on getting started with ML Toolkit, a helper library to jumpstart your machine learning project based on Tabular or Structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create MLPipeline with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For example, Here we are using RandomForestRegressor from Scikit-Learn, on  [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*\n",
    "\n",
    "\n",
    "*No need to install scikit-learn as it comes preinstall with Tabular_ML_Toolkit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tabular_ml_toolkit.MLPipeline import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"https://raw.githubusercontent.com/psmathur/tabular_ml_toolkit/master/input/home_data/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# create scikit-learn ml model\n",
    "scikit_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# createm ml pipeline for scikit-learn model\n",
    "sci_ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\", target=\"SalePrice\",\n",
    "    model=scikit_model,\n",
    "    random_state=42,\n",
    "    valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_ml_pl.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now fit and predict\n",
    "sci_ml_pl.spl.fit(sci_ml_pl.dfl.X_train, sci_ml_pl.dfl.y_train)\n",
    "\n",
    "preds = sci_ml_pl.spl.predict(sci_ml_pl.dfl.X_valid)\n",
    "print('X_valid MAE:', mean_absolute_error(sci_ml_pl.dfl.y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do Cross Validation for Scikit-Learn Model on our MLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# createm ml pipeline for scikit-learn model without valid_size i.e. no split, so X,y remain original size\n",
    "sci_ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    model=scikit_model,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_ml_pl.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sci_ml_pl.dfl.final_cols))\n",
    "print(sci_ml_pl.dfl.final_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sci_ml_pl.dfl.numerical_cols))\n",
    "print(sci_ml_pl.dfl.numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sci_ml_pl.dfl.low_card_cat_cols))\n",
    "print(sci_ml_pl.dfl.low_card_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sci_ml_pl.dfl.high_card_cat_cols))\n",
    "print(sci_ml_pl.dfl.high_card_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_ml_pl.dfl.X[sci_ml_pl.dfl.high_card_cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.01 * sci_ml_pl.dfl.shape_X_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit and predict\n",
    "scores = sci_ml_pl.do_cross_validation(cv=5, scoring='neg_mean_absolute_error')\n",
    "print(\"scores:\", scores)\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In background `prepare_data_for_training` and `prepare_data_for_cv`  methods loads your input data into Pandas DataFrame, seprates X(features) and y(target).\n",
    "\n",
    "Then `prepare_data_for_training` methods split X(features) into X_train, y_train, X_valid and y_valid DataFrames.\n",
    "However, `prepare_data_for_cv`  method do not split but let's cross validation split internally X and y DataFrames.\n",
    "\n",
    "Then both methods preprocess all numerical and categorical type data found in these DataFrames using scikit-learn pipelines. Then it bundle preprocessed data with your given model and return an MLPipeline object, this class instance has dataframeloader, preprocessor and scikit-lean pipeline instances, so you can call fit methods on X_train and y_train and predict methods on X_valid or X_test.\n",
    "\n",
    "Please check detail documentation and source code for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: If you want to customize data and preprocessing steps you can do so by using `DataFrameLoader` and `PreProessor` classes. Check detail documentations for these classes for more options.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do Grid Search for HyperParameters Tunning for Scikit Model on our MLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blanket scikit-learn RandomForestRegressor model\n",
    "scikit_model = RandomForestRegressor()\n",
    "\n",
    "# createm ml pipeline for scikit-learn model\n",
    "sci_ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    model=scikit_model,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_ml_pl.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "#     \"preprocessor__num__imputer__strategy\": [\"constant\", \"mean\", \"median\"],\n",
    "#     \"preprocessor__low_cad_cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "#     \"model__max_depth\": [40,80,100],\n",
    "#     'model__max_features': [2, 3],\n",
    "#     'model__min_samples_leaf': [3,4,5],\n",
    "#     'model__min_samples_split': [8,10,12],\n",
    "    \"model__n_estimators\": [250,350,450]\n",
    "}\n",
    "\n",
    "grid_search = sci_ml_pl.do_grid_search(param_grid=param_grid, cv=3,\n",
    "                                       scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"Internal CV Metrics score: {-1*(grid_search.best_score_):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit-learn RandomForestRegressor model with best params from grid search\n",
    "scikit_model = RandomForestRegressor(\n",
    "    n_estimators=550,\n",
    "#     max_depth = 80,\n",
    "#     max_features = 3,\n",
    "#     min_samples_leaf = 3,\n",
    "#     min_samples_split = 8,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createm ml pipeline for scikit-learn model\n",
    "sci_ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    model=scikit_model,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_ml_pl.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sci_ml_pl.dfl.final_cols))\n",
    "# sci_ml_pl.dataframeloader.final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "k_fold, metrics_score = sci_ml_pl.do_k_fold_training(n_splits=10, metrics=mean_absolute_error)\n",
    "print(\"mean metrics score:\", np.mean(metrics_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "preds = sci_ml_pl.do_k_fold_prediction(k_fold=k_fold)\n",
    "\n",
    "print(preds.shape)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use XGBosst on MLPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You can also use MLPipeline with XGBoost model, Just make sure to install XGBooost first depending upon your OS.*\n",
    "\n",
    "*After that all steps remains same. Here is example using XGBRegressor with [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best way to install xgboost if you are on macosx and windows machine is using conda\n",
    "# !conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from xgboost import XGBRegressor\n",
    "# create xgb ml model\n",
    "xgb_model = XGBRegressor(n_estimators=250,learning_rate=0.05, random_state=42)\n",
    "\n",
    "# createm ml pipeline for xgb model\n",
    "xgb_ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    model=xgb_model,\n",
    "    random_state=42,\n",
    "    valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ml_pl.scikit_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit and predict\n",
    "xgb_ml_pl.scikit_pipeline.fit(xgb_ml_pl.dataframeloader.X_train, xgb_ml_pl.dataframeloader.y_train)\n",
    "preds = xgb_ml_pl.scikit_pipeline.predict(xgb_ml_pl.dataframeloader.X_valid)\n",
    "print('X_valid MAE:', mean_absolute_error(xgb_ml_pl.dataframeloader.y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do Cross Validation for XGB Model on our MLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createm ml pipeline for scikit-learn model\n",
    "xgb_ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    model=xgb_model,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ml_pl.scikit_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit and predict\n",
    "scores = xgb_ml_pl.do_cross_validation(cv=3, scoring='neg_mean_absolute_error')\n",
    "print(\"scores:\", scores)\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
