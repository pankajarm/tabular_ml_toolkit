{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp Kaggle_TPS_Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Kaggle TPS Challenge with Tabular ML Toolkit\n",
    "\n",
    "> A Tutorial to showcase usage of Tabular_ML_Toolkit library on Kaggle TPS Challenge Nov 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create MLPipeline with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You can use MLPipeline to quickly train any model which supports scikit-lear fit and transform methods.*\n",
    "\n",
    "*For example, Here we are using LogisticRegression from Scikit-Learn, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.MLPipeline import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for visualizing pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/Users/pamathur/kaggle_datasets/tps_nov_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create scikit-learn ml model\n",
    "scikit_model = MLPClassifier(early_stopping=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createm ml pipeline for scikit-learn model\n",
    "ml_pl = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target column\n",
    "    idx_col=\"id\",\n",
    "    target=\"target\",\n",
    "    model=scikit_model,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pl.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pl.dfl.create_train_valid(valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on dataframe shapes\n",
    "print(f\"X_train shape is {ml_pl.dfl.X_train.shape}\" )\n",
    "print(f\"X_valid shape is {ml_pl.dfl.X_valid.shape}\" )\n",
    "print(f\"y_train shape is {ml_pl.dfl.y_train.shape}\")\n",
    "print(f\"y_valid shape is {ml_pl.dfl.y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & Predict\n",
    "ml_pl.spl.fit(ml_pl.dfl.X_train, ml_pl.dfl.y_train)\n",
    "preds = ml_pl.spl.predict(ml_pl.dfl.X_valid)\n",
    "preds_probs = ml_pl.spl.predict_proba(ml_pl.dfl.X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check on predictions and predictions probabilities shape\n",
    "print(f\"preds shape is {preds.shape}\" )\n",
    "print(f\"preds_probs shape is {preds_probs.shape}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "auc = roc_auc_score(ml_pl.dfl.y_valid, preds_probs)\n",
    "acc = accuracy_score(ml_pl.dfl.y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In background `prepare_data_for_training` and `prepare_data_for_cv`  methods loads your input data into Pandas DataFrame, seprates X(features) and y(target).\n",
    "\n",
    "Then `prepare_data_for_training` methods split X(features) into X_train, y_train, X_valid and y_valid DataFrames.\n",
    "However, `prepare_data_for_cv`  method do not split but let's cross validation split internally X and y DataFrames.\n",
    "\n",
    "Then both methods preprocess all numerical and categorical type data found in these DataFrames using scikit-learn pipelines. Then it bundle preprocessed data with your given model and return an MLPipeline object, this class instance has dataframeloader, preprocessor and scikit-lean pipeline instances, so you can call fit methods on X_train and y_train and predict methods on X_valid or X_test.\n",
    "\n",
    "Please check detail documentation and source code for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: If you want to customize data and preprocessing steps you can do so by using `DataFrameLoader` and `PreProessor` classes. Check detail documentations for these classes for more options.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use XGBosst on MLPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You can also use MLPipeline with XGBoost model, Just make sure to install XGBooost first depending upon your OS.*\n",
    "\n",
    "*After that all steps remains same. Here is example using XGBRegressor with [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best way to install xgboost if you are on macosx and windows machine is using conda\n",
    "# !conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set xgb_params\n",
    "xgb_params = {\n",
    "#     'n_estimators': 1000,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'max_depth': 9,\n",
    "#     'booster': 'gbtree',\n",
    "    'eval_metric': 'auc',\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# create xgb Classifier model\n",
    "xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update pipline with xgb model\n",
    "ml_pl.model = xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & Predict\n",
    "ml_pl.spl.fit(ml_pl.dfl.X_train, ml_pl.dfl.y_train)\n",
    "preds = ml_pl.spl.predict(ml_pl.dfl.X_valid)\n",
    "preds_probs = ml_pl.spl.predict_proba(ml_pl.dfl.X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "auc = roc_auc_score(ml_pl.dfl.y_valid, preds_probs)\n",
    "acc = accuracy_score(ml_pl.dfl.y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do Grid Search for HyperParameters Tunning for Scikit Model on our MLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create blanket scikit-learn LogisticRegression model\n",
    "\n",
    "# # createm ml pipeline for scikit-learn model\n",
    "# ml_pl = MLPipeline().prepare_data_for_training(\n",
    "#     train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "#     test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "#     idx_col=\"id\",\n",
    "#     target=\"target\",\n",
    "#     model=scikit_model,\n",
    "#     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_pl.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "# #     \"preprocessor__num__imputer__strategy\": [\"constant\", \"mean\", \"median\"],\n",
    "# #     \"preprocessor__low_cad_cat__imputer__strategy\": [\"most_frequent\", \"constant\"],\n",
    "# #     \"model__solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n",
    "#     \"model__learning_rate_init\": [0.001, 0.01]\n",
    "# }\n",
    "\n",
    "# grid_search = ml_pl.do_grid_search(param_grid=param_grid, cv=3,\n",
    "#                                        scoring='roc_auc')\n",
    "\n",
    "# print(\"Best params:\")\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "# print(f\"Internal CV Metrics score: {(grid_search.best_score_):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit-learn LosisticRegression model with best params from grid search\n",
    "scikit_model_new = MLPClassifier(learning_rate_init=0.001,\n",
    "                                 early_stopping=True,\n",
    "                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update pipeline with new model\n",
    "ml_pl.model = scikit_model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "sci_model_new_k_fold, sci_model_new_metrics_score = ml_pl.do_k_fold_training(n_splits=15, metrics=roc_auc_score)\n",
    "print(\"mean metrics score:\", np.mean(sci_model_new_metrics_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean metrics_score is : {round(np.mean(metrics_score)*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "sci_model_new_preds = ml_pl.do_k_fold_prediction(k_fold=sci_model_new_k_fold)\n",
    "\n",
    "print(sci_model_new_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training for xgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update pipeline with new model\n",
    "ml_pl.model = scikit_model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "xgb_k_fold, xgb_k_metrics_score = ml_pl.do_k_fold_training(n_splits=15, metrics=roc_auc_score)\n",
    "print(\"mean metrics score:\", np.mean(xgb_k_metrics_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean metrics_score is : {round(np.mean(xgb_k_metrics_score)*100,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "xgb_preds = ml_pl.do_k_fold_prediction(k_fold=xgb_k_fold)\n",
    "\n",
    "print(xgb_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take weighted average of both k-fold models predictions\n",
    "final_preds = ((0.7 * sci_model_new_preds) + (0.3* xgb_pred)) / 2\n",
    "print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "sub['target'] = final_preds\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
