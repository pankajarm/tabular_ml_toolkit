{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Kaggle TPS Challenge with Tabular ML Toolkit\n",
    "\n",
    "> A Tutorial to showcase usage of tabular_ml_toolkit (tmlt) library on Kaggle TPS Challenge Nov 2021.\n",
    "\n",
    "> tabular_ml_toolkit is a helper library to jumpstart your machine learning project based on Tabular or Structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter search techniques.\n",
    "\n",
    "> Under the hood TMLT uses optuna, xgboost and scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create **tmlt** with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are using XGBClassifier, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajmathur/anaconda3/envs/nbdev_env/lib/python3.9/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/Users/pankajmathur/kaggle_datasets/tps_nov_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"kaggle_tps_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just point tmlt in the direction of your data\n",
    "\n",
    "#### Let it know what are idx and target columns in your tabular data\n",
    "\n",
    "#### what kind of problem type you are trying to resolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:19,951 INFO 8 cores found, model and data parallel processing should worked!\n",
      "2021-12-09 23:58:20,069 INFO DataFrame Memory usage decreased to 0.80 Mb (74.4% reduction)\n",
      "2021-12-09 23:58:20,176 INFO DataFrame Memory usage decreased to 0.79 Mb (74.3% reduction)\n",
      "2021-12-09 23:58:20,222 INFO PreProcessing will include target(s) encoding!\n",
      "2021-12-09 23:58:20,222 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "# create tmlt\n",
    "tmlt = TMLT().prepare_data(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"id\",\n",
    "    target=\"target\",\n",
    "    random_state=42,\n",
    "    problem_type=\"binary_classification\",\n",
    "    nrows=4000\n",
    ")\n",
    "\n",
    "\n",
    "# supports only task type\n",
    "# \"binary_classification\"\n",
    "# \"multi_label_classification\"\n",
    "# \"multi_class_classification\"\n",
    "# \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(tmlt.dfl.X))\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(type(tmlt.dfl.y))\n",
    "print(tmlt.dfl.y.shape)\n",
    "print(type(tmlt.dfl.X_test))\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106628</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>3.183594</td>\n",
       "      <td>0.081970</td>\n",
       "      <td>1.188477</td>\n",
       "      <td>3.732422</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>-0.011719</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>1.978516</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.240479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.673828</td>\n",
       "      <td>76.562500</td>\n",
       "      <td>3.378906</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>1.275391</td>\n",
       "      <td>-0.471436</td>\n",
       "      <td>4.546875</td>\n",
       "      <td>0.037720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135864</td>\n",
       "      <td>3.460938</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.124878</td>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.606934</td>\n",
       "      <td>-0.267822</td>\n",
       "      <td>2.578125</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.024719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036316</td>\n",
       "      <td>1.497070</td>\n",
       "      <td>233.500000</td>\n",
       "      <td>2.195312</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>3.126953</td>\n",
       "      <td>5.058594</td>\n",
       "      <td>3.849609</td>\n",
       "      <td>1.801758</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>4.882812</td>\n",
       "      <td>0.085205</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.116089</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.520020</td>\n",
       "      <td>2.140625</td>\n",
       "      <td>0.124451</td>\n",
       "      <td>0.148193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014076</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>1.890625</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>2.697266</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.503906</td>\n",
       "      <td>0.123474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>3.474609</td>\n",
       "      <td>-0.017105</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.044861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003260</td>\n",
       "      <td>3.714844</td>\n",
       "      <td>156.125000</td>\n",
       "      <td>2.148438</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>2.097656</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>-0.038239</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013779</td>\n",
       "      <td>1.910156</td>\n",
       "      <td>-0.042938</td>\n",
       "      <td>0.105591</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>1.043945</td>\n",
       "      <td>1.075195</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.072815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.242188</td>\n",
       "      <td>2.324219</td>\n",
       "      <td>-19.109375</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>0.424561</td>\n",
       "      <td>2.269531</td>\n",
       "      <td>3.621094</td>\n",
       "      <td>4.062500</td>\n",
       "      <td>1.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>3.064453</td>\n",
       "      <td>0.112427</td>\n",
       "      <td>0.100220</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.451172</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>0.056183</td>\n",
       "      <td>0.029724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.138306</td>\n",
       "      <td>0.679199</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>2.736328</td>\n",
       "      <td>-0.043549</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>4.542969</td>\n",
       "      <td>3.132812</td>\n",
       "      <td>4.972656</td>\n",
       "      <td>0.097961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060730</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>-0.031433</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>0.164673</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>-0.237427</td>\n",
       "      <td>2.123047</td>\n",
       "      <td>-0.049316</td>\n",
       "      <td>0.050842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.025436</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>250.375000</td>\n",
       "      <td>3.689453</td>\n",
       "      <td>0.015312</td>\n",
       "      <td>2.490234</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>3.556641</td>\n",
       "      <td>4.164062</td>\n",
       "      <td>0.156860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>4.667969</td>\n",
       "      <td>0.157593</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>1.055664</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>1.661133</td>\n",
       "      <td>1.484375</td>\n",
       "      <td>-0.027924</td>\n",
       "      <td>0.098083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.109253</td>\n",
       "      <td>2.169922</td>\n",
       "      <td>123.062500</td>\n",
       "      <td>3.279297</td>\n",
       "      <td>0.018204</td>\n",
       "      <td>3.630859</td>\n",
       "      <td>4.636719</td>\n",
       "      <td>4.507812</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>0.037140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>3.623047</td>\n",
       "      <td>0.108765</td>\n",
       "      <td>0.111877</td>\n",
       "      <td>0.020645</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>2.648438</td>\n",
       "      <td>2.753906</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.035583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-0.032227</td>\n",
       "      <td>1.616211</td>\n",
       "      <td>171.875000</td>\n",
       "      <td>3.207031</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>4.097656</td>\n",
       "      <td>1.391602</td>\n",
       "      <td>2.486328</td>\n",
       "      <td>0.121643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074463</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.097046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f0        f1          f2        f3        f4        f5        f6  \\\n",
       "id                                                                             \n",
       "0     0.106628  3.593750  132.750000  3.183594  0.081970  1.188477  3.732422   \n",
       "1     0.125000  1.673828   76.562500  3.378906  0.099426  5.093750  1.275391   \n",
       "2     0.036316  1.497070  233.500000  2.195312  0.026917  3.126953  5.058594   \n",
       "3    -0.014076  0.245972  780.000000  1.890625  0.006947  1.531250  2.697266   \n",
       "4    -0.003260  3.714844  156.125000  2.148438  0.018280  2.097656  4.156250   \n",
       "...        ...       ...         ...       ...       ...       ...       ...   \n",
       "3995  0.242188  2.324219  -19.109375  0.984375  0.036438  0.424561  2.269531   \n",
       "3996  0.138306  0.679199   37.125000  2.736328 -0.043549  0.514648  4.542969   \n",
       "3997  0.025436  1.316406  250.375000  3.689453  0.015312  2.490234  1.983398   \n",
       "3998  0.109253  2.169922  123.062500  3.279297  0.018204  3.630859  4.636719   \n",
       "3999 -0.032227  1.616211  171.875000  3.207031 -0.008675  2.208984  4.097656   \n",
       "\n",
       "            f7        f8        f9  ...       f90       f91       f92  \\\n",
       "id                                  ...                                 \n",
       "0     2.265625  2.099609  0.012329  ...  0.010742  1.098633  0.013329   \n",
       "1    -0.471436  4.546875  0.037720  ...  0.135864  3.460938  0.017059   \n",
       "2     3.849609  1.801758  0.057007  ...  0.117310  4.882812  0.085205   \n",
       "3     4.515625  4.503906  0.123474  ... -0.015350  3.474609 -0.017105   \n",
       "4    -0.038239  3.371094  0.034180  ...  0.013779  1.910156 -0.042938   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3995  3.621094  4.062500  1.197266  ... -0.002506  3.064453  0.112427   \n",
       "3996  3.132812  4.972656  0.097961  ...  0.060730  4.125000 -0.031433   \n",
       "3997  3.556641  4.164062  0.156860  ...  0.038269  4.667969  0.157593   \n",
       "3998  4.507812  3.585938  0.037140  ...  0.083191  3.623047  0.108765   \n",
       "3999  1.391602  2.486328  0.121643  ...  0.074463  0.529785  0.002768   \n",
       "\n",
       "           f93       f94       f95       f96       f97       f98       f99  \n",
       "id                                                                          \n",
       "0    -0.011719  0.052765  0.065430  4.210938  1.978516  0.085999  0.240479  \n",
       "1     0.124878  0.154053  0.606934 -0.267822  2.578125 -0.020874  0.024719  \n",
       "2     0.032410  0.116089 -0.001689 -0.520020  2.140625  0.124451  0.148193  \n",
       "3    -0.008102  0.062012  0.041199  0.511719  1.968750  0.040009  0.044861  \n",
       "4     0.105591  0.125122  0.037506  1.043945  1.075195 -0.012817  0.072815  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3995  0.100220  0.036530  0.451172  1.316406  4.625000  0.056183  0.029724  \n",
       "3996  0.059143  0.164673  0.058075 -0.237427  2.123047 -0.049316  0.050842  \n",
       "3997  0.102234  1.055664  0.031769  1.661133  1.484375 -0.027924  0.098083  \n",
       "3998  0.111877  0.020645  0.125122  2.648438  2.753906  0.012726  0.035583  \n",
       "3999  0.098877  0.148682  0.056061  4.210938  0.759766  0.063538  0.097046  \n",
       "\n",
       "[4000 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlt.dfl.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.41 ms, sys: 1.83 ms, total: 8.24 ms\n",
      "Wall time: 6.76 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 100)\n",
      "(3200,)\n",
      "(800, 100)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 24.3 ms, sys: 1.62 ms, total: 25.9 ms\n",
      "Wall time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base xgb classifier model with your best guess params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    # your best guess params\n",
    "    'learning_rate':0.01,\n",
    "    'eval_metric':'auc',\n",
    "    # must for xgb classifier otherwise warning will be shown\n",
    "    'use_label_encoder':False,\n",
    "    # because 42 is the answer for all the randomness of this universe\n",
    "    'random_state':42,\n",
    "    #for GPU\n",
    "    #'tree_method': 'gpu_hist',\n",
    "    #'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.6142302819132088 while Accuracy is : 0.6175 \n",
      "CPU times: user 10.6 s, sys: 123 ms, total: 10.8 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs)\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Meta Ensemble Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure to PreProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 23.6 ms, total: 293 ms\n",
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 1: linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:25,808 INFO Training Started!\n",
      "2021-12-09 23:58:25,840 INFO Training Finished!\n",
      "2021-12-09 23:58:25,844 INFO fold: 1 OOF Model ROC AUC: 0.7257962604771115!\n",
      "2021-12-09 23:58:25,848 INFO Training Started!\n",
      "2021-12-09 23:58:25,881 INFO Training Finished!\n",
      "2021-12-09 23:58:25,884 INFO fold: 2 OOF Model ROC AUC: 0.6957833655705996!\n",
      "2021-12-09 23:58:25,890 INFO Training Started!\n",
      "2021-12-09 23:58:25,920 INFO Training Finished!\n",
      "2021-12-09 23:58:25,923 INFO fold: 3 OOF Model ROC AUC: 0.6616956802063185!\n",
      "2021-12-09 23:58:25,928 INFO Training Started!\n",
      "2021-12-09 23:58:25,963 INFO Training Finished!\n",
      "2021-12-09 23:58:25,965 INFO fold: 4 OOF Model ROC AUC: 0.7076379002699064!\n",
      "2021-12-09 23:58:25,969 INFO Training Started!\n",
      "2021-12-09 23:58:25,994 INFO Training Finished!\n",
      "2021-12-09 23:58:25,996 INFO fold: 5 OOF Model ROC AUC: 0.7227243154104317!\n",
      "2021-12-09 23:58:26,000 INFO Mean OOF Model ROC AUC: 0.7027275043868735!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 501 ms, sys: 114 ms, total: 615 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "#choose model\n",
    "linear_oof_model = LinearSVC(tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=42)\n",
    "\n",
    "#fit and predict\n",
    "linear_oof_model_preds, linear_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=linear_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if linear_oof_model_preds is not None:\n",
    "    print(linear_oof_model_preds.shape)\n",
    "\n",
    "if linear_oof_model_test_preds is not None:    \n",
    "    print(linear_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 2: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:27,128 INFO Training Started!\n",
      "2021-12-09 23:58:27,157 INFO Training Finished!\n",
      "2021-12-09 23:58:27,160 INFO fold: 1 OOF Model ROC AUC: 0.7271695680206318!\n",
      "2021-12-09 23:58:27,165 INFO Training Started!\n",
      "2021-12-09 23:58:27,196 INFO Training Finished!\n",
      "2021-12-09 23:58:27,199 INFO fold: 2 OOF Model ROC AUC: 0.694622823984526!\n",
      "2021-12-09 23:58:27,204 INFO Training Started!\n",
      "2021-12-09 23:58:27,233 INFO Training Finished!\n",
      "2021-12-09 23:58:27,236 INFO fold: 3 OOF Model ROC AUC: 0.662114764667956!\n",
      "2021-12-09 23:58:27,241 INFO Training Started!\n",
      "2021-12-09 23:58:27,264 INFO Training Finished!\n",
      "2021-12-09 23:58:27,267 INFO fold: 4 OOF Model ROC AUC: 0.7080501678057705!\n",
      "2021-12-09 23:58:27,271 INFO Training Started!\n",
      "2021-12-09 23:58:27,298 INFO Training Finished!\n",
      "2021-12-09 23:58:27,301 INFO fold: 5 OOF Model ROC AUC: 0.7226276902067136!\n",
      "2021-12-09 23:58:27,304 INFO Mean OOF Model ROC AUC: 0.7029170029371196!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 469 ms, sys: 102 ms, total: 571 ms\n",
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "log_oof_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "#fit and predict\n",
    "log_oof_model_preds, log_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=log_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "if log_oof_model_preds is not None:\n",
    "    print(log_oof_model_preds.shape)\n",
    "\n",
    "if log_oof_model_test_preds is not None:    \n",
    "    print(log_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 3: SKLearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:30,843 INFO Training Started!\n",
      "2021-12-09 23:58:31,114 INFO Training Finished!\n",
      "2021-12-09 23:58:31,118 INFO fold: 1 OOF Model ROC AUC: 0.7148613797549968!\n",
      "2021-12-09 23:58:31,127 INFO Training Started!\n",
      "2021-12-09 23:58:31,448 INFO Training Finished!\n",
      "2021-12-09 23:58:31,452 INFO fold: 2 OOF Model ROC AUC: 0.6774081237911025!\n",
      "2021-12-09 23:58:31,460 INFO Training Started!\n",
      "2021-12-09 23:58:31,653 INFO Training Finished!\n",
      "2021-12-09 23:58:31,656 INFO fold: 3 OOF Model ROC AUC: 0.6363958736299162!\n",
      "2021-12-09 23:58:31,665 INFO Training Started!\n",
      "2021-12-09 23:58:31,868 INFO Training Finished!\n",
      "2021-12-09 23:58:31,871 INFO fold: 4 OOF Model ROC AUC: 0.6835524578230986!\n",
      "2021-12-09 23:58:31,880 INFO Training Started!\n",
      "2021-12-09 23:58:32,085 INFO Training Finished!\n",
      "2021-12-09 23:58:32,088 INFO fold: 5 OOF Model ROC AUC: 0.6923646764021928!\n",
      "2021-12-09 23:58:32,097 INFO Mean OOF Model ROC AUC: 0.6809165022802615!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 4.09 s, sys: 708 ms, total: 4.8 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "mlp_oof_model = MLPClassifier(max_iter=1000, early_stopping=True)\n",
    "\n",
    "#update the model on sklearn pipeline\n",
    "# tmlt = tmlt.update_model(mlp_oof_model)\n",
    "\n",
    "# # lets see updated sklearn pipeline with new model\n",
    "# tmlt.spl\n",
    "\n",
    "#fit and predict\n",
    "mlp_oof_model_preds, mlp_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=mlp_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "if mlp_oof_model_preds is not None:\n",
    "    print(mlp_oof_model_preds.shape)\n",
    "\n",
    "if mlp_oof_model_test_preds is not None:    \n",
    "    print(mlp_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 4: TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:34,242 INFO Training Started!\n",
      "2021-12-09 23:58:42,305 INFO Training Finished!\n",
      "2021-12-09 23:58:42,335 INFO fold: 1 OOF Model ROC AUC: 0.558401031592521!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 50 with best_epoch = 44 and best_val_0_auc = 0.5584\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:42,448 INFO Training Started!\n",
      "2021-12-09 23:58:44,406 INFO Training Finished!\n",
      "2021-12-09 23:58:44,437 INFO fold: 2 OOF Model ROC AUC: 0.6584655061250806!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.65847\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:44,547 INFO Training Started!\n",
      "2021-12-09 23:58:46,455 INFO Training Finished!\n",
      "2021-12-09 23:58:46,487 INFO fold: 3 OOF Model ROC AUC: 0.6677756286266925!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.66778\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:46,604 INFO Training Started!\n",
      "2021-12-09 23:58:48,428 INFO Training Finished!\n",
      "2021-12-09 23:58:48,458 INFO fold: 4 OOF Model ROC AUC: 0.6324828168179388!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.63248\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:48,572 INFO Training Started!\n",
      "2021-12-09 23:58:50,446 INFO Training Finished!\n",
      "2021-12-09 23:58:50,477 INFO fold: 5 OOF Model ROC AUC: 0.6354975231739447!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.6355\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:58:50,592 INFO Mean OOF Model ROC AUC: 0.6305245012672356!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 17.5 s, sys: 5.88 s, total: 23.4 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "tabnet_oof_model = TabNetClassifier(optimizer_params=dict(lr=0.02), verbose=0)\n",
    "\n",
    "#fit and predict\n",
    "tabnet_oof_model_preds, tabnet_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=tabnet_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if tabnet_oof_model_preds is not None:\n",
    "    print(tabnet_oof_model_preds.shape)\n",
    "\n",
    "if tabnet_oof_model_test_preds is not None:\n",
    "    print(tabnet_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add back based models predictions to X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 101)\n",
      "(4000, 101)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"linear_preds\"] = linear_oof_model_preds\n",
    "tmlt.dfl.X_test[\"linear_preds\"] = linear_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 102)\n",
      "(4000, 102)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"log_reg_preds\"] = log_oof_model_preds\n",
    "tmlt.dfl.X_test[\"log_reg_preds\"] = log_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 103)\n",
      "(4000, 103)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"mlp_preds\"] = mlp_oof_model_preds\n",
    "tmlt.dfl.X_test[\"mlp_preds\"] = mlp_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 104)\n",
      "(4000, 104)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"tabnet_preds\"] = tabnet_oof_model_preds\n",
    "tmlt.dfl.X_test[\"tabnet_preds\"] = tabnet_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just update the tmlt with this new X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:59:07,293 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "tmlt = tmlt.update_dfl(X=tmlt.dfl.X, y=tmlt.dfl.y, X_test=tmlt.dfl.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For META Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 104)\n",
      "(3200,)\n",
      "(800, 104)\n",
      "(800,)\n",
      "CPU times: user 7.03 ms, sys: 1.77 ms, total: 8.81 ms\n",
      "Wall time: 7.15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200, 104)\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 104)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 47.8 ms, sys: 4.4 ms, total: 52.2 ms\n",
      "Wall time: 50.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_params = {\n",
    "#     'objective': 'binary:logistic', \n",
    "#     'use_label_encoder': False,\n",
    "#     'n_estimators': 40000,\n",
    "#     'learning_rate': 0.18515462875481553,\n",
    "#     'subsample': 0.97, \n",
    "#     'colsample_bytree': 0.32,\n",
    "#     'max_depth': 1,\n",
    "#     'booster': 'gbtree',\n",
    "#     'gamma': 0.2, \n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'reg_lambda': 0.11729916523488974, \n",
    "#     'reg_alpha': 0.6318827156945853,\n",
    "#     'random_state': 42,\n",
    "#     'n_jobs': 4, \n",
    "#     'min_child_weight': 256,\n",
    "#     #for GPU\n",
    "# #     'tree_method': 'gpu_hist',\n",
    "# #     'predictor': 'gpu_predictor',\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': 0.21761562020600114,\n",
    "    'eval_metric': 'auc',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42,\n",
    "    'booster': 'gblinear',\n",
    "    'colsample_bytree': 0.1027132584989078,\n",
    "    'early_stopping_rounds': 171,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 7000,\n",
    "    'reg_alpha': 9.583579660175245e-06,\n",
    "    'reg_lambda': 9.238315962782784e-05,\n",
    "    'subsample': 0.4464473710560276,\n",
    "    'tree_method': 'approx',\n",
    "    #for GPU\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.6933467954809418 while Accuracy is : 0.67625 \n",
      "CPU times: user 10.9 s, sys: 95.7 ms, total: 11 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs)\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOW!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Meta Model, Let's do Optuna based HyperParameter search to get best params for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:59:37,980 INFO Optimization Direction is: maximize\n",
      "\u001b[32m[I 2021-12-09 23:59:38,032]\u001b[0m Using an existing study with name 'tmlt_autoxgb' instead of creating a new one.\u001b[0m\n",
      "2021-12-09 23:59:38,210 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:59:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, eval_set, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 23:59:47,028 INFO Training Ended!\n",
      "2021-12-09 23:59:47,033 INFO roc_auc_score: 0.7741592756836658\n",
      "2021-12-09 23:59:47,034 INFO accuracy_score: 0.72625\n",
      "\u001b[32m[I 2021-12-09 23:59:47,072]\u001b[0m Trial 38 finished with value: 0.7741592756836658 and parameters: {'learning_rate': 0.07948707900984789, 'n_estimators': 15000, 'reg_lambda': 7.984356925605064e-06, 'reg_alpha': 2.5216595944303144e-06, 'subsample': 0.9949361731336916, 'colsample_bytree': 0.30017155189532796, 'max_depth': 5, 'early_stopping_rounds': 268, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n",
      "2021-12-09 23:59:47,269 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:59:47] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, eval_set } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:00:17,929 INFO Training Ended!\n",
      "2021-12-10 00:00:17,953 INFO roc_auc_score: 0.6579558652729384\n",
      "2021-12-10 00:00:17,954 INFO accuracy_score: 0.65375\n",
      "\u001b[32m[I 2021-12-10 00:00:17,995]\u001b[0m Trial 39 finished with value: 0.6579558652729384 and parameters: {'learning_rate': 0.08007436447328488, 'n_estimators': 15000, 'reg_lambda': 5.254743398476762e-06, 'reg_alpha': 6.9699386536621e-08, 'subsample': 0.9322044962669489, 'colsample_bytree': 0.5785734419018447, 'max_depth': 5, 'early_stopping_rounds': 333, 'tree_method': 'hist', 'booster': 'gbtree', 'gamma': 0.034707793715938046, 'grow_policy': 'depthwise'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n",
      "2021-12-10 00:00:18,141 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, eval_set, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:00:27,180 INFO Training Ended!\n",
      "2021-12-10 00:00:27,185 INFO roc_auc_score: 0.7741064829479463\n",
      "2021-12-10 00:00:27,186 INFO accuracy_score: 0.72625\n",
      "\u001b[32m[I 2021-12-10 00:00:27,209]\u001b[0m Trial 40 finished with value: 0.7741064829479463 and parameters: {'learning_rate': 0.03198809904064819, 'n_estimators': 15000, 'reg_lambda': 1.7499374468232044e-07, 'reg_alpha': 2.6296423826705236e-06, 'subsample': 0.8688234611344541, 'colsample_bytree': 0.32645970885560793, 'max_depth': 5, 'early_stopping_rounds': 219, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n",
      "2021-12-10 00:00:27,385 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, eval_set, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:00:37,591 INFO Training Ended!\n",
      "2021-12-10 00:00:37,596 INFO roc_auc_score: 0.7740338929363321\n",
      "2021-12-10 00:00:37,596 INFO accuracy_score: 0.72625\n",
      "\u001b[32m[I 2021-12-10 00:00:37,618]\u001b[0m Trial 41 finished with value: 0.7740338929363321 and parameters: {'learning_rate': 0.028897229702683155, 'n_estimators': 15000, 'reg_lambda': 1.1888881323452074e-07, 'reg_alpha': 2.6601366349662993e-06, 'subsample': 0.9680189654657809, 'colsample_bytree': 0.32353956611499696, 'max_depth': 5, 'early_stopping_rounds': 220, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n",
      "2021-12-10 00:00:37,778 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, eval_set, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:00:47,908 INFO Training Ended!\n",
      "2021-12-10 00:00:47,912 INFO roc_auc_score: 0.7739876992925773\n",
      "2021-12-10 00:00:47,912 INFO accuracy_score: 0.72625\n",
      "\u001b[32m[I 2021-12-10 00:00:47,937]\u001b[0m Trial 42 finished with value: 0.7739876992925773 and parameters: {'learning_rate': 0.025719759580870857, 'n_estimators': 15000, 'reg_lambda': 4.475205533321173e-08, 'reg_alpha': 1.8902434023049325e-06, 'subsample': 0.9801702965579114, 'colsample_bytree': 0.2874747934370655, 'max_depth': 5, 'early_stopping_rounds': 216, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=38, values=[0.7741592756836658], datetime_start=datetime.datetime(2021, 12, 9, 23, 59, 38, 76564), datetime_complete=datetime.datetime(2021, 12, 9, 23, 59, 47, 35226), params={'booster': 'gblinear', 'colsample_bytree': 0.30017155189532796, 'early_stopping_rounds': 268, 'learning_rate': 0.07948707900984789, 'max_depth': 5, 'n_estimators': 15000, 'reg_alpha': 2.5216595944303144e-06, 'reg_lambda': 7.984356925605064e-06, 'subsample': 0.9949361731336916, 'tree_method': 'approx'}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear')), 'colsample_bytree': UniformDistribution(high=1.0, low=0.1), 'early_stopping_rounds': IntUniformDistribution(high=500, low=100, step=1), 'learning_rate': LogUniformDistribution(high=0.25, low=0.01), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'n_estimators': CategoricalDistribution(choices=(7000, 15000, 20000)), 'reg_alpha': LogUniformDistribution(high=100.0, low=1e-08), 'reg_lambda': LogUniformDistribution(high=100.0, low=1e-08), 'subsample': UniformDistribution(high=1.0, low=0.1), 'tree_method': CategoricalDistribution(choices=('exact', 'approx', 'hist'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=39, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "# **Just make sure to supply an output directory path so hyperparameter search is saved**\n",
    "study = tmlt.do_xgb_optuna_optimization(optuna_db_path=OUTPUT_PATH, opt_timeout=60)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now update the meta model with best params from study and then update the sklearn pipeline with this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb_params {'learning_rate': 0.07948707900984789, 'eval_metric': 'auc', 'use_label_encoder': False, 'random_state': 42, 'booster': 'gblinear', 'colsample_bytree': 0.30017155189532796, 'early_stopping_rounds': 268, 'max_depth': 5, 'n_estimators': 15000, 'reg_alpha': 2.5216595944303144e-06, 'reg_lambda': 7.984356925605064e-06, 'subsample': 0.9949361731336916, 'tree_method': 'approx'}\n"
     ]
    }
   ],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "print(\"xgb_params\", xgb_params)\n",
    "updated_xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 560 ms, sys: 17.7 ms, total: 578 ms\n",
      "Wall time: 78.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:51:39,405 INFO  model class:<class 'xgboost.sklearn.XGBClassifier'>\n",
      "2021-12-10 00:51:39,410 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:51:39] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:52:00,853 INFO Training Finished!\n",
      "2021-12-10 00:52:00,854 INFO Predicting Val Probablities!\n",
      "2021-12-10 00:52:00,856 INFO Predicting Val Score!\n",
      "2021-12-10 00:52:00,860 INFO fold: 1 roc_auc_score : 0.7867504835589942\n",
      "2021-12-10 00:52:00,861 INFO fold: 1 accuracy_score : 0.72625\n",
      "2021-12-10 00:52:00,861 INFO Predicting Test Probablities!\n",
      "2021-12-10 00:52:00,867 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:52:00] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# k-fold training\n",
    "xgb_model_metrics_score, xgb_model_test_preds = tmlt.do_kfold_training(X_np,\n",
    "                                                                       y_np,\n",
    "                                                                       X_test=X_test_np,\n",
    "                                                                       n_splits=5,\n",
    "                                                                       model=updated_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "# predict on test dataset\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take weighted average of both k-fold models predictions\n",
    "# final_preds = ((0.45 * sci_model_preds) + (0.55* xgb_model_test_preds)) / 2\n",
    "# print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "# sub['target'] = final_preds\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "# # run the script to build \n",
    "\n",
    "# from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev_env",
   "language": "python",
   "name": "nbdev_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
