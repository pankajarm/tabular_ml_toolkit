{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Kaggle TPS Challenge with Tabular ML Toolkit\n",
    "\n",
    "> A Tutorial to showcase usage of tabular_ml_toolkit (tmlt) library on Kaggle TPS Challenge Nov 2021.\n",
    "\n",
    "> tabular_ml_toolkit is a helper library to jumpstart your machine learning project based on Tabular or Structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter search techniques.\n",
    "\n",
    "> Under the hood TMLT uses optuna, xgboost and scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create **tmlt** with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are using XGBClassifier, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajmathur/anaconda3/envs/nbdev_env/lib/python3.9/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/Users/pankajmathur/kaggle_datasets/tps_nov_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"kaggle_tps_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just point tmlt in the direction of your data\n",
    "\n",
    "#### Let it know what are idx and target columns in your tabular data\n",
    "\n",
    "#### what kind of problem type you are trying to resolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:08,236 INFO 8 cores found, model and data parallel processing should worked!\n",
      "2021-12-06 23:20:08,350 INFO DataFrame Memory usage decreased to 0.80 Mb (74.4% reduction)\n",
      "2021-12-06 23:20:08,457 INFO DataFrame Memory usage decreased to 0.79 Mb (74.3% reduction)\n",
      "2021-12-06 23:20:08,502 INFO PreProcessing will include target(s) encoding!\n",
      "2021-12-06 23:20:08,503 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "# create tmlt\n",
    "tmlt = TMLT().prepare_data(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"id\",\n",
    "    target=\"target\",\n",
    "    random_state=42,\n",
    "    problem_type=\"binary_classification\", nrows=4000)\n",
    "\n",
    "\n",
    "# supports only task type\n",
    "# \"binary_classification\"\n",
    "# \"multi_label_classification\"\n",
    "# \"multi_class_classification\"\n",
    "# \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(tmlt.dfl.X))\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(type(tmlt.dfl.y))\n",
    "print(tmlt.dfl.y.shape)\n",
    "print(type(tmlt.dfl.X_test))\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106628</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>3.183594</td>\n",
       "      <td>0.081970</td>\n",
       "      <td>1.188477</td>\n",
       "      <td>3.732422</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>-0.011719</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>1.978516</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.240479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.673828</td>\n",
       "      <td>76.562500</td>\n",
       "      <td>3.378906</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>1.275391</td>\n",
       "      <td>-0.471436</td>\n",
       "      <td>4.546875</td>\n",
       "      <td>0.037720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135864</td>\n",
       "      <td>3.460938</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.124878</td>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.606934</td>\n",
       "      <td>-0.267822</td>\n",
       "      <td>2.578125</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.024719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036316</td>\n",
       "      <td>1.497070</td>\n",
       "      <td>233.500000</td>\n",
       "      <td>2.195312</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>3.126953</td>\n",
       "      <td>5.058594</td>\n",
       "      <td>3.849609</td>\n",
       "      <td>1.801758</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>4.882812</td>\n",
       "      <td>0.085205</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.116089</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.520020</td>\n",
       "      <td>2.140625</td>\n",
       "      <td>0.124451</td>\n",
       "      <td>0.148193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014076</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>1.890625</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>2.697266</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.503906</td>\n",
       "      <td>0.123474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>3.474609</td>\n",
       "      <td>-0.017105</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.044861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003260</td>\n",
       "      <td>3.714844</td>\n",
       "      <td>156.125000</td>\n",
       "      <td>2.148438</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>2.097656</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>-0.038239</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013779</td>\n",
       "      <td>1.910156</td>\n",
       "      <td>-0.042938</td>\n",
       "      <td>0.105591</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>1.043945</td>\n",
       "      <td>1.075195</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.072815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.242188</td>\n",
       "      <td>2.324219</td>\n",
       "      <td>-19.109375</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>0.424561</td>\n",
       "      <td>2.269531</td>\n",
       "      <td>3.621094</td>\n",
       "      <td>4.062500</td>\n",
       "      <td>1.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>3.064453</td>\n",
       "      <td>0.112427</td>\n",
       "      <td>0.100220</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.451172</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>0.056183</td>\n",
       "      <td>0.029724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.138306</td>\n",
       "      <td>0.679199</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>2.736328</td>\n",
       "      <td>-0.043549</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>4.542969</td>\n",
       "      <td>3.132812</td>\n",
       "      <td>4.972656</td>\n",
       "      <td>0.097961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060730</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>-0.031433</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>0.164673</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>-0.237427</td>\n",
       "      <td>2.123047</td>\n",
       "      <td>-0.049316</td>\n",
       "      <td>0.050842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.025436</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>250.375000</td>\n",
       "      <td>3.689453</td>\n",
       "      <td>0.015312</td>\n",
       "      <td>2.490234</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>3.556641</td>\n",
       "      <td>4.164062</td>\n",
       "      <td>0.156860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>4.667969</td>\n",
       "      <td>0.157593</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>1.055664</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>1.661133</td>\n",
       "      <td>1.484375</td>\n",
       "      <td>-0.027924</td>\n",
       "      <td>0.098083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.109253</td>\n",
       "      <td>2.169922</td>\n",
       "      <td>123.062500</td>\n",
       "      <td>3.279297</td>\n",
       "      <td>0.018204</td>\n",
       "      <td>3.630859</td>\n",
       "      <td>4.636719</td>\n",
       "      <td>4.507812</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>0.037140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>3.623047</td>\n",
       "      <td>0.108765</td>\n",
       "      <td>0.111877</td>\n",
       "      <td>0.020645</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>2.648438</td>\n",
       "      <td>2.753906</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.035583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-0.032227</td>\n",
       "      <td>1.616211</td>\n",
       "      <td>171.875000</td>\n",
       "      <td>3.207031</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>4.097656</td>\n",
       "      <td>1.391602</td>\n",
       "      <td>2.486328</td>\n",
       "      <td>0.121643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074463</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.097046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f0        f1          f2        f3        f4        f5        f6  \\\n",
       "id                                                                             \n",
       "0     0.106628  3.593750  132.750000  3.183594  0.081970  1.188477  3.732422   \n",
       "1     0.125000  1.673828   76.562500  3.378906  0.099426  5.093750  1.275391   \n",
       "2     0.036316  1.497070  233.500000  2.195312  0.026917  3.126953  5.058594   \n",
       "3    -0.014076  0.245972  780.000000  1.890625  0.006947  1.531250  2.697266   \n",
       "4    -0.003260  3.714844  156.125000  2.148438  0.018280  2.097656  4.156250   \n",
       "...        ...       ...         ...       ...       ...       ...       ...   \n",
       "3995  0.242188  2.324219  -19.109375  0.984375  0.036438  0.424561  2.269531   \n",
       "3996  0.138306  0.679199   37.125000  2.736328 -0.043549  0.514648  4.542969   \n",
       "3997  0.025436  1.316406  250.375000  3.689453  0.015312  2.490234  1.983398   \n",
       "3998  0.109253  2.169922  123.062500  3.279297  0.018204  3.630859  4.636719   \n",
       "3999 -0.032227  1.616211  171.875000  3.207031 -0.008675  2.208984  4.097656   \n",
       "\n",
       "            f7        f8        f9  ...       f90       f91       f92  \\\n",
       "id                                  ...                                 \n",
       "0     2.265625  2.099609  0.012329  ...  0.010742  1.098633  0.013329   \n",
       "1    -0.471436  4.546875  0.037720  ...  0.135864  3.460938  0.017059   \n",
       "2     3.849609  1.801758  0.057007  ...  0.117310  4.882812  0.085205   \n",
       "3     4.515625  4.503906  0.123474  ... -0.015350  3.474609 -0.017105   \n",
       "4    -0.038239  3.371094  0.034180  ...  0.013779  1.910156 -0.042938   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3995  3.621094  4.062500  1.197266  ... -0.002506  3.064453  0.112427   \n",
       "3996  3.132812  4.972656  0.097961  ...  0.060730  4.125000 -0.031433   \n",
       "3997  3.556641  4.164062  0.156860  ...  0.038269  4.667969  0.157593   \n",
       "3998  4.507812  3.585938  0.037140  ...  0.083191  3.623047  0.108765   \n",
       "3999  1.391602  2.486328  0.121643  ...  0.074463  0.529785  0.002768   \n",
       "\n",
       "           f93       f94       f95       f96       f97       f98       f99  \n",
       "id                                                                          \n",
       "0    -0.011719  0.052765  0.065430  4.210938  1.978516  0.085999  0.240479  \n",
       "1     0.124878  0.154053  0.606934 -0.267822  2.578125 -0.020874  0.024719  \n",
       "2     0.032410  0.116089 -0.001689 -0.520020  2.140625  0.124451  0.148193  \n",
       "3    -0.008102  0.062012  0.041199  0.511719  1.968750  0.040009  0.044861  \n",
       "4     0.105591  0.125122  0.037506  1.043945  1.075195 -0.012817  0.072815  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3995  0.100220  0.036530  0.451172  1.316406  4.625000  0.056183  0.029724  \n",
       "3996  0.059143  0.164673  0.058075 -0.237427  2.123047 -0.049316  0.050842  \n",
       "3997  0.102234  1.055664  0.031769  1.661133  1.484375 -0.027924  0.098083  \n",
       "3998  0.111877  0.020645  0.125122  2.648438  2.753906  0.012726  0.035583  \n",
       "3999  0.098877  0.148682  0.056061  4.210938  0.759766  0.063538  0.097046  \n",
       "\n",
       "[4000 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlt.dfl.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.46 ms, sys: 1.46 ms, total: 6.92 ms\n",
      "Wall time: 6.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 100)\n",
      "(3200,)\n",
      "(800, 100)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 24.3 ms, sys: 1.71 ms, total: 26 ms\n",
      "Wall time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base xgb classifier model with your best guess params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    # your best guess params\n",
    "    'learning_rate':0.01,\n",
    "    'eval_metric':'auc',\n",
    "    # must for xgb classifier otherwise warning will be shown\n",
    "    'use_label_encoder':False,\n",
    "    # because 42 is the answer for all the randomness of this universe\n",
    "    'random_state':42,\n",
    "    #for GPU\n",
    "    #'tree_method': 'gpu_hist',\n",
    "    #'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.6142302819132088 while Accuracy is : 0.6175 \n",
      "CPU times: user 10.9 s, sys: 167 ms, total: 11.1 s\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs)\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Meta Ensemble Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure to PreProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 273 ms, sys: 19.2 ms, total: 292 ms\n",
      "Wall time: 45.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 1: linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:10,385 INFO Training Started!\n",
      "2021-12-06 23:20:10,418 INFO Training Finished!\n",
      "2021-12-06 23:20:10,421 INFO fold: 1 OOF Model ROC AUC: 0.7257962604771115!\n",
      "2021-12-06 23:20:10,426 INFO Training Started!\n",
      "2021-12-06 23:20:10,453 INFO Training Finished!\n",
      "2021-12-06 23:20:10,456 INFO fold: 2 OOF Model ROC AUC: 0.6957833655705996!\n",
      "2021-12-06 23:20:10,461 INFO Training Started!\n",
      "2021-12-06 23:20:10,492 INFO Training Finished!\n",
      "2021-12-06 23:20:10,495 INFO fold: 3 OOF Model ROC AUC: 0.6616956802063185!\n",
      "2021-12-06 23:20:10,500 INFO Training Started!\n",
      "2021-12-06 23:20:10,538 INFO Training Finished!\n",
      "2021-12-06 23:20:10,541 INFO fold: 4 OOF Model ROC AUC: 0.7076379002699064!\n",
      "2021-12-06 23:20:10,546 INFO Training Started!\n",
      "2021-12-06 23:20:10,580 INFO Training Finished!\n",
      "2021-12-06 23:20:10,584 INFO fold: 5 OOF Model ROC AUC: 0.7227243154104317!\n",
      "2021-12-06 23:20:10,588 INFO Mean OOF Model ROC AUC: 0.7027275043868735!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 793 ms, sys: 186 ms, total: 979 ms\n",
      "Wall time: 206 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "#choose model\n",
    "linear_oof_model = LinearSVC(tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=42)\n",
    "\n",
    "#fit and predict\n",
    "linear_oof_model_preds, linear_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=linear_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if linear_oof_model_preds is not None:\n",
    "    print(linear_oof_model_preds.shape)\n",
    "\n",
    "if linear_oof_model_test_preds is not None:    \n",
    "    print(linear_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 2: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:10,605 INFO Training Started!\n",
      "2021-12-06 23:20:10,630 INFO Training Finished!\n",
      "2021-12-06 23:20:10,633 INFO fold: 1 OOF Model ROC AUC: 0.7271695680206318!\n",
      "2021-12-06 23:20:10,637 INFO Training Started!\n",
      "2021-12-06 23:20:10,665 INFO Training Finished!\n",
      "2021-12-06 23:20:10,667 INFO fold: 2 OOF Model ROC AUC: 0.694622823984526!\n",
      "2021-12-06 23:20:10,672 INFO Training Started!\n",
      "2021-12-06 23:20:10,699 INFO Training Finished!\n",
      "2021-12-06 23:20:10,703 INFO fold: 3 OOF Model ROC AUC: 0.662114764667956!\n",
      "2021-12-06 23:20:10,707 INFO Training Started!\n",
      "2021-12-06 23:20:10,730 INFO Training Finished!\n",
      "2021-12-06 23:20:10,733 INFO fold: 4 OOF Model ROC AUC: 0.7080501678057705!\n",
      "2021-12-06 23:20:10,737 INFO Training Started!\n",
      "2021-12-06 23:20:10,762 INFO Training Finished!\n",
      "2021-12-06 23:20:10,764 INFO fold: 5 OOF Model ROC AUC: 0.7226276902067136!\n",
      "2021-12-06 23:20:10,768 INFO Mean OOF Model ROC AUC: 0.7029170029371196!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 503 ms, sys: 116 ms, total: 618 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "log_oof_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "#fit and predict\n",
    "log_oof_model_preds, log_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=log_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "if log_oof_model_preds is not None:\n",
    "    print(log_oof_model_preds.shape)\n",
    "\n",
    "if log_oof_model_test_preds is not None:    \n",
    "    print(log_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 3: SKLearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:10,782 INFO Training Started!\n",
      "2021-12-06 23:20:11,064 INFO Training Finished!\n",
      "2021-12-06 23:20:11,067 INFO fold: 1 OOF Model ROC AUC: 0.7153836234687297!\n",
      "2021-12-06 23:20:11,076 INFO Training Started!\n",
      "2021-12-06 23:20:11,294 INFO Training Finished!\n",
      "2021-12-06 23:20:11,297 INFO fold: 2 OOF Model ROC AUC: 0.6823726627981948!\n",
      "2021-12-06 23:20:11,306 INFO Training Started!\n",
      "2021-12-06 23:20:11,476 INFO Training Finished!\n",
      "2021-12-06 23:20:11,479 INFO fold: 3 OOF Model ROC AUC: 0.6245067698259188!\n",
      "2021-12-06 23:20:11,487 INFO Training Started!\n",
      "2021-12-06 23:20:11,916 INFO Training Finished!\n",
      "2021-12-06 23:20:11,920 INFO fold: 4 OOF Model ROC AUC: 0.6928606857812791!\n",
      "2021-12-06 23:20:11,928 INFO Training Started!\n",
      "2021-12-06 23:20:12,115 INFO Training Finished!\n",
      "2021-12-06 23:20:12,119 INFO fold: 5 OOF Model ROC AUC: 0.6891696029992462!\n",
      "2021-12-06 23:20:12,127 INFO Mean OOF Model ROC AUC: 0.6808586689746737!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 4.5 s, sys: 730 ms, total: 5.23 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "mlp_oof_model = MLPClassifier(max_iter=1000, early_stopping=True)\n",
    "\n",
    "#update the model on sklearn pipeline\n",
    "# tmlt = tmlt.update_model(mlp_oof_model)\n",
    "\n",
    "# # lets see updated sklearn pipeline with new model\n",
    "# tmlt.spl\n",
    "\n",
    "#fit and predict\n",
    "mlp_oof_model_preds, mlp_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=mlp_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "if mlp_oof_model_preds is not None:\n",
    "    print(mlp_oof_model_preds.shape)\n",
    "\n",
    "if mlp_oof_model_test_preds is not None:    \n",
    "    print(mlp_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 4: TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:12,142 INFO Training Started!\n",
      "2021-12-06 23:20:20,277 INFO Training Finished!\n",
      "2021-12-06 23:20:20,308 INFO fold: 1 OOF Model ROC AUC: 0.558401031592521!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 50 with best_epoch = 44 and best_val_0_auc = 0.5584\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:20,423 INFO Training Started!\n",
      "2021-12-06 23:20:22,374 INFO Training Finished!\n",
      "2021-12-06 23:20:22,405 INFO fold: 2 OOF Model ROC AUC: 0.6584655061250806!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_auc = 0.65847\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:22,518 INFO Training Started!\n",
      "2021-12-06 23:20:24,393 INFO Training Finished!\n",
      "2021-12-06 23:20:24,425 INFO fold: 3 OOF Model ROC AUC: 0.6677756286266925!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.66778\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:24,539 INFO Training Started!\n",
      "2021-12-06 23:20:26,372 INFO Training Finished!\n",
      "2021-12-06 23:20:26,403 INFO fold: 4 OOF Model ROC AUC: 0.6324828168179388!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.63248\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:26,520 INFO Training Started!\n",
      "2021-12-06 23:20:28,385 INFO Training Finished!\n",
      "2021-12-06 23:20:28,416 INFO fold: 5 OOF Model ROC AUC: 0.6354975231739447!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 0 and best_val_0_auc = 0.6355\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:28,533 INFO Mean OOF Model ROC AUC: 0.6305245012672356!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 17.6 s, sys: 5.93 s, total: 23.6 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "tabnet_oof_model = TabNetClassifier(optimizer_params=dict(lr=0.02), verbose=0)\n",
    "\n",
    "#fit and predict\n",
    "tabnet_oof_model_preds, tabnet_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    oof_model=tabnet_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if tabnet_oof_model_preds is not None:\n",
    "    print(tabnet_oof_model_preds.shape)\n",
    "\n",
    "if tabnet_oof_model_test_preds is not None:\n",
    "    print(tabnet_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add back based models predictions to X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 101)\n",
      "(4000, 101)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"linear_preds\"] = linear_oof_model_preds\n",
    "tmlt.dfl.X_test[\"linear_preds\"] = linear_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 102)\n",
      "(4000, 102)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"log_reg_preds\"] = log_oof_model_preds\n",
    "tmlt.dfl.X_test[\"log_reg_preds\"] = log_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 103)\n",
      "(4000, 103)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"mlp_preds\"] = mlp_oof_model_preds\n",
    "tmlt.dfl.X_test[\"mlp_preds\"] = mlp_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 104)\n",
      "(4000, 104)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"tabnet_preds\"] = tabnet_oof_model_preds\n",
    "tmlt.dfl.X_test[\"tabnet_preds\"] = tabnet_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just update the tmlt with this new X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:28,605 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "tmlt = tmlt.update_dfl(X=tmlt.dfl.X, y=tmlt.dfl.y, X_test=tmlt.dfl.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For META Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 104)\n",
      "(3200,)\n",
      "(800, 104)\n",
      "(800,)\n",
      "CPU times: user 6.7 ms, sys: 1.61 ms, total: 8.31 ms\n",
      "Wall time: 7.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200, 104)\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 104)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 40 ms, sys: 3.39 ms, total: 43.4 ms\n",
      "Wall time: 42.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_params = {\n",
    "#     'objective': 'binary:logistic', \n",
    "#     'use_label_encoder': False,\n",
    "#     'n_estimators': 40000,\n",
    "#     'learning_rate': 0.18515462875481553,\n",
    "#     'subsample': 0.97, \n",
    "#     'colsample_bytree': 0.32,\n",
    "#     'max_depth': 1,\n",
    "#     'booster': 'gbtree',\n",
    "#     'gamma': 0.2, \n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'reg_lambda': 0.11729916523488974, \n",
    "#     'reg_alpha': 0.6318827156945853,\n",
    "#     'random_state': 42,\n",
    "#     'n_jobs': 4, \n",
    "#     'min_child_weight': 256,\n",
    "#     #for GPU\n",
    "# #     'tree_method': 'gpu_hist',\n",
    "# #     'predictor': 'gpu_predictor',\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': 0.21761562020600114,\n",
    "    'eval_metric': 'auc',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42,\n",
    "    'booster': 'gblinear',\n",
    "    'colsample_bytree': 0.1027132584989078,\n",
    "    'early_stopping_rounds': 171,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 7000,\n",
    "    'reg_alpha': 9.583579660175245e-06,\n",
    "    'reg_lambda': 9.238315962782784e-05,\n",
    "    'subsample': 0.4464473710560276,\n",
    "    'tree_method': 'approx',\n",
    "    #for GPU\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.6986887604265654 while Accuracy is : 0.6825 \n",
      "CPU times: user 11 s, sys: 121 ms, total: 11.1 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs)\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOW!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Meta Model, Let's do Optuna based HyperParameter search to get best params for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:30,245 INFO Optimization Direction is: maximize\n",
      "\u001b[32m[I 2021-12-06 23:20:30,307]\u001b[0m Using an existing study with name 'tmlt_autoxgb' instead of creating a new one.\u001b[0m\n",
      "2021-12-06 23:20:30,497 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:20:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, eval_set, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:20:37,693 INFO Training Ended!\n",
      "2021-12-06 23:20:37,706 INFO roc_auc_score: 0.7720541653468482\n",
      "2021-12-06 23:20:37,706 INFO log_loss: 0.5504097771365195\n",
      "2021-12-06 23:20:37,707 INFO accuracy_score: 0.72375\n",
      "2021-12-06 23:20:37,707 INFO f1_score: 0.6285714285714287\n",
      "2021-12-06 23:20:37,708 INFO precision_score: 0.6515679442508711\n",
      "2021-12-06 23:20:37,708 INFO recall_score: 0.6071428571428571\n",
      "\u001b[32m[I 2021-12-06 23:20:37,741]\u001b[0m Trial 35 finished with value: 0.7720541653468482 and parameters: {'learning_rate': 0.06917055652243276, 'n_estimators': 7000, 'reg_lambda': 0.0005717720260170862, 'reg_alpha': 6.546458405891649e-07, 'subsample': 0.6290090458891053, 'colsample_bytree': 0.9155898469768312, 'max_depth': 7, 'early_stopping_rounds': 139, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 18 with value: 0.7734399746594869.\u001b[0m\n",
      "2021-12-06 23:20:37,923 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:20:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds, eval_set } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:21:17,526 INFO Training Ended!\n",
      "2021-12-06 23:21:17,594 INFO roc_auc_score: 0.6885426565304614\n",
      "2021-12-06 23:21:17,595 INFO log_loss: 0.6858780123793986\n",
      "2021-12-06 23:21:17,595 INFO accuracy_score: 0.66875\n",
      "2021-12-06 23:21:17,595 INFO f1_score: 0.5190562613430126\n",
      "2021-12-06 23:21:17,596 INFO precision_score: 0.588477366255144\n",
      "2021-12-06 23:21:17,596 INFO recall_score: 0.4642857142857143\n",
      "\u001b[32m[I 2021-12-06 23:21:17,637]\u001b[0m Trial 36 finished with value: 0.6885426565304614 and parameters: {'learning_rate': 0.02265165070501582, 'n_estimators': 7000, 'reg_lambda': 2.624192127564028e-07, 'reg_alpha': 0.00036391611478680695, 'subsample': 0.5915030624370501, 'colsample_bytree': 0.23237799890226452, 'max_depth': 9, 'early_stopping_rounds': 164, 'tree_method': 'hist', 'booster': 'gbtree', 'gamma': 0.740465140416424, 'grow_policy': 'depthwise'}. Best is trial 18 with value: 0.7734399746594869.\u001b[0m\n",
      "2021-12-06 23:21:17,776 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:21:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, eval_set, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:21:32,413 INFO Training Ended!\n",
      "2021-12-06 23:21:32,425 INFO roc_auc_score: 0.7444303663815859\n",
      "2021-12-06 23:21:32,426 INFO log_loss: 0.5777584500750527\n",
      "2021-12-06 23:21:32,426 INFO accuracy_score: 0.7\n",
      "2021-12-06 23:21:32,427 INFO f1_score: 0.574468085106383\n",
      "2021-12-06 23:21:32,428 INFO precision_score: 0.6328125\n",
      "2021-12-06 23:21:32,428 INFO recall_score: 0.525974025974026\n",
      "\u001b[32m[I 2021-12-06 23:21:32,452]\u001b[0m Trial 37 finished with value: 0.7444303663815859 and parameters: {'learning_rate': 0.056158497794197924, 'n_estimators': 20000, 'reg_lambda': 0.010294961570250994, 'reg_alpha': 1.7037741384478345e-05, 'subsample': 0.4147626780645989, 'colsample_bytree': 0.397278539868545, 'max_depth': 6, 'early_stopping_rounds': 185, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 18 with value: 0.7734399746594869.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=18, values=[0.7734399746594869], datetime_start=datetime.datetime(2021, 12, 6, 21, 57, 0, 88637), datetime_complete=datetime.datetime(2021, 12, 6, 21, 57, 4, 336934), params={'booster': 'gblinear', 'colsample_bytree': 0.1027132584989078, 'early_stopping_rounds': 171, 'learning_rate': 0.21761562020600114, 'max_depth': 6, 'n_estimators': 7000, 'reg_alpha': 9.583579660175245e-06, 'reg_lambda': 9.238315962782784e-05, 'subsample': 0.4464473710560276, 'tree_method': 'approx'}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear')), 'colsample_bytree': UniformDistribution(high=1.0, low=0.1), 'early_stopping_rounds': IntUniformDistribution(high=500, low=100, step=1), 'learning_rate': LogUniformDistribution(high=0.25, low=0.01), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'n_estimators': CategoricalDistribution(choices=(7000, 15000, 20000)), 'reg_alpha': LogUniformDistribution(high=100.0, low=1e-08), 'reg_lambda': LogUniformDistribution(high=100.0, low=1e-08), 'subsample': UniformDistribution(high=1.0, low=0.1), 'tree_method': CategoricalDistribution(choices=('exact', 'approx', 'hist'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=19, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "# **Just make sure to supply an output directory path so hyperparameter search is saved**\n",
    "study = tmlt.do_xgb_optuna_optimization(optuna_db_path=OUTPUT_PATH, opt_timeout=60)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now update the meta model with best params from study and then update the sklearn pipeline with this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb_params {'learning_rate': 0.21761562020600114, 'eval_metric': 'auc', 'use_label_encoder': False, 'random_state': 42, 'booster': 'gblinear', 'colsample_bytree': 0.1027132584989078, 'early_stopping_rounds': 171, 'max_depth': 6, 'n_estimators': 7000, 'reg_alpha': 9.583579660175245e-06, 'reg_lambda': 9.238315962782784e-05, 'subsample': 0.4464473710560276, 'tree_method': 'approx'}\n"
     ]
    }
   ],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "print(\"xgb_params\", xgb_params)\n",
    "updated_xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 493 ms, sys: 43.3 ms, total: 536 ms\n",
      "Wall time: 83.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:21:32,574 INFO  model class:<class 'xgboost.sklearn.XGBClassifier'>\n",
      "2021-12-06 23:21:32,578 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:21:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:21:42,349 INFO Training Finished!\n",
      "2021-12-06 23:21:42,350 INFO Predicting Val Probablities!\n",
      "2021-12-06 23:21:42,352 INFO Predicting Val Score!\n",
      "2021-12-06 23:21:42,360 INFO fold: 1 roc_auc_score : 0.7861895551257254\n",
      "2021-12-06 23:21:42,361 INFO fold: 1 log_loss : 0.5517830674474681\n",
      "2021-12-06 23:21:42,361 INFO fold: 1 accuracy_score : 0.72125\n",
      "2021-12-06 23:21:42,362 INFO fold: 1 f1_score : 0.6626323751891074\n",
      "2021-12-06 23:21:42,363 INFO fold: 1 precision_score : 0.6616314199395771\n",
      "2021-12-06 23:21:42,363 INFO fold: 1 recall_score : 0.6636363636363637\n",
      "2021-12-06 23:21:42,364 INFO Predicting Test Probablities!\n",
      "2021-12-06 23:21:42,370 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:21:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:21:51,670 INFO Training Finished!\n",
      "2021-12-06 23:21:51,670 INFO Predicting Val Probablities!\n",
      "2021-12-06 23:21:51,672 INFO Predicting Val Score!\n",
      "2021-12-06 23:21:51,679 INFO fold: 2 roc_auc_score : 0.7962733720180528\n",
      "2021-12-06 23:21:51,680 INFO fold: 2 log_loss : 0.5342360259930138\n",
      "2021-12-06 23:21:51,680 INFO fold: 2 accuracy_score : 0.74125\n",
      "2021-12-06 23:21:51,681 INFO fold: 2 f1_score : 0.6532663316582914\n",
      "2021-12-06 23:21:51,682 INFO fold: 2 precision_score : 0.7303370786516854\n",
      "2021-12-06 23:21:51,682 INFO fold: 2 recall_score : 0.5909090909090909\n",
      "2021-12-06 23:21:51,683 INFO Predicting Test Probablities!\n",
      "2021-12-06 23:21:51,688 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:21:51] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:22:01,036 INFO Training Finished!\n",
      "2021-12-06 23:22:01,037 INFO Predicting Val Probablities!\n",
      "2021-12-06 23:22:01,038 INFO Predicting Val Score!\n",
      "/Users/pankajmathur/anaconda3/envs/nbdev_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2403: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "2021-12-06 23:22:01,047 INFO fold: 3 roc_auc_score : 0.7733784655061251\n",
      "2021-12-06 23:22:01,048 INFO fold: 3 log_loss : inf\n",
      "2021-12-06 23:22:01,048 INFO fold: 3 accuracy_score : 0.72\n",
      "2021-12-06 23:22:01,049 INFO fold: 3 f1_score : 0.643312101910828\n",
      "2021-12-06 23:22:01,049 INFO fold: 3 precision_score : 0.6778523489932886\n",
      "2021-12-06 23:22:01,050 INFO fold: 3 recall_score : 0.6121212121212121\n",
      "2021-12-06 23:22:01,050 INFO Predicting Test Probablities!\n",
      "2021-12-06 23:22:01,056 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:22:01] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:22:09,796 INFO Training Finished!\n",
      "2021-12-06 23:22:09,797 INFO Predicting Val Probablities!\n",
      "2021-12-06 23:22:09,799 INFO Predicting Val Score!\n",
      "2021-12-06 23:22:09,806 INFO fold: 4 roc_auc_score : 0.789911040395777\n",
      "2021-12-06 23:22:09,807 INFO fold: 4 log_loss : 0.5406639995649312\n",
      "2021-12-06 23:22:09,808 INFO fold: 4 accuracy_score : 0.7425\n",
      "2021-12-06 23:22:09,808 INFO fold: 4 f1_score : 0.6644951140065146\n",
      "2021-12-06 23:22:09,809 INFO fold: 4 precision_score : 0.7208480565371025\n",
      "2021-12-06 23:22:09,810 INFO fold: 4 recall_score : 0.6163141993957704\n",
      "2021-12-06 23:22:09,810 INFO Predicting Test Probablities!\n",
      "2021-12-06 23:22:09,816 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:22:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { colsample_bytree, early_stopping_rounds, max_depth, subsample, tree_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:22:19,205 INFO Training Finished!\n",
      "2021-12-06 23:22:19,205 INFO Predicting Val Probablities!\n",
      "2021-12-06 23:22:19,207 INFO Predicting Val Score!\n",
      "2021-12-06 23:22:19,215 INFO fold: 5 roc_auc_score : 0.7993287769181713\n",
      "2021-12-06 23:22:19,215 INFO fold: 5 log_loss : 0.5347861251386348\n",
      "2021-12-06 23:22:19,216 INFO fold: 5 accuracy_score : 0.73375\n",
      "2021-12-06 23:22:19,216 INFO fold: 5 f1_score : 0.6490939044481056\n",
      "2021-12-06 23:22:19,217 INFO fold: 5 precision_score : 0.7137681159420289\n",
      "2021-12-06 23:22:19,217 INFO fold: 5 recall_score : 0.595166163141994\n",
      "2021-12-06 23:22:19,218 INFO Predicting Test Probablities!\n",
      "2021-12-06 23:22:19,224 INFO  Mean Metrics Results from all Folds are: {'roc_auc_score': 0.7890162419927703, 'log_loss': inf, 'accuracy_score': 0.7317499999999999, 'f1_score': 0.6545599654425693, 'precision_score': 0.7008874040127365, 'recall_score': 0.6156294058408863}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 36s, sys: 6.25 s, total: 5min 43s\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# k-fold training\n",
    "xgb_model_metrics_score, xgb_model_test_preds = tmlt.do_kfold_training(X_np,\n",
    "                                                                       y_np,\n",
    "                                                                       X_test=X_test_np,\n",
    "                                                                       n_splits=5,\n",
    "                                                                       model=updated_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "# predict on test dataset\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take weighted average of both k-fold models predictions\n",
    "# final_preds = ((0.45 * sci_model_preds) + (0.55* xgb_model_test_preds)) / 2\n",
    "# print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "# sub['target'] = final_preds\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "# # run the script to build \n",
    "\n",
    "# from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev_env",
   "language": "python",
   "name": "nbdev_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
