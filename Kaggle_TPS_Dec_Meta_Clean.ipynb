{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are using XGBClassifier, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/home/pankaj/kaggle_datasets/tpc_dec_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"kaggle_tps_dec_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:31:16,286 INFO 8 cores found, model and data parallel processing should worked!\n",
      "2021-12-20 16:31:32,266 INFO DataFrame Memory usage decreased to 274.66 Mb (83.9% reduction)\n",
      "2021-12-20 16:31:34,993 INFO DataFrame Memory usage decreased to 67.71 Mb (83.9% reduction)\n",
      "2021-12-20 16:31:35,288 INFO The least class label is :5 and value count is: 1\n",
      "2021-12-20 16:31:35,293 INFO The Original X shape is: (4000000, 55)\n",
      "2021-12-20 16:31:35,408 INFO The X shape after least class duplicates appends is: (4000021, 55)\n",
      "2021-12-20 16:31:37,071 INFO PreProcessing will include target(s) encoding!\n",
      "2021-12-20 16:31:37,180 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "# create tmlt\n",
    "tmlt = TMLT().prepare_data(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"Id\",\n",
    "    target=\"Cover_Type\",\n",
    "    random_state=42,\n",
    "    problem_type=\"multi_class_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000021, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000021,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1000000, 54)\n"
     ]
    }
   ],
   "source": [
    "print(type(tmlt.dfl.X))\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(type(tmlt.dfl.y))\n",
    "print(tmlt.dfl.y.shape)\n",
    "print(type(tmlt.dfl.X_test))\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2262087, 0: 1468136, 2: 195712, 6: 62261, 5: 11426, 3: 377, 4: 22}\n"
     ]
    }
   ],
   "source": [
    "print(dict(pd.Series(tmlt.dfl.y).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcess X, y and X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- and apply SMOTEENN combine technique (oversample+undersample) to resample imbalance classses -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000021, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000021,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1000000, 54)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_np, y_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.y, tmlt.dfl.X_test)\n",
    "print(X_np.shape)\n",
    "print(type(X_np))\n",
    "print(y_np.shape)\n",
    "print(type(y_np))\n",
    "print(X_test_np.shape)\n",
    "print(type(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2262087, 0: 1468136, 2: 195712, 6: 62261, 5: 11426, 3: 377, 4: 22}\n"
     ]
    }
   ],
   "source": [
    "print(dict(pd.Series(y_np).value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Meta Ensemble Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 1: TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:31:42,277 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.64566 | val_0_logloss: 0.7088  |  0:00:19s\n",
      "epoch 1  | loss: 0.42927 | val_0_logloss: 0.3567  |  0:00:39s\n",
      "epoch 2  | loss: 0.30277 | val_0_logloss: 0.31855 |  0:00:59s\n",
      "epoch 3  | loss: 0.27035 | val_0_logloss: 0.27764 |  0:01:19s\n",
      "epoch 4  | loss: 0.2512  | val_0_logloss: 0.24447 |  0:01:39s\n",
      "epoch 5  | loss: 0.23766 | val_0_logloss: 0.23266 |  0:01:58s\n",
      "epoch 6  | loss: 0.2281  | val_0_logloss: 0.22314 |  0:02:18s\n",
      "epoch 7  | loss: 0.21536 | val_0_logloss: 0.21099 |  0:02:38s\n",
      "epoch 8  | loss: 0.20126 | val_0_logloss: 0.21066 |  0:02:58s\n",
      "epoch 9  | loss: 0.19007 | val_0_logloss: 0.18278 |  0:03:17s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_0_logloss = 0.18278\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:35:19,302 INFO Training Finished!\n",
      "2021-12-20 16:35:24,966 INFO fold: 1 OOF Model Metrics: 0.1827772870072086!\n",
      "2021-12-20 16:35:29,120 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.21469 | val_0_logloss: 0.18665 |  0:00:19s\n",
      "epoch 1  | loss: 0.17476 | val_0_logloss: 0.16753 |  0:00:39s\n",
      "epoch 2  | loss: 0.16039 | val_0_logloss: 0.15348 |  0:00:58s\n",
      "epoch 3  | loss: 0.1508  | val_0_logloss: 0.14996 |  0:01:18s\n",
      "epoch 4  | loss: 0.14364 | val_0_logloss: 0.14239 |  0:01:38s\n",
      "epoch 5  | loss: 0.13699 | val_0_logloss: 0.13611 |  0:01:57s\n",
      "epoch 6  | loss: 0.13403 | val_0_logloss: 0.15121 |  0:02:17s\n",
      "epoch 7  | loss: 0.12962 | val_0_logloss: 0.12613 |  0:02:36s\n",
      "epoch 8  | loss: 0.12704 | val_0_logloss: 0.12459 |  0:02:56s\n",
      "epoch 9  | loss: 0.12414 | val_0_logloss: 0.12519 |  0:03:16s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_val_0_logloss = 0.12459\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:39:01,776 INFO Training Finished!\n",
      "2021-12-20 16:39:07,405 INFO fold: 2 OOF Model Metrics: 0.12459373409085422!\n",
      "2021-12-20 16:39:11,501 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.1597  | val_0_logloss: 0.13646 |  0:00:19s\n",
      "epoch 1  | loss: 0.13051 | val_0_logloss: 0.12656 |  0:00:39s\n",
      "epoch 2  | loss: 0.12371 | val_0_logloss: 0.12304 |  0:00:58s\n",
      "epoch 3  | loss: 0.11951 | val_0_logloss: 0.11906 |  0:01:18s\n",
      "epoch 4  | loss: 0.11653 | val_0_logloss: 0.11701 |  0:01:37s\n",
      "epoch 5  | loss: 0.11451 | val_0_logloss: 0.11349 |  0:01:57s\n",
      "epoch 6  | loss: 0.1125  | val_0_logloss: 0.11324 |  0:02:16s\n",
      "epoch 7  | loss: 0.11117 | val_0_logloss: 0.1102  |  0:02:36s\n",
      "epoch 8  | loss: 0.10994 | val_0_logloss: 0.1094  |  0:02:55s\n",
      "epoch 9  | loss: 0.10875 | val_0_logloss: 0.11873 |  0:03:15s\n",
      "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_val_0_logloss = 0.1094\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:42:43,502 INFO Training Finished!\n",
      "2021-12-20 16:42:49,171 INFO fold: 3 OOF Model Metrics: 0.10939878690928634!\n",
      "2021-12-20 16:42:53,340 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.13354 | val_0_logloss: 0.11966 |  0:00:19s\n",
      "epoch 1  | loss: 0.11402 | val_0_logloss: 0.11186 |  0:00:39s\n",
      "epoch 2  | loss: 0.1095  | val_0_logloss: 0.10926 |  0:00:58s\n",
      "epoch 3  | loss: 0.10741 | val_0_logloss: 0.10641 |  0:01:18s\n",
      "epoch 4  | loss: 0.1081  | val_0_logloss: 0.10683 |  0:01:38s\n",
      "epoch 5  | loss: 0.10605 | val_0_logloss: 0.10861 |  0:01:57s\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 3 and best_val_0_logloss = 0.10641\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:45:07,781 INFO Training Finished!\n",
      "2021-12-20 16:45:13,410 INFO fold: 4 OOF Model Metrics: 0.10640530068344113!\n",
      "2021-12-20 16:45:17,495 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.13787 | val_0_logloss: 0.12041 |  0:00:19s\n",
      "epoch 1  | loss: 0.1122  | val_0_logloss: 0.11299 |  0:00:39s\n",
      "epoch 2  | loss: 0.10739 | val_0_logloss: 0.12172 |  0:00:59s\n",
      "epoch 3  | loss: 0.10551 | val_0_logloss: 0.10786 |  0:01:18s\n",
      "epoch 4  | loss: 0.10471 | val_0_logloss: 0.10572 |  0:01:38s\n",
      "epoch 5  | loss: 0.10364 | val_0_logloss: 0.11127 |  0:01:57s\n",
      "epoch 6  | loss: 0.10259 | val_0_logloss: 0.10323 |  0:02:17s\n",
      "epoch 7  | loss: 0.10212 | val_0_logloss: 0.10639 |  0:02:36s\n",
      "epoch 8  | loss: 0.10176 | val_0_logloss: 0.10419 |  0:02:56s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 6 and best_val_0_logloss = 0.10323\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:48:30,510 INFO Training Finished!\n",
      "2021-12-20 16:48:36,165 INFO fold: 5 OOF Model Metrics: 0.1032339081470943!\n",
      "2021-12-20 16:48:39,778 INFO Mean OOF Model Metrics: 0.1252818033675769!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000021,)\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#TOD0: Add tabnet_params and pass through do_oof_kfold_train_preds, keep it optional for non tabnet models\n",
    "# GOTO tmlt update method with tabnet_params\n",
    "tabnet_params = {\n",
    "    'max_epochs': 10,\n",
    "    'patience': 2,\n",
    "    'batch_size': 4096*6*tmlt.IDEAL_CPU_CORES,\n",
    "    'virtual_batch_size' : 512*6*tmlt.IDEAL_CPU_CORES\n",
    "}\n",
    "\n",
    "#choose model\n",
    "tabnet_oof_model = TabNetClassifier(optimizer_params=dict(lr=0.02), verbose=1)\n",
    "\n",
    "#fit and predict\n",
    "tabnet_oof_model_preds, tabnet_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    model=tabnet_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np,\n",
    "                                                                                    tabnet_params=tabnet_params)\n",
    "gc.collect()\n",
    "\n",
    "if tabnet_oof_model_preds is not None:\n",
    "    print(tabnet_oof_model_preds.shape)\n",
    "\n",
    "if tabnet_oof_model_test_preds is not None:\n",
    "    print(tabnet_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add back based models predictions to X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000021, 55)\n",
      "(1000000, 55)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"tabnet_preds\"] = tabnet_oof_model_preds\n",
    "tmlt.dfl.X_test[\"tabnet_preds\"] = tabnet_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now just update the tmlt with this new X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:48:41,714 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "tmlt = tmlt.update_dfl(X=tmlt.dfl.X, y=tmlt.dfl.y, X_test=tmlt.dfl.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For META Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess updated X, y, X_test\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000021, 55)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000021,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1000000, 55)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_np, y_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.y, tmlt.dfl.X_test)\n",
    "\n",
    "print(X_np.shape)\n",
    "print(type(X_np))\n",
    "print(y_np.shape)\n",
    "print(type(y_np))\n",
    "print(X_test_np.shape)\n",
    "print(type(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Meta Model, Let's do Optuna based HyperParameter search to get best params for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200016, 55)\n",
      "<class 'numpy.ndarray'>\n",
      "(3200016,)\n",
      "<class 'numpy.ndarray'>\n",
      "(800005, 55)\n",
      "<class 'numpy.ndarray'>\n",
      "(800005,)\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 1.92 s, sys: 92 ms, total: 2.01 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train_np, X_valid_np,  y_train_np, y_valid_np =  tmlt.dfl.create_train_valid(X_np, y_np, valid_size=0.2)\n",
    "\n",
    "print(X_train_np.shape)\n",
    "print(type(X_train_np))\n",
    "print(y_train_np.shape)\n",
    "print(type(y_train_np))\n",
    "print(X_valid_np.shape)\n",
    "print(type(X_valid_np))\n",
    "print(y_valid_np.shape)\n",
    "print(type(y_valid_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 19:40:02,965 INFO Optimization Direction is: maximize\n",
      "\u001b[32m[I 2021-12-20 19:40:02,980]\u001b[0m Using an existing study with name 'tmlt_autoxgb' instead of creating a new one.\u001b[0m\n",
      "2021-12-20 19:40:03,296 INFO final params {'learning_rate': 0.08927538038963782, 'n_estimators': 70, 'reg_lambda': 1.3729952949458818e-06, 'reg_alpha': 1.660148933599981e-06, 'subsample': 0.12798062790415102, 'colsample_bytree': 0.6236595537308991, 'max_depth': 3, 'tree_method': 'hist', 'booster': 'gbtree', 'gamma': 5.642432394837139e-05, 'grow_policy': 'lossguide'}\n",
      "2021-12-20 19:40:03,297 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:40:04] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"eval_set\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 19:41:12,732 INFO Training Ended!\n",
      "2021-12-20 19:41:13,502 INFO accuracy_score: 0.9462415859900876\n",
      "\u001b[32m[I 2021-12-20 19:41:13,537]\u001b[0m Trial 1 finished with value: 0.9462415859900876 and parameters: {'learning_rate': 0.08927538038963782, 'n_estimators': 70, 'reg_lambda': 1.3729952949458818e-06, 'reg_alpha': 1.660148933599981e-06, 'subsample': 0.12798062790415102, 'colsample_bytree': 0.6236595537308991, 'max_depth': 3, 'early_stopping_rounds': 424, 'tree_method': 'hist', 'booster': 'gbtree', 'gamma': 5.642432394837139e-05, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.9462415859900876.\u001b[0m\n",
      "2021-12-20 19:41:13,789 INFO final params {'learning_rate': 0.03498153801071581, 'n_estimators': 70, 'reg_lambda': 8.689474220867741e-07, 'reg_alpha': 0.0001144337197787049, 'subsample': 0.7678175053610523, 'colsample_bytree': 0.7144759497123172, 'max_depth': 7, 'tree_method': 'approx', 'booster': 'gbtree', 'gamma': 9.826303946378805e-06, 'grow_policy': 'lossguide'}\n",
      "2021-12-20 19:41:13,790 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:41:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"eval_set\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:13:19,376 INFO Training Ended!\n",
      "2021-12-20 20:13:21,039 INFO accuracy_score: 0.9520665495840651\n",
      "\u001b[32m[I 2021-12-20 20:13:21,069]\u001b[0m Trial 2 finished with value: 0.9520665495840651 and parameters: {'learning_rate': 0.03498153801071581, 'n_estimators': 70, 'reg_lambda': 8.689474220867741e-07, 'reg_alpha': 0.0001144337197787049, 'subsample': 0.7678175053610523, 'colsample_bytree': 0.7144759497123172, 'max_depth': 7, 'early_stopping_rounds': 111, 'tree_method': 'approx', 'booster': 'gbtree', 'gamma': 9.826303946378805e-06, 'grow_policy': 'lossguide'}. Best is trial 2 with value: 0.9520665495840651.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=2, values=[0.9520665495840651], datetime_start=datetime.datetime(2021, 12, 20, 19, 41, 13, 543204), datetime_complete=datetime.datetime(2021, 12, 20, 20, 13, 21, 40156), params={'booster': 'gbtree', 'colsample_bytree': 0.7144759497123172, 'early_stopping_rounds': 111, 'gamma': 9.826303946378805e-06, 'grow_policy': 'lossguide', 'learning_rate': 0.03498153801071581, 'max_depth': 7, 'n_estimators': 70, 'reg_alpha': 0.0001144337197787049, 'reg_lambda': 8.689474220867741e-07, 'subsample': 0.7678175053610523, 'tree_method': 'approx'}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear')), 'colsample_bytree': UniformDistribution(high=1.0, low=0.1), 'early_stopping_rounds': IntUniformDistribution(high=500, low=100, step=1), 'gamma': LogUniformDistribution(high=1.0, low=1e-08), 'grow_policy': CategoricalDistribution(choices=('depthwise', 'lossguide')), 'learning_rate': LogUniformDistribution(high=0.25, low=0.01), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'n_estimators': CategoricalDistribution(choices=(70, 150, 200)), 'reg_alpha': LogUniformDistribution(high=100.0, low=1e-08), 'reg_lambda': LogUniformDistribution(high=100.0, low=1e-08), 'subsample': UniformDistribution(high=1.0, low=0.1), 'tree_method': CategoricalDistribution(choices=('exact', 'approx', 'hist'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=3, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "# **Just make sure to supply an output directory path so hyperparameter search is saved**\n",
    "study = tmlt.do_xgb_optuna_optimization(X_train_np, y_train_np, X_valid_np, y_valid_np,\n",
    "                                        optuna_db_path=OUTPUT_PATH, opt_timeout=720)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb_params {'use_label_encoder': False, 'learning_rate': 0.03498153801071581, 'n_estimators': 70, 'reg_lambda': 8.689474220867741e-07, 'reg_alpha': 0.0001144337197787049, 'subsample': 0.7678175053610523, 'colsample_bytree': 0.7144759497123172, 'max_depth': 7, 'tree_method': 'approx', 'gpu_id': 0, 'predictor': 'gpu_predictor', 'early_stopping_rounds': 111, 'booster': 'gbtree', 'gamma': 9.826303946378805e-06, 'grow_policy': 'lossguide'}\n"
     ]
    }
   ],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "# xgb_params.update({'n_estimators': 1500})\n",
    "print(\"xgb_params\", xgb_params)\n",
    "updated_xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:13:22,169 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:13:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:17:10,145 INFO Training Finished!\n",
      "2021-12-20 20:17:10,146 INFO Predicting Val Probablities!\n",
      "2021-12-20 20:17:11,832 INFO Predicting Val Score!\n",
      "2021-12-20 20:17:13,534 INFO fold: 1 accuracy_score : 0.9551077805763714\n",
      "2021-12-20 20:17:13,535 INFO Predicting Test Scores!\n",
      "2021-12-20 20:17:16,293 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:17:17] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:21:05,119 INFO Training Finished!\n",
      "2021-12-20 20:21:05,120 INFO Predicting Val Probablities!\n",
      "2021-12-20 20:21:06,816 INFO Predicting Val Score!\n",
      "2021-12-20 20:21:08,525 INFO fold: 2 accuracy_score : 0.9586202068989655\n",
      "2021-12-20 20:21:08,526 INFO Predicting Test Scores!\n",
      "2021-12-20 20:21:11,276 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:21:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:25:00,295 INFO Training Finished!\n",
      "2021-12-20 20:25:00,295 INFO Predicting Val Probablities!\n",
      "2021-12-20 20:25:01,971 INFO Predicting Val Score!\n",
      "2021-12-20 20:25:03,670 INFO fold: 3 accuracy_score : 0.9590564547177264\n",
      "2021-12-20 20:25:03,671 INFO Predicting Test Scores!\n",
      "2021-12-20 20:25:06,434 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:25:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:28:55,225 INFO Training Finished!\n",
      "2021-12-20 20:28:55,226 INFO Predicting Val Probablities!\n",
      "2021-12-20 20:28:56,923 INFO Predicting Val Score!\n",
      "2021-12-20 20:28:58,643 INFO fold: 4 accuracy_score : 0.9592277038614807\n",
      "2021-12-20 20:28:58,643 INFO Predicting Test Scores!\n",
      "2021-12-20 20:29:01,411 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:02] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 20:32:50,323 INFO Training Finished!\n",
      "2021-12-20 20:32:50,324 INFO Predicting Val Probablities!\n",
      "2021-12-20 20:32:52,008 INFO Predicting Val Score!\n",
      "2021-12-20 20:32:53,715 INFO fold: 5 accuracy_score : 0.9588027059864701\n",
      "2021-12-20 20:32:53,716 INFO Predicting Test Scores!\n",
      "2021-12-20 20:32:55,905 INFO  Mean Metrics Results from all Folds are: {'accuracy_score': 0.9581629704082029}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 4s, sys: 6.21 s, total: 20min 10s\n",
      "Wall time: 19min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# k-fold training\n",
    "xgb_model_metrics_score, xgb_model_test_preds = tmlt.do_kfold_training(X_np,\n",
    "                                                                       y_np,\n",
    "                                                                       X_test=X_test_np,\n",
    "                                                                       n_splits=5,\n",
    "                                                                       model=xgb_model)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021-12-20 17:08:30,867 INFO Training Finished!\n",
    "2021-12-20 17:08:30,868 INFO Predicting Val Probablities!\n",
    "2021-12-20 17:08:32,559 INFO Predicting Val Score!\n",
    "2021-12-20 17:08:34,262 INFO fold: 5 accuracy_score : 0.9588027059864701\n",
    "2021-12-20 17:08:34,263 INFO Predicting Test Scores!\n",
    "2021-12-20 17:08:36,422 INFO  Mean Metrics Results from all Folds are: {'accuracy_score': 0.9581629704082029}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "# predict on test dataset\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabnet_oof_model_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "test_preds = xgb_model_test_preds\n",
    "print(type(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1,\n",
       "       2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 3, 6, 1, 0, 1, 1,\n",
       "       1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 6, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 4, 3, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 5, 0, 1, 1, 0, 0, 1, 0, 1, 5, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 4, 0, 0, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 2, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 2,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 1, 1, 2, 1, 0, 2, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 1, 2, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 2, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0,\n",
       "       1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 6, 0, 2, 1, 0, 1, 1,\n",
       "       0, 2, 1, 1, 2, 0, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 1, 1, 1,\n",
       "       0, 4, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 0,\n",
       "       1, 0, 1, 1, 2, 1, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 1,\n",
       "       2, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 2, 1, 1, 2, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1, 6, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 6, 0, 2, 1,\n",
       "       1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 2, 0, 1, 6, 1, 1, 1, 0, 6, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 1, 2, 1, 0, 2,\n",
       "       0, 0, 0, 1, 2, 1, 2, 0, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1,\n",
       "       0, 6, 0, 1, 1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 6, 1, 4, 0, 0, 1, 1, 1, 6, 2, 1, 1, 0, 2, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 1, 2, 0, 2, 0,\n",
       "       0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 4, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 2,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_round = np.around(test_preds).astype(int)\n",
    "test_preds_round[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 516503, 0: 383003, 2: 77484, 6: 9688, 5: 4584, 4: 4464, 3: 4274}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dict(pd.Series(test_preds_round).value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# target encoding changes 1 to 7 classes to 0 to 6\n",
    "test_preds_round = test_preds_round + 1\n",
    "print(type(test_preds_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 516503, 1: 383003, 3: 77484, 7: 9688, 6: 4584, 5: 4464, 4: 4274}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dict(pd.Series(test_preds_round).value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mon_dec_20_2209_submission.csv saved!\n"
     ]
    }
   ],
   "source": [
    "submission_file_name = 'mon_dec_20_2209_submission.csv'\n",
    "\n",
    "sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "sub['Cover_Type'] = test_preds_round\n",
    "\n",
    "sub.to_csv(submission_file_name, index=False)\n",
    "print(f\"{submission_file_name} saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
