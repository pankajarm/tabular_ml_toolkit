{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Kaggle TPS Challenge with Tabular ML Toolkit\n",
    "\n",
    "> A Tutorial to showcase usage of tabular_ml_toolkit (tmlt) library on Kaggle TPS Challenge Nov 2021.\n",
    "\n",
    "> tabular_ml_toolkit is a helper library to jumpstart your machine learning project based on Tabular or Structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter search techniques.\n",
    "\n",
    "> Under the hood TMLT uses optuna, xgboost and scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create **tmlt** with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are using XGBClassifier, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/home/pankaj/kaggle_datasets/tpc_dec_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"kaggle_tps_dec_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just point tmlt in the direction of your data\n",
    "\n",
    "#### Let it know what are idx and target columns in your tabular data\n",
    "\n",
    "#### what kind of problem type you are trying to resolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 17:02:33,026 INFO 8 cores found, model and data parallel processing should worked!\n",
      "2021-12-15 17:02:49,147 INFO DataFrame Memory usage decreased to 274.66 Mb (83.9% reduction)\n",
      "2021-12-15 17:02:51,956 INFO DataFrame Memory usage decreased to 67.71 Mb (83.9% reduction)\n",
      "2021-12-15 17:02:52,264 INFO The least class label is :5 and value count is: 1\n",
      "2021-12-15 17:02:52,268 INFO The time took to concat 12 rows: 0.001493215560913086\n",
      "2021-12-15 17:02:52,268 INFO The X shape BEFORE append is: (4000000, 55)\n",
      "2021-12-15 17:02:52,385 INFO The time took to append 1 dataframe to existing one!: 0.11675643920898438\n",
      "2021-12-15 17:02:52,386 INFO The X shape AFTER append is: (4000012, 55)\n",
      "2021-12-15 17:02:54,021 INFO PreProcessing will include target(s) encoding!\n",
      "2021-12-15 17:02:54,129 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "# create tmlt\n",
    "tmlt = TMLT().prepare_data(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"Id\",\n",
    "    target=\"Cover_Type\",\n",
    "    random_state=42,\n",
    "    problem_type=\"multi_class_classification\",\n",
    "#     nrows=4000\n",
    ")\n",
    "\n",
    "\n",
    "# tmlt supports only below task type:\n",
    "    # \"binary_classification\"\n",
    "    # \"multi_label_classification\"\n",
    "    # \"multi_class_classification\"\n",
    "    # \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000012, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000012,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1000000, 54)\n"
     ]
    }
   ],
   "source": [
    "print(type(tmlt.dfl.X))\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(type(tmlt.dfl.y))\n",
    "print(tmlt.dfl.y.shape)\n",
    "print(type(tmlt.dfl.X_test))\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type31</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3189</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>3270</td>\n",
       "      <td>206</td>\n",
       "      <td>234</td>\n",
       "      <td>193</td>\n",
       "      <td>4873</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3026</td>\n",
       "      <td>182</td>\n",
       "      <td>5</td>\n",
       "      <td>280</td>\n",
       "      <td>29</td>\n",
       "      <td>3270</td>\n",
       "      <td>233</td>\n",
       "      <td>240</td>\n",
       "      <td>106</td>\n",
       "      <td>5423</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3106</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>351</td>\n",
       "      <td>37</td>\n",
       "      <td>2914</td>\n",
       "      <td>208</td>\n",
       "      <td>234</td>\n",
       "      <td>137</td>\n",
       "      <td>5269</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3022</td>\n",
       "      <td>276</td>\n",
       "      <td>13</td>\n",
       "      <td>192</td>\n",
       "      <td>16</td>\n",
       "      <td>3034</td>\n",
       "      <td>207</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>2866</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2906</td>\n",
       "      <td>186</td>\n",
       "      <td>13</td>\n",
       "      <td>266</td>\n",
       "      <td>22</td>\n",
       "      <td>2916</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>154</td>\n",
       "      <td>2642</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000007</th>\n",
       "      <td>2953</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>981</td>\n",
       "      <td>181</td>\n",
       "      <td>209</td>\n",
       "      <td>184</td>\n",
       "      <td>7633</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000008</th>\n",
       "      <td>2953</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>981</td>\n",
       "      <td>181</td>\n",
       "      <td>209</td>\n",
       "      <td>184</td>\n",
       "      <td>7633</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000009</th>\n",
       "      <td>2953</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>981</td>\n",
       "      <td>181</td>\n",
       "      <td>209</td>\n",
       "      <td>184</td>\n",
       "      <td>7633</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000010</th>\n",
       "      <td>2953</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>981</td>\n",
       "      <td>181</td>\n",
       "      <td>209</td>\n",
       "      <td>184</td>\n",
       "      <td>7633</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000011</th>\n",
       "      <td>2953</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>981</td>\n",
       "      <td>181</td>\n",
       "      <td>209</td>\n",
       "      <td>184</td>\n",
       "      <td>7633</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000012 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0             3189      40      8                                30   \n",
       "1             3026     182      5                               280   \n",
       "2             3106      13      7                               351   \n",
       "3             3022     276     13                               192   \n",
       "4             2906     186     13                               266   \n",
       "...            ...     ...    ...                               ...   \n",
       "4000007       2953     114     39                                97   \n",
       "4000008       2953     114     39                                97   \n",
       "4000009       2953     114     39                                97   \n",
       "4000010       2953     114     39                                97   \n",
       "4000011       2953     114     39                                97   \n",
       "\n",
       "         Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                    13                             3270   \n",
       "1                                    29                             3270   \n",
       "2                                    37                             2914   \n",
       "3                                    16                             3034   \n",
       "4                                    22                             2916   \n",
       "...                                 ...                              ...   \n",
       "4000007                             111                              981   \n",
       "4000008                             111                              981   \n",
       "4000009                             111                              981   \n",
       "4000010                             111                              981   \n",
       "4000011                             111                              981   \n",
       "\n",
       "         Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                  206             234            193   \n",
       "1                  233             240            106   \n",
       "2                  208             234            137   \n",
       "3                  207             238            156   \n",
       "4                  231             231            154   \n",
       "...                ...             ...            ...   \n",
       "4000007            181             209            184   \n",
       "4000008            181             209            184   \n",
       "4000009            181             209            184   \n",
       "4000010            181             209            184   \n",
       "4000011            181             209            184   \n",
       "\n",
       "         Horizontal_Distance_To_Fire_Points  ...  Soil_Type31  Soil_Type32  \\\n",
       "0                                      4873  ...            0            0   \n",
       "1                                      5423  ...            0            0   \n",
       "2                                      5269  ...            0            0   \n",
       "3                                      2866  ...            0            0   \n",
       "4                                      2642  ...            0            0   \n",
       "...                                     ...  ...          ...          ...   \n",
       "4000007                                7633  ...            0            0   \n",
       "4000008                                7633  ...            0            0   \n",
       "4000009                                7633  ...            0            0   \n",
       "4000010                                7633  ...            0            0   \n",
       "4000011                                7633  ...            0            0   \n",
       "\n",
       "         Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0                  0            0            0            0            0   \n",
       "1                  0            0            0            0            0   \n",
       "2                  0            0            0            0            0   \n",
       "3                  0            0            0            0            0   \n",
       "4                  0            0            0            0            0   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "4000007            0            0            0            0            0   \n",
       "4000008            0            0            0            0            0   \n",
       "4000009            0            0            0            0            0   \n",
       "4000010            0            0            0            0            0   \n",
       "4000011            0            0            0            0            0   \n",
       "\n",
       "         Soil_Type38  Soil_Type39  Soil_Type40  \n",
       "0                  0            0            0  \n",
       "1                  0            0            0  \n",
       "2                  0            0            0  \n",
       "3                  0            0            0  \n",
       "4                  0            0            0  \n",
       "...              ...          ...          ...  \n",
       "4000007            0            0            0  \n",
       "4000008            0            0            0  \n",
       "4000009            0            0            0  \n",
       "4000010            0            0            0  \n",
       "4000011            0            0            0  \n",
       "\n",
       "[4000012 rows x 54 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlt.dfl.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2262087, 0: 1468136, 2: 195712, 6: 62261, 5: 11426, 3: 377, 4: 13}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(dict(pd.Series(tmlt.dfl.y).value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 984 ms, sys: 0 ns, total: 984 ms\n",
      "Wall time: 983 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200009, 54)\n",
      "(3200009,)\n",
      "(800003, 54)\n",
      "(800003,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1809818, 0: 1174197, 2: 156714, 6: 49866, 5: 9111, 3: 293, 4: 10}\n",
      "{1: 452269, 0: 293939, 2: 38998, 6: 12395, 5: 2315, 3: 84, 4: 3}\n"
     ]
    }
   ],
   "source": [
    "# check for class values see if both train and valid have same class labels\n",
    "print(dict(pd.Series(y_train).value_counts()))\n",
    "print(dict(pd.Series(y_valid).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200009, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "(800003, 54)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 2.47 s, sys: 536 ms, total: 3.01 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base xgb classifier model with your best guess params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    # your best guess params\n",
    "    'learning_rate':0.1,\n",
    "    'eval_metric':'mlogloss',\n",
    "    # must for xgb classifier otherwise warning will be shown\n",
    "    'use_label_encoder':False,\n",
    "    # because 42 is the answer for all the randomness of this universe\n",
    "    'random_state':42,\n",
    "    #for GPU\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 548 ms, total: 19.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a {color: black;background-color: white;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a pre{padding: 0;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-toggleable {background-color: white;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-estimator:hover {background-color: #d4ebff;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-item {z-index: 1;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-parallel-item:only-child::after {width: 0;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-6edc36b8-0d15-42e3-83f9-d68bcf19bc3a\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"67a6ea15-ec10-4fec-86ed-a33e4cde09f7\" type=\"checkbox\" checked><label class=\"sk-toggleable__label\" for=\"67a6ea15-ec10-4fec-86ed-a33e4cde09f7\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='mlogloss', gamma=0, gpu_id=0, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='gpu_predictor', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='gpu_hist', use_label_encoder=False,\n",
       "              validate_parameters=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='mlogloss', gamma=0, gpu_id=0, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='gpu_predictor', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='gpu_hist', use_label_encoder=False,\n",
       "              validate_parameters=1, ...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_valid_np, y_valid)],\n",
    "              eval_metric=\"mlogloss\",\n",
    "              early_stopping_rounds=300\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.997025867656879 while Accuracy is : 0.95200267998995 \n",
      "CPU times: user 4.23 s, sys: 147 ms, total: 4.37 s\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs, multi_class='ovr')\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Meta Ensemble Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure to PreProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.73 s, sys: 668 ms, total: 3.4 s\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 1: linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # OOF training and prediction on both train and test dataset by a given model\n",
    "# #choose model\n",
    "# linear_oof_model = LinearSVC(tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=42)\n",
    "\n",
    "# #fit and predict\n",
    "# linear_oof_model_preds, linear_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "#                                                                                     model=linear_oof_model,\n",
    "#                                                                                     X = X_np,\n",
    "#                                                                                     y = y_np,\n",
    "#                                                                                     X_test = X_test_np)\n",
    "\n",
    "# if linear_oof_model_preds is not None:\n",
    "#     print(linear_oof_model_preds.shape)\n",
    "\n",
    "# if linear_oof_model_test_preds is not None:    \n",
    "#     print(linear_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 2: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "# #choose model\n",
    "# log_oof_model = LogisticRegression(multi_class='multinomial', solver='lbfg', random_state=42)\n",
    "\n",
    "# #fit and predict\n",
    "# log_oof_model_preds, log_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "#                                                                                     model=log_oof_model,\n",
    "#                                                                                     X = X_np,\n",
    "#                                                                                     y = y_np,\n",
    "#                                                                                     X_test = X_test_np)\n",
    "# if log_oof_model_preds is not None:\n",
    "#     print(log_oof_model_preds.shape)\n",
    "\n",
    "# if log_oof_model_test_preds is not None:    \n",
    "#     print(log_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 3: SKLearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "# #choose model\n",
    "# mlp_oof_model = MLPClassifier(max_iter=1000, early_stopping=True)\n",
    "\n",
    "# #update the model on sklearn pipeline\n",
    "# # tmlt = tmlt.update_model(mlp_oof_model)\n",
    "\n",
    "# # # lets see updated sklearn pipeline with new model\n",
    "# # tmlt.spl\n",
    "\n",
    "# #fit and predict\n",
    "# mlp_oof_model_preds, mlp_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "#                                                                                     model=mlp_oof_model,\n",
    "#                                                                                     X = X_np,\n",
    "#                                                                                     y = y_np,\n",
    "#                                                                                     X_test = X_test_np)\n",
    "# if mlp_oof_model_preds is not None:\n",
    "#     print(mlp_oof_model_preds.shape)\n",
    "\n",
    "# if mlp_oof_model_test_preds is not None:    \n",
    "#     print(mlp_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 4: TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 17:03:20,336 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.53432 | val_0_logloss: 0.24197 |  0:00:23s\n",
      "epoch 1  | loss: 0.2018  | val_0_logloss: 0.17972 |  0:00:47s\n",
      "epoch 2  | loss: 0.16707 | val_0_logloss: 0.15196 |  0:01:10s\n",
      "epoch 3  | loss: 0.14941 | val_0_logloss: 0.14011 |  0:01:33s\n",
      "epoch 4  | loss: 0.14068 | val_0_logloss: 0.13515 |  0:01:56s\n",
      "Stop training because you reached max_epochs = 5 with best_epoch = 4 and best_val_0_logloss = 0.13515\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 17:05:37,806 INFO Training Finished!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/tabular_ml_toolkit/tabular_ml_toolkit/tmlt.py\u001b[0m in \u001b[0;36mdo_oof_kfold_train_preds\u001b[0;34m(self, X, y, n_splits, model, X_test, random_state)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;31m# Getting linear model metric results for each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0moof_model_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_preds_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fold: {n+1} OOF Model Metrics: {oof_model_metrics}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# for mean score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nbdev/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nbdev/lib/python3.9/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     95\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "tabnet_oof_model = TabNetClassifier(optimizer_params=dict(lr=0.02), verbose=1)\n",
    "\n",
    "#fit and predict\n",
    "tabnet_oof_model_preds, tabnet_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    model=tabnet_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if tabnet_oof_model_preds is not None:\n",
    "    print(tabnet_oof_model_preds.shape)\n",
    "\n",
    "if tabnet_oof_model_test_preds is not None:\n",
    "    print(tabnet_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add back based models predictions to X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add based model oof predictions back to X and X_test before Meta model training\n",
    "# tmlt.dfl.X[\"linear_preds\"] = linear_oof_model_preds\n",
    "# tmlt.dfl.X_test[\"linear_preds\"] = linear_oof_model_test_preds\n",
    "\n",
    "# print(tmlt.dfl.X.shape)\n",
    "# print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add based model oof predictions back to X and X_test before Meta model training\n",
    "# tmlt.dfl.X[\"log_reg_preds\"] = log_oof_model_preds\n",
    "# tmlt.dfl.X_test[\"log_reg_preds\"] = log_oof_model_test_preds\n",
    "\n",
    "# print(tmlt.dfl.X.shape)\n",
    "# print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add based model oof predictions back to X and X_test before Meta model training\n",
    "# tmlt.dfl.X[\"mlp_preds\"] = mlp_oof_model_preds\n",
    "# tmlt.dfl.X_test[\"mlp_preds\"] = mlp_oof_model_test_preds\n",
    "\n",
    "# print(tmlt.dfl.X.shape)\n",
    "# print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tabnet_oof_model_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21588/1035022344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# add based model oof predictions back to X and X_test before Meta model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tabnet_preds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabnet_oof_model_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtmlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tabnet_preds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabnet_oof_model_test_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tabnet_oof_model_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"tabnet_preds\"] = tabnet_oof_model_preds\n",
    "tmlt.dfl.X_test[\"tabnet_preds\"] = tabnet_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just update the tmlt with this new X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmlt = tmlt.update_dfl(X=tmlt.dfl.X, y=tmlt.dfl.y, X_test=tmlt.dfl.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For META Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_params = {\n",
    "#     'objective': 'binary:logistic', \n",
    "#     'use_label_encoder': False,\n",
    "#     'n_estimators': 40000,\n",
    "#     'learning_rate': 0.18515462875481553,\n",
    "#     'subsample': 0.97, \n",
    "#     'colsample_bytree': 0.32,\n",
    "#     'max_depth': 1,\n",
    "#     'booster': 'gbtree',\n",
    "#     'gamma': 0.2, \n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'reg_lambda': 0.11729916523488974, \n",
    "#     'reg_alpha': 0.6318827156945853,\n",
    "#     'random_state': 42,\n",
    "#     'n_jobs': 4, \n",
    "#     'min_child_weight': 256,\n",
    "#     #for GPU\n",
    "# #     'tree_method': 'gpu_hist',\n",
    "# #     'predictor': 'gpu_predictor',\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': 0.21761562020600114,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42,\n",
    "    'booster': 'gblinear',\n",
    "    'colsample_bytree': 0.1027132584989078,\n",
    "    'early_stopping_rounds': 171,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 7000,\n",
    "    'reg_alpha': 9.583579660175245e-06,\n",
    "    'reg_lambda': 9.238315962782784e-05,\n",
    "    'subsample': 0.4464473710560276,\n",
    "    'tree_method': 'approx',\n",
    "    #for GPU\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"mlogloss\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs, multi_class='ovr')\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOW!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Meta Model, Let's do Optuna based HyperParameter search to get best params for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Just make sure to supply an output directory path so hyperparameter search is saved**\n",
    "study = tmlt.do_xgb_optuna_optimization(optuna_db_path=OUTPUT_PATH, opt_timeout=360)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now update the meta model with best params from study and then update the sklearn pipeline with this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "print(\"xgb_params\", xgb_params)\n",
    "updated_xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# k-fold training\n",
    "xgb_model_metrics_score, xgb_model_test_preds = tmlt.do_kfold_training(X_np,\n",
    "                                                                       y_np,\n",
    "                                                                       X_test=X_test_np,\n",
    "                                                                       n_splits=5,\n",
    "                                                                       model=updated_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test dataset\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take weighted average of both k-fold models predictions\n",
    "# final_preds = ((0.45 * sci_model_preds) + (0.55* xgb_model_test_preds)) / 2\n",
    "# print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "# sub['target'] = xgb_model_test_preds\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
