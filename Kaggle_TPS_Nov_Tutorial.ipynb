{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Kaggle TPS Challenge with Tabular ML Toolkit\n",
    "\n",
    "> A Tutorial to showcase usage of tabular_ml_toolkit (tmlt) library on Kaggle TPS Challenge Nov 2021.\n",
    "\n",
    "> tabular_ml_toolkit is a helper library to jumpstart your machine learning project based on Tabular or Structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter search techniques.\n",
    "\n",
    "> Under the hood TMLT uses optuna, xgboost and scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create **tmlt** with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are using XGBClassifier, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/Users/pamathur/kaggle_datasets/tps_nov_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"kaggle_tps_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just point tmlt in the direction of your data\n",
    "\n",
    "#### Let it know what are idx and target columns in your tabular data\n",
    "\n",
    "#### what kind of problem type you are trying to resolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:34:48,624 INFO 12 cores found, model and data parallel processing should worked!\n",
      "2021-12-13 15:34:48,841 INFO DataFrame Memory usage decreased to 0.80 Mb (74.4% reduction)\n",
      "2021-12-13 15:34:49,077 INFO DataFrame Memory usage decreased to 0.79 Mb (74.3% reduction)\n",
      "2021-12-13 15:34:49,247 INFO PreProcessing will include target(s) encoding!\n",
      "2021-12-13 15:34:49,249 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "# create tmlt\n",
    "tmlt = TMLT().prepare_data(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"id\",\n",
    "    target=\"target\",\n",
    "    random_state=42,\n",
    "    problem_type=\"binary_classification\",\n",
    "    nrows=4000\n",
    ")\n",
    "\n",
    "\n",
    "# supports only task type\n",
    "# \"binary_classification\"\n",
    "# \"multi_label_classification\"\n",
    "# \"multi_class_classification\"\n",
    "# \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "(4000,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(4000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(tmlt.dfl.X))\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(type(tmlt.dfl.y))\n",
    "print(tmlt.dfl.y.shape)\n",
    "print(type(tmlt.dfl.X_test))\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106628</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>3.183594</td>\n",
       "      <td>0.081970</td>\n",
       "      <td>1.188477</td>\n",
       "      <td>3.732422</td>\n",
       "      <td>2.265625</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>-0.011719</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>1.978516</td>\n",
       "      <td>0.085999</td>\n",
       "      <td>0.240479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.673828</td>\n",
       "      <td>76.562500</td>\n",
       "      <td>3.378906</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>1.275391</td>\n",
       "      <td>-0.471436</td>\n",
       "      <td>4.546875</td>\n",
       "      <td>0.037720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135864</td>\n",
       "      <td>3.460938</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.124878</td>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.606934</td>\n",
       "      <td>-0.267822</td>\n",
       "      <td>2.578125</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.024719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036316</td>\n",
       "      <td>1.497070</td>\n",
       "      <td>233.500000</td>\n",
       "      <td>2.195312</td>\n",
       "      <td>0.026917</td>\n",
       "      <td>3.126953</td>\n",
       "      <td>5.058594</td>\n",
       "      <td>3.849609</td>\n",
       "      <td>1.801758</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>4.882812</td>\n",
       "      <td>0.085205</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.116089</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.520020</td>\n",
       "      <td>2.140625</td>\n",
       "      <td>0.124451</td>\n",
       "      <td>0.148193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.014076</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>1.890625</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>2.697266</td>\n",
       "      <td>4.515625</td>\n",
       "      <td>4.503906</td>\n",
       "      <td>0.123474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>3.474609</td>\n",
       "      <td>-0.017105</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>0.040009</td>\n",
       "      <td>0.044861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003260</td>\n",
       "      <td>3.714844</td>\n",
       "      <td>156.125000</td>\n",
       "      <td>2.148438</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>2.097656</td>\n",
       "      <td>4.156250</td>\n",
       "      <td>-0.038239</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013779</td>\n",
       "      <td>1.910156</td>\n",
       "      <td>-0.042938</td>\n",
       "      <td>0.105591</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>0.037506</td>\n",
       "      <td>1.043945</td>\n",
       "      <td>1.075195</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.072815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.242188</td>\n",
       "      <td>2.324219</td>\n",
       "      <td>-19.109375</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>0.424561</td>\n",
       "      <td>2.269531</td>\n",
       "      <td>3.621094</td>\n",
       "      <td>4.062500</td>\n",
       "      <td>1.197266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002506</td>\n",
       "      <td>3.064453</td>\n",
       "      <td>0.112427</td>\n",
       "      <td>0.100220</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.451172</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>0.056183</td>\n",
       "      <td>0.029724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.138306</td>\n",
       "      <td>0.679199</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>2.736328</td>\n",
       "      <td>-0.043549</td>\n",
       "      <td>0.514648</td>\n",
       "      <td>4.542969</td>\n",
       "      <td>3.132812</td>\n",
       "      <td>4.972656</td>\n",
       "      <td>0.097961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060730</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>-0.031433</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>0.164673</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>-0.237427</td>\n",
       "      <td>2.123047</td>\n",
       "      <td>-0.049316</td>\n",
       "      <td>0.050842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.025436</td>\n",
       "      <td>1.316406</td>\n",
       "      <td>250.375000</td>\n",
       "      <td>3.689453</td>\n",
       "      <td>0.015312</td>\n",
       "      <td>2.490234</td>\n",
       "      <td>1.983398</td>\n",
       "      <td>3.556641</td>\n",
       "      <td>4.164062</td>\n",
       "      <td>0.156860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038269</td>\n",
       "      <td>4.667969</td>\n",
       "      <td>0.157593</td>\n",
       "      <td>0.102234</td>\n",
       "      <td>1.055664</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>1.661133</td>\n",
       "      <td>1.484375</td>\n",
       "      <td>-0.027924</td>\n",
       "      <td>0.098083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.109253</td>\n",
       "      <td>2.169922</td>\n",
       "      <td>123.062500</td>\n",
       "      <td>3.279297</td>\n",
       "      <td>0.018204</td>\n",
       "      <td>3.630859</td>\n",
       "      <td>4.636719</td>\n",
       "      <td>4.507812</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>0.037140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>3.623047</td>\n",
       "      <td>0.108765</td>\n",
       "      <td>0.111877</td>\n",
       "      <td>0.020645</td>\n",
       "      <td>0.125122</td>\n",
       "      <td>2.648438</td>\n",
       "      <td>2.753906</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.035583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-0.032227</td>\n",
       "      <td>1.616211</td>\n",
       "      <td>171.875000</td>\n",
       "      <td>3.207031</td>\n",
       "      <td>-0.008675</td>\n",
       "      <td>2.208984</td>\n",
       "      <td>4.097656</td>\n",
       "      <td>1.391602</td>\n",
       "      <td>2.486328</td>\n",
       "      <td>0.121643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074463</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>0.148682</td>\n",
       "      <td>0.056061</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>0.097046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f0        f1          f2        f3        f4        f5        f6  \\\n",
       "id                                                                             \n",
       "0     0.106628  3.593750  132.750000  3.183594  0.081970  1.188477  3.732422   \n",
       "1     0.125000  1.673828   76.562500  3.378906  0.099426  5.093750  1.275391   \n",
       "2     0.036316  1.497070  233.500000  2.195312  0.026917  3.126953  5.058594   \n",
       "3    -0.014076  0.245972  780.000000  1.890625  0.006947  1.531250  2.697266   \n",
       "4    -0.003260  3.714844  156.125000  2.148438  0.018280  2.097656  4.156250   \n",
       "...        ...       ...         ...       ...       ...       ...       ...   \n",
       "3995  0.242188  2.324219  -19.109375  0.984375  0.036438  0.424561  2.269531   \n",
       "3996  0.138306  0.679199   37.125000  2.736328 -0.043549  0.514648  4.542969   \n",
       "3997  0.025436  1.316406  250.375000  3.689453  0.015312  2.490234  1.983398   \n",
       "3998  0.109253  2.169922  123.062500  3.279297  0.018204  3.630859  4.636719   \n",
       "3999 -0.032227  1.616211  171.875000  3.207031 -0.008675  2.208984  4.097656   \n",
       "\n",
       "            f7        f8        f9  ...       f90       f91       f92  \\\n",
       "id                                  ...                                 \n",
       "0     2.265625  2.099609  0.012329  ...  0.010742  1.098633  0.013329   \n",
       "1    -0.471436  4.546875  0.037720  ...  0.135864  3.460938  0.017059   \n",
       "2     3.849609  1.801758  0.057007  ...  0.117310  4.882812  0.085205   \n",
       "3     4.515625  4.503906  0.123474  ... -0.015350  3.474609 -0.017105   \n",
       "4    -0.038239  3.371094  0.034180  ...  0.013779  1.910156 -0.042938   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3995  3.621094  4.062500  1.197266  ... -0.002506  3.064453  0.112427   \n",
       "3996  3.132812  4.972656  0.097961  ...  0.060730  4.125000 -0.031433   \n",
       "3997  3.556641  4.164062  0.156860  ...  0.038269  4.667969  0.157593   \n",
       "3998  4.507812  3.585938  0.037140  ...  0.083191  3.623047  0.108765   \n",
       "3999  1.391602  2.486328  0.121643  ...  0.074463  0.529785  0.002768   \n",
       "\n",
       "           f93       f94       f95       f96       f97       f98       f99  \n",
       "id                                                                          \n",
       "0    -0.011719  0.052765  0.065430  4.210938  1.978516  0.085999  0.240479  \n",
       "1     0.124878  0.154053  0.606934 -0.267822  2.578125 -0.020874  0.024719  \n",
       "2     0.032410  0.116089 -0.001689 -0.520020  2.140625  0.124451  0.148193  \n",
       "3    -0.008102  0.062012  0.041199  0.511719  1.968750  0.040009  0.044861  \n",
       "4     0.105591  0.125122  0.037506  1.043945  1.075195 -0.012817  0.072815  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3995  0.100220  0.036530  0.451172  1.316406  4.625000  0.056183  0.029724  \n",
       "3996  0.059143  0.164673  0.058075 -0.237427  2.123047 -0.049316  0.050842  \n",
       "3997  0.102234  1.055664  0.031769  1.661133  1.484375 -0.027924  0.098083  \n",
       "3998  0.111877  0.020645  0.125122  2.648438  2.753906  0.012726  0.035583  \n",
       "3999  0.098877  0.148682  0.056061  4.210938  0.759766  0.063538  0.097046  \n",
       "\n",
       "[4000 rows x 100 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlt.dfl.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.76 ms, sys: 2.28 ms, total: 12 ms\n",
      "Wall time: 10.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 100)\n",
      "(3200,)\n",
      "(800, 100)\n",
      "(800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 46.5 ms, sys: 3.01 ms, total: 49.5 ms\n",
      "Wall time: 47.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base xgb classifier model with your best guess params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    # your best guess params\n",
    "    'learning_rate':0.01,\n",
    "    'eval_metric':'auc',\n",
    "    # must for xgb classifier otherwise warning will be shown\n",
    "    'use_label_encoder':False,\n",
    "    # because 42 is the answer for all the randomness of this universe\n",
    "    'random_state':42,\n",
    "    #for GPU\n",
    "    #'tree_method': 'gpu_hist',\n",
    "    #'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.6142302819132088 while Accuracy is : 0.6175 \n",
      "CPU times: user 17.2 s, sys: 647 ms, total: 17.8 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs)\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Meta Ensemble Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure to PreProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 227 ms, sys: 38.2 ms, total: 265 ms\n",
      "Wall time: 65.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 1: linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:34:52,025 INFO Training Started!\n",
      "2021-12-13 15:34:52,067 INFO Training Finished!\n",
      "2021-12-13 15:34:52,071 INFO fold: 1 OOF Model Metrics: 0.7257962604771115!\n",
      "2021-12-13 15:34:52,154 INFO Training Started!\n",
      "2021-12-13 15:34:52,193 INFO Training Finished!\n",
      "2021-12-13 15:34:52,197 INFO fold: 2 OOF Model Metrics: 0.6957833655705996!\n",
      "2021-12-13 15:34:52,276 INFO Training Started!\n",
      "2021-12-13 15:34:52,319 INFO Training Finished!\n",
      "2021-12-13 15:34:52,324 INFO fold: 3 OOF Model Metrics: 0.6616956802063185!\n",
      "2021-12-13 15:34:52,404 INFO Training Started!\n",
      "2021-12-13 15:34:52,452 INFO Training Finished!\n",
      "2021-12-13 15:34:52,456 INFO fold: 4 OOF Model Metrics: 0.7076379002699064!\n",
      "2021-12-13 15:34:52,536 INFO Training Started!\n",
      "2021-12-13 15:34:52,575 INFO Training Finished!\n",
      "2021-12-13 15:34:52,580 INFO fold: 5 OOF Model Metrics: 0.7227243154104317!\n",
      "2021-12-13 15:34:52,661 INFO Mean OOF Model Metrics: 0.7027275043868735!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 995 ms, sys: 406 ms, total: 1.4 s\n",
      "Wall time: 713 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "#choose model\n",
    "linear_oof_model = LinearSVC(tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=42)\n",
    "\n",
    "#fit and predict\n",
    "linear_oof_model_preds, linear_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    model=linear_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if linear_oof_model_preds is not None:\n",
    "    print(linear_oof_model_preds.shape)\n",
    "\n",
    "if linear_oof_model_test_preds is not None:    \n",
    "    print(linear_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 2: Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:34:52,754 INFO Training Started!\n",
      "2021-12-13 15:34:52,789 INFO Training Finished!\n",
      "2021-12-13 15:34:52,793 INFO fold: 1 OOF Model Metrics: 0.7271695680206318!\n",
      "2021-12-13 15:34:52,873 INFO Training Started!\n",
      "2021-12-13 15:34:52,909 INFO Training Finished!\n",
      "2021-12-13 15:34:52,913 INFO fold: 2 OOF Model Metrics: 0.694622823984526!\n",
      "2021-12-13 15:34:52,993 INFO Training Started!\n",
      "2021-12-13 15:34:53,036 INFO Training Finished!\n",
      "2021-12-13 15:34:53,040 INFO fold: 3 OOF Model Metrics: 0.662114764667956!\n",
      "2021-12-13 15:34:53,122 INFO Training Started!\n",
      "2021-12-13 15:34:53,160 INFO Training Finished!\n",
      "2021-12-13 15:34:53,164 INFO fold: 4 OOF Model Metrics: 0.7080501678057705!\n",
      "2021-12-13 15:34:53,243 INFO Training Started!\n",
      "2021-12-13 15:34:53,279 INFO Training Finished!\n",
      "2021-12-13 15:34:53,283 INFO fold: 5 OOF Model Metrics: 0.7226276902067136!\n",
      "2021-12-13 15:34:53,362 INFO Mean OOF Model Metrics: 0.7029170029371196!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 719 ms, sys: 375 ms, total: 1.09 s\n",
      "Wall time: 686 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "log_oof_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "#fit and predict\n",
    "log_oof_model_preds, log_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    model=log_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "if log_oof_model_preds is not None:\n",
    "    print(log_oof_model_preds.shape)\n",
    "\n",
    "if log_oof_model_test_preds is not None:    \n",
    "    print(log_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 3: SKLearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:34:53,454 INFO Training Started!\n",
      "2021-12-13 15:34:54,103 INFO Training Finished!\n",
      "2021-12-13 15:34:54,109 INFO fold: 1 OOF Model Metrics: 0.7065248226950354!\n",
      "2021-12-13 15:34:54,200 INFO Training Started!\n",
      "2021-12-13 15:34:54,788 INFO Training Finished!\n",
      "2021-12-13 15:34:54,794 INFO fold: 2 OOF Model Metrics: 0.6637201805286911!\n",
      "2021-12-13 15:34:54,903 INFO Training Started!\n",
      "2021-12-13 15:34:55,617 INFO Training Finished!\n",
      "2021-12-13 15:34:55,622 INFO fold: 3 OOF Model Metrics: 0.640515796260477!\n",
      "2021-12-13 15:34:55,708 INFO Training Started!\n",
      "2021-12-13 15:34:56,311 INFO Training Finished!\n",
      "2021-12-13 15:34:56,317 INFO fold: 4 OOF Model Metrics: 0.6965839769645514!\n",
      "2021-12-13 15:34:56,403 INFO Training Started!\n",
      "2021-12-13 15:34:56,987 INFO Training Finished!\n",
      "2021-12-13 15:34:56,992 INFO fold: 5 OOF Model Metrics: 0.6985680144808971!\n",
      "2021-12-13 15:34:57,078 INFO Mean OOF Model Metrics: 0.6811825581859303!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "CPU times: user 3.61 s, sys: 2.1 s, total: 5.71 s\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "mlp_oof_model = MLPClassifier(max_iter=1000, early_stopping=True)\n",
    "\n",
    "#fit and predict\n",
    "mlp_oof_model_preds, mlp_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    model=mlp_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "if mlp_oof_model_preds is not None:\n",
    "    print(mlp_oof_model_preds.shape)\n",
    "\n",
    "if mlp_oof_model_test_preds is not None:    \n",
    "    print(mlp_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model 4: TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:34:57,172 INFO Training Started!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "a is not available, choose in ['auc', 'accuracy', 'balanced_accuracy', 'logloss', 'mae', 'mse', 'rmsle', 'unsup_loss', 'rmse']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/tabular_ml_toolkit/tabular_ml_toolkit/tmlt.py\u001b[0m in \u001b[0;36mdo_oof_kfold_train_preds\u001b[0;34m(self, X, y, n_splits, model, X_test, random_state)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"tabnet\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;31m#change for tabnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m    245\u001b[0m                           \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                           \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nbdev/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_network_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nbdev/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_set_metrics\u001b[0;34m(self, metrics, eval_names)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             self._metric_container_dict.update(\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMetricContainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{name}_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             )\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nbdev/lib/python3.9/site-packages/pytorch_tabnet/metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, metric_names, prefix)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nbdev/lib/python3.9/site-packages/pytorch_tabnet/metrics.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__post_init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics_by_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nbdev/lib/python3.9/site-packages/pytorch_tabnet/metrics.py\u001b[0m in \u001b[0;36mget_metrics_by_names\u001b[0;34m(cls, names)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             assert (\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             ), f\"{name} is not available, choose in {available_names}\"\n",
      "\u001b[0;31mAssertionError\u001b[0m: a is not available, choose in ['auc', 'accuracy', 'balanced_accuracy', 'logloss', 'mae', 'mse', 'rmsle', 'unsup_loss', 'rmse']"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# OOF training and prediction on both train and test dataset by a given model\n",
    "\n",
    "#choose model\n",
    "tabnet_oof_model = TabNetClassifier(optimizer_params=dict(lr=0.02), verbose=0)\n",
    "\n",
    "\n",
    "#fit and predict\n",
    "tabnet_oof_model_preds, tabnet_oof_model_test_preds = tmlt.do_oof_kfold_train_preds(n_splits=5,\n",
    "                                                                                    model=tabnet_oof_model,\n",
    "                                                                                    X = X_np,\n",
    "                                                                                    y = y_np,\n",
    "                                                                                    X_test = X_test_np)\n",
    "\n",
    "if tabnet_oof_model_preds is not None:\n",
    "    print(tabnet_oof_model_preds.shape)\n",
    "\n",
    "if tabnet_oof_model_test_preds is not None:\n",
    "    print(tabnet_oof_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now add back based models predictions to X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 103)\n",
      "(4000, 103)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"linear_preds\"] = linear_oof_model_preds\n",
    "tmlt.dfl.X_test[\"linear_preds\"] = linear_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 103)\n",
      "(4000, 103)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"log_reg_preds\"] = log_oof_model_preds\n",
    "tmlt.dfl.X_test[\"log_reg_preds\"] = log_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 103)\n",
      "(4000, 103)\n"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"mlp_preds\"] = mlp_oof_model_preds\n",
    "tmlt.dfl.X_test[\"mlp_preds\"] = mlp_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tabnet_oof_model_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y1/qlydcd39421gt5zgh4ptnmn00000gp/T/ipykernel_21054/1035022344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# add based model oof predictions back to X and X_test before Meta model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tabnet_preds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabnet_oof_model_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtmlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tabnet_preds\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabnet_oof_model_test_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tabnet_oof_model_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# add based model oof predictions back to X and X_test before Meta model training\n",
    "tmlt.dfl.X[\"tabnet_preds\"] = tabnet_oof_model_preds\n",
    "tmlt.dfl.X_test[\"tabnet_preds\"] = tabnet_oof_model_test_preds\n",
    "\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just update the tmlt with this new X and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:35:41,785 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "tmlt = tmlt.update_dfl(X=tmlt.dfl.X, y=tmlt.dfl.y, X_test=tmlt.dfl.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For META Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 103)\n",
      "(3200,)\n",
      "(800, 103)\n",
      "(800,)\n",
      "CPU times: user 6.87 ms, sys: 1.6 ms, total: 8.47 ms\n",
      "Wall time: 6.94 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train, X_valid,  y_train, y_valid =  tmlt.dfl.create_train_valid(valid_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# print(X_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3200, 103)\n",
      "<class 'numpy.ndarray'>\n",
      "(800, 103)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 55.2 ms, sys: 3.75 ms, total: 58.9 ms\n",
      "Wall time: 57.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_np,  X_valid_np = tmlt.pp_fit_transform(X_train, X_valid)\n",
    "\n",
    "print(type(X_train_np))\n",
    "print(X_train_np.shape)\n",
    "# print(X_train_np)\n",
    "print(type(X_valid_np))\n",
    "print(X_valid_np.shape)\n",
    "# print(X_valid_np)\n",
    "print(type(y_valid))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_params = {\n",
    "#     'objective': 'binary:logistic', \n",
    "#     'use_label_encoder': False,\n",
    "#     'n_estimators': 40000,\n",
    "#     'learning_rate': 0.18515462875481553,\n",
    "#     'subsample': 0.97, \n",
    "#     'colsample_bytree': 0.32,\n",
    "#     'max_depth': 1,\n",
    "#     'booster': 'gbtree',\n",
    "#     'gamma': 0.2, \n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'reg_lambda': 0.11729916523488974, \n",
    "#     'reg_alpha': 0.6318827156945853,\n",
    "#     'random_state': 42,\n",
    "#     'n_jobs': 4, \n",
    "#     'min_child_weight': 256,\n",
    "#     #for GPU\n",
    "# #     'tree_method': 'gpu_hist',\n",
    "# #     'predictor': 'gpu_predictor',\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate': 0.21761562020600114,\n",
    "    'eval_metric': 'auc',\n",
    "    'use_label_encoder': False,\n",
    "    'random_state': 42,\n",
    "    'booster': 'gblinear',\n",
    "    'colsample_bytree': 0.1027132584989078,\n",
    "    'early_stopping_rounds': 171,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 7000,\n",
    "    'reg_alpha': 9.583579660175245e-06,\n",
    "    'reg_lambda': 9.238315962782784e-05,\n",
    "    'subsample': 0.4464473710560276,\n",
    "    'tree_method': 'approx',\n",
    "    #for GPU\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC is : 0.6817257945306725 while Accuracy is : 0.68875 \n",
      "CPU times: user 13.5 s, sys: 52.6 ms, total: 13.5 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train), (X_valid_np, y_valid)],\n",
    "              eval_metric=\"auc\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "preds_probs = xgb_model.predict_proba(X_valid_np)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_valid, preds_probs)\n",
    "acc = accuracy_score(y_valid, preds)\n",
    "\n",
    "print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOW!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Meta Model, Let's do Optuna based HyperParameter search to get best params for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:35:57,183 INFO Optimization Direction is: maximize\n",
      "\u001b[32m[I 2021-12-13 15:35:57,245]\u001b[0m Using an existing study with name 'tmlt_autoxgb' instead of creating a new one.\u001b[0m\n",
      "2021-12-13 15:35:57,455 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:35:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"eval_set\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:36:05,441 INFO Training Ended!\n",
      "2021-12-13 15:36:05,446 INFO roc_auc_score: 0.7648479569211276\n",
      "\u001b[32m[I 2021-12-13 15:36:05,484]\u001b[0m Trial 45 finished with value: 0.7648479569211276 and parameters: {'learning_rate': 0.02552655505374164, 'n_estimators': 15000, 'reg_lambda': 3.646574226061381e-07, 'reg_alpha': 3.08227620916927e-06, 'subsample': 0.9918980035129225, 'colsample_bytree': 0.35039587482158074, 'max_depth': 5, 'early_stopping_rounds': 217, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n",
      "2021-12-13 15:36:05,668 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"eval_set\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:36:14,283 INFO Training Ended!\n",
      "2021-12-13 15:36:14,288 INFO roc_auc_score: 0.7648281596452329\n",
      "\u001b[32m[I 2021-12-13 15:36:14,318]\u001b[0m Trial 46 finished with value: 0.7648281596452329 and parameters: {'learning_rate': 0.03103801350689903, 'n_estimators': 15000, 'reg_lambda': 7.171397641431569e-08, 'reg_alpha': 5.448600947004519e-07, 'subsample': 0.872708135773233, 'colsample_bytree': 0.29949184289978, 'max_depth': 4, 'early_stopping_rounds': 436, 'tree_method': 'approx', 'booster': 'gblinear'}. Best is trial 38 with value: 0.7741592756836658.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=38, values=[0.7741592756836658], datetime_start=datetime.datetime(2021, 12, 9, 23, 59, 38, 76564), datetime_complete=datetime.datetime(2021, 12, 9, 23, 59, 47, 35226), params={'booster': 'gblinear', 'colsample_bytree': 0.30017155189532796, 'early_stopping_rounds': 268, 'learning_rate': 0.07948707900984789, 'max_depth': 5, 'n_estimators': 15000, 'reg_alpha': 2.5216595944303144e-06, 'reg_lambda': 7.984356925605064e-06, 'subsample': 0.9949361731336916, 'tree_method': 'approx'}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear')), 'colsample_bytree': UniformDistribution(high=1.0, low=0.1), 'early_stopping_rounds': IntUniformDistribution(high=500, low=100, step=1), 'learning_rate': LogUniformDistribution(high=0.25, low=0.01), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'n_estimators': CategoricalDistribution(choices=(7000, 15000, 20000)), 'reg_alpha': LogUniformDistribution(high=100.0, low=1e-08), 'reg_lambda': LogUniformDistribution(high=100.0, low=1e-08), 'subsample': UniformDistribution(high=1.0, low=0.1), 'tree_method': CategoricalDistribution(choices=('exact', 'approx', 'hist'))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=39, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "# **Just make sure to supply an output directory path so hyperparameter search is saved**\n",
    "study = tmlt.do_xgb_optuna_optimization(optuna_db_path=OUTPUT_PATH, opt_timeout=10)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now update the meta model with best params from study and then update the sklearn pipeline with this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb_params {'learning_rate': 0.07948707900984789, 'eval_metric': 'auc', 'use_label_encoder': False, 'random_state': 42, 'booster': 'gblinear', 'colsample_bytree': 0.30017155189532796, 'early_stopping_rounds': 268, 'max_depth': 5, 'n_estimators': 15000, 'reg_alpha': 2.5216595944303144e-06, 'reg_lambda': 7.984356925605064e-06, 'subsample': 0.9949361731336916, 'tree_method': 'approx'}\n"
     ]
    }
   ],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "print(\"xgb_params\", xgb_params)\n",
    "updated_xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 544 ms, sys: 40.2 ms, total: 585 ms\n",
      "Wall time: 85.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.X_test)\n",
    "y_np = tmlt.dfl.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:36:14,528 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:36:32,640 INFO Training Finished!\n",
      "2021-12-13 15:36:32,641 INFO Predicting Val Probablities!\n",
      "2021-12-13 15:36:32,645 INFO Predicting Val Score!\n",
      "2021-12-13 15:36:32,650 INFO fold: 1 roc_auc_score : 0.8026756931012251\n",
      "2021-12-13 15:36:32,651 INFO Predicting Test Probablities!\n",
      "2021-12-13 15:36:32,728 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:36:51,360 INFO Training Finished!\n",
      "2021-12-13 15:36:51,361 INFO Predicting Val Probablities!\n",
      "2021-12-13 15:36:51,365 INFO Predicting Val Score!\n",
      "2021-12-13 15:36:51,370 INFO fold: 2 roc_auc_score : 0.7804513217279174\n",
      "2021-12-13 15:36:51,371 INFO Predicting Test Probablities!\n",
      "2021-12-13 15:36:51,465 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:37:15,903 INFO Training Finished!\n",
      "2021-12-13 15:37:15,903 INFO Predicting Val Probablities!\n",
      "2021-12-13 15:37:15,907 INFO Predicting Val Score!\n",
      "2021-12-13 15:37:15,913 INFO fold: 3 roc_auc_score : 0.7533913604126369\n",
      "2021-12-13 15:37:15,914 INFO Predicting Test Probablities!\n",
      "2021-12-13 15:37:15,994 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:37:40,629 INFO Training Finished!\n",
      "2021-12-13 15:37:40,629 INFO Predicting Val Probablities!\n",
      "2021-12-13 15:37:40,634 INFO Predicting Val Score!\n",
      "2021-12-13 15:37:40,640 INFO fold: 4 roc_auc_score : 0.7868834506792751\n",
      "2021-12-13 15:37:40,641 INFO Predicting Test Probablities!\n",
      "2021-12-13 15:37:40,734 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:37:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"early_stopping_rounds\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 15:38:08,412 INFO Training Finished!\n",
      "2021-12-13 15:38:08,413 INFO Predicting Val Probablities!\n",
      "2021-12-13 15:38:08,416 INFO Predicting Val Score!\n",
      "2021-12-13 15:38:08,422 INFO fold: 5 roc_auc_score : 0.7905165583390772\n",
      "2021-12-13 15:38:08,423 INFO Predicting Test Probablities!\n",
      "2021-12-13 15:38:08,521 INFO  Mean Metrics Results from all Folds are: {'roc_auc_score': 0.7827836768520264}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 11s, sys: 46.1 s, total: 19min 57s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# k-fold training\n",
    "xgb_model_metrics_score, xgb_model_test_preds = tmlt.do_kfold_training(X_np,\n",
    "                                                                       y_np,\n",
    "                                                                       X_test=X_test_np,\n",
    "                                                                       n_splits=5,\n",
    "                                                                       model=updated_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "# predict on test dataset\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take weighted average of both k-fold models predictions\n",
    "# final_preds = ((0.45 * sci_model_preds) + (0.55* xgb_model_test_preds)) / 2\n",
    "# print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "# sub['target'] = final_preds\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "# # run the script to build \n",
    "\n",
    "# from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
