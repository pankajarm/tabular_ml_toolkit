{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular ML Toolkit\n",
    "\n",
    "> A superfast helper library to jumpstart your machine learning project based on tabular or structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter tuning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create MLPipeline with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For example, Here we are using RandomForestRegressor from Scikit-Learn, on  [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*\n",
    "\n",
    "\n",
    "*No need to install scikit-learn as it comes preinstall with Tabular_ML_Toolkit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.MLPipeline import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"https://raw.githubusercontent.com/psmathur/tabular_ml_toolkit/master/input/home_data/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit-learn ml model\n",
    "scikit_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# createm ml pipeline for scikit-learn model\n",
    "tmlt = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\", target=\"SalePrice\",\n",
    "    model=scikit_model,\n",
    "    random_state=42)\n",
    "\n",
    "#scikit-pipeline\n",
    "# tmlt.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Time: 1.0225746631622314\n",
      "X_valid MAE: 17634.989965753426\n"
     ]
    }
   ],
   "source": [
    "# create train, valid split to evaulate model on valid dataset\n",
    "tmlt.dfl.create_train_valid(valid_size=0.2)\n",
    "\n",
    "start = time.time()\n",
    "# Now fit\n",
    "tmlt.spl.fit(tmlt.dfl.X_train, tmlt.dfl.y_train)\n",
    "end = time.time()\n",
    "print(\"Fit Time:\", end - start)\n",
    "\n",
    "#predict\n",
    "preds = tmlt.spl.predict(tmlt.dfl.X_valid)\n",
    "print('X_valid MAE:', mean_absolute_error(tmlt.dfl.y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You can also use MLPipeline with XGBoost model, Just make sure to install XGBooost first depending upon your OS.*\n",
    "\n",
    "*After that all steps remains same. Here is example using XGBRegressor with [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_params = {\n",
    "    'n_estimators':250,\n",
    "    'learning_rate':0.05,\n",
    "    'random_state':42,\n",
    "    # for GPU\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "# create xgb model\n",
    "xgb_model = XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pipeline with xgb model\n",
    "tmlt.update_model(xgb_model)\n",
    "# tmlt.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Time: 0.502791166305542\n",
      "X_valid MAE: 15851.009123501712\n"
     ]
    }
   ],
   "source": [
    "# create train, valid split to evaulate model on valid dataset\n",
    "tmlt.dfl.create_train_valid(valid_size=0.2)\n",
    "\n",
    "start = time.time()\n",
    "# Now fit\n",
    "tmlt.spl.fit(tmlt.dfl.X_train, tmlt.dfl.y_train)\n",
    "end = time.time()\n",
    "print(\"Fit Time:\", end - start)\n",
    "\n",
    "#predict\n",
    "preds = tmlt.spl.predict(tmlt.dfl.X_valid)\n",
    "print('X_valid MAE:', mean_absolute_error(tmlt.dfl.y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In background `prepare_data_for_training` method loads your input data into Pandas DataFrame, seprates X(features) and y(target), Then it preprocess all numerical and categorical type data found in these DataFrames. Then it bundle preprocessed data with your given model and return an MLPipeline object which contains dataframeloader, preprocessor and scikit-learn pipeline.\n",
    "\n",
    "`create_train_valid` methods split X(features) into X_train, y_train, X_valid, y_valid DataFrames.\n",
    "\n",
    "so you can call scikit-learn pipeline fit method on X_train and y_train and predict on X_valid or X_test.\n",
    "\n",
    "Here is detail documentation and source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to customize data and preprocessing steps you can do so by using `DataFrameLoader` and `PreProessor` classes. Please Check other Tutorials and detail documentations for these classes for more options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
