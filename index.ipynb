{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Tutorial with TMLT (Tabular ML Toolkit)\n",
    "\n",
    "> A tutorial on getting started with TMLT (Tabular ML Toolkit)\n",
    "\n",
    "> tabular_ml_toolkit is a helper library to jumpstart your machine learning project based on Tabular or Structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter search techniques.\n",
    "\n",
    "> Under the hood TMLT uses optuna, xgboost and scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create tmlt with one API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For example, Here we are using XGBRegressor on  [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"input/home_data/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just point tmlt in the direction of your data\n",
    "#####  let it know what are idx and target columns in your tabular data\n",
    "#####  and what kind of problem type you are trying to resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# tmlt\n",
    "tmlt = TMLT().prepare_data(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\", target=\"SalePrice\",\n",
    "    random_state=42,\n",
    "    problem_type=\"regression\")\n",
    "\n",
    "# TMLT currently only supports below problem_type:\n",
    "\n",
    "# \"binary_classification\"\n",
    "# \"multi_label_classification\"\n",
    "# \"multi_class_classification\"\n",
    "# \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tmlt.dfl.X))\n",
    "print(tmlt.dfl.X.shape)\n",
    "print(type(tmlt.dfl.y))\n",
    "print(tmlt.dfl.y.shape)\n",
    "print(type(tmlt.dfl.X_test))\n",
    "print(tmlt.dfl.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmlt.dfl.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now PreProcess X_train, X_valid\n",
    "\n",
    "NOTE: Preprocessing gives back numpy arrays for pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_np, y_np, X_test_np = tmlt.pp_fit_transform(tmlt.dfl.X, tmlt.dfl.y, tmlt.dfl.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_np.shape)\n",
    "print(type(X_np))\n",
    "print(y_np.shape)\n",
    "print(type(y_np))\n",
    "print(X_test_np.shape)\n",
    "print(type(X_test_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create train valid dataframes for quick preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create train, valid split to evaulate model on valid dataset\n",
    "X_train_np, X_valid_np,  y_train_np, y_valid_np =  tmlt.dfl.create_train_valid(X_np, y_np, valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_np.shape)\n",
    "print(X_valid_np.shape)\n",
    "print(y_train_np.shape)\n",
    "print(y_valid_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a base xgb classifier model with your best guess params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'learning_rate':0.1,\n",
    "    'use_label_encoder':False,\n",
    "    'eval_metric':'rmse',\n",
    "    'random_state':42,\n",
    "    # for GPU\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "}\n",
    "# create xgb ml model\n",
    "xgb_model = XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now do model training\n",
    "xgb_model.fit(X_train_np, y_train_np,\n",
    "              verbose=False,\n",
    "              #detect & avoid overfitting\n",
    "              eval_set=[(X_train_np, y_train_np), (X_valid_np, y_valid_np)],\n",
    "              eval_metric=\"mae\",\n",
    "              early_stopping_rounds=300\n",
    "             )\n",
    "\n",
    "#predict\n",
    "preds = xgb_model.predict(X_valid_np)\n",
    "print('X_valid MAE:', mean_absolute_error(y_valid_np, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In background `prepare_data` method loads your input data into Pandas DataFrame, seprates X(features) and y(target), preprocess all numerical and categorical type data found in these DataFrames using scikit-learn pipelines. Then it bundle preprocessor and data return a TMLT object, this class instance has dataframeloader, preprocessor instances.\n",
    "\n",
    "The `create_train_valid` method use valid_size to split X(features) into X_train, y_train, X_valid and y_valid DataFrames, so you can call fit methods on X_train and y_train and predict methods on X_valid or X_test.\n",
    "\n",
    "Please check detail documentation and source code for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: If you want to customize data and preprocessing steps you can do so by using `DataFrameLoader` and `PreProessor` classes. Check detail documentations for these classes for more options.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### To see more clear picture of model performance, Let's do a quick Cross Validation on our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure to PreProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now do cross_validation\n",
    "scores = tmlt.do_cross_validation(X_np, y_np, xgb_model, scoring='neg_mean_absolute_error', cv=5)\n",
    "\n",
    "print(\"scores:\", scores)\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MAE did came out slightly bad with cross validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's see if we can improve our cross validation score with hyperparams tunning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are using optuna based hyperparameter search here!**\n",
    "\n",
    "**TMLT has inbuilt xgb optuna optimization helper method!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Just make sure to supply an output directory path so hyperparameter search is saved**\n",
    "study = tmlt.do_xgb_optuna_optimization(optuna_db_path=OUTPUT_PATH, opt_timeout=10)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use our newly found best params to update the model on sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "print(\"xgb_params\", xgb_params)\n",
    "updated_xgb_model = XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, Let's use 5 K-Fold Training on this Updated XGB model with best params found from Optuna search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# k-fold training\n",
    "xgb_model_metrics_score, xgb_model_test_preds = tmlt.do_kfold_training(X_np, y_np, model=updated_xgb_model, \n",
    "X_test=X_test_np, n_splits=5, kfold_metric=mean_absolute_error, eval_metric = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test dataset\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### You can even improve metrics score further by running Optuna search for longer time or rerunning the study, check documentation for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
