{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular ML Toolkit\n",
    "\n",
    "> A super fast helper library to jumpstart your machine learning project based on tabular or structured data.\n",
    "\n",
    "> It comes with model parallelism and cutting edge hyperparameter tuning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create MLPipeline with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For example, Here we are using RandomForestRegressor from Scikit-Learn, on  [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*\n",
    "\n",
    "\n",
    "*No need to install scikit-learn as it comes preinstall with Tabular_ML_Toolkit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.MLPipeline import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"https://raw.githubusercontent.com/psmathur/tabular_ml_toolkit/master/input/home_data/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scikit-learn ml model\n",
    "scikit_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# createm ml pipeline for scikit-learn model\n",
    "tmlt = MLPipeline().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH+TRAIN_FILE,\n",
    "    test_file_path= DIRECTORY_PATH+TEST_FILE,\n",
    "    idx_col=\"Id\",\n",
    "    target=\"SalePrice\",\n",
    "    model=scikit_model,\n",
    "    random_state=42)\n",
    "\n",
    "# visualize scikit-pipeline\n",
    "# tmlt.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time: 14.697277069091797\n",
      "scores: [16871.87979452 18135.86726027 16032.48842466 19186.10719178\n",
      " 19341.86143836 14970.44808219 15863.47123288 16053.91267123\n",
      " 20180.36609589 17375.76856164]\n",
      "Average MAE score: 17401.217075342465\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Now do cross_validation\n",
    "scores = tmlt.do_cross_validation(cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Cross Validation Time:\", end - start)\n",
    "\n",
    "print(\"scores:\", scores)\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can also use XGBoost model on same pipeline\n",
    "\n",
    "*Just make sure to install XGBooost first depending upon your OS.*\n",
    "\n",
    "*After that all steps remains same. Here is example using XGBRegressor with [Melbourne Home Sale price data](https://www.kaggle.com/estrotococo/home-data-for-ml-course)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb_params = {\n",
    "    'n_estimators':250,\n",
    "    'learning_rate':0.05,\n",
    "    'random_state':42,\n",
    "    # for GPU\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "# create xgb model\n",
    "xgb_model = XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to update pipeline with xgb model\n",
    "tmlt.update_model(xgb_model)\n",
    "\n",
    "# visualize scikit-pipeline\n",
    "# tmlt.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time: 6.754866123199463\n",
      "scores: [14655.50866866 15914.2911494  15265.50021404 17544.02900257\n",
      " 18052.48084332 15120.70601455 14776.67005565 13291.54387842\n",
      " 17425.94231592 16157.00203339]\n",
      "Average MAE score: 15820.36741759418\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Now do cross_validation\n",
    "scores = tmlt.do_cross_validation(cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Cross Validation Time:\", end - start)\n",
    "\n",
    "print(\"scores:\", scores)\n",
    "print(\"Average MAE score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB model looks more promising!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In background `prepare_data_for_training` method loads your input data into Pandas DataFrame, seprates X(features) and y(target), \n",
    "\n",
    "Then it preprocess all numerical and categorical type data found in these dataframes.\n",
    "\n",
    "Then it bundle preprocessed data with your given model and return an MLPipeline object which contains dataframeloader, preprocessor and scikit-learn pipeline.\n",
    "\n",
    "\n",
    "Please see tutorials for more features from ML Tabular Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do Hyper Parameters Optimization and find the best params for XGB Model\n",
    "\n",
    " Let's give our Grid Search 2 minute time budget, Because you don't have eternity to wait for hyperparam tunning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pamathur/miniconda3/envs/nbdev_env/lib/python3.9/site-packages/tune_sklearn/tune_basesearch.py:400: UserWarning: max_iters is set > 1 but incremental/partial training is not enabled. To enable partial training, ensure the estimator has `partial_fit` or `warm_start` and set `early_stopping=True`. Automatically setting max_iters=1.\n",
      "  warnings.warn(\n",
      "/Users/pamathur/miniconda3/envs/nbdev_env/lib/python3.9/site-packages/ray/tune/tune.py:368: UserWarning: The `loggers` argument is deprecated. Please pass the respective `LoggerCallback` classes to the `callbacks` argument instead. See https://docs.ray.io/en/latest/tune/api_docs/logging.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# let's do tune grid search for faster hyperparams tuning for data preprocessing and model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# let's tune data preprocessing and model hyperparams\n",
    "param_grid = {\n",
    "    \"preprocessor__num_cols__scaler\": [StandardScaler(), MinMaxScaler()],\n",
    "    \"preprocessor__cat_cols__imputer\": [SimpleImputer(strategy='constant'),\n",
    "                                                 SimpleImputer(strategy='most_frequent')],\n",
    "    'model__n_estimators': [500,1000],\n",
    "    'model__learning_rate': [0.02,0.05],\n",
    "    'model__max_depth': [5,10]\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "# Now do tune grid search\n",
    "tune_search = tmlt.do_tune_grid_search(param_grid=param_grid,\n",
    "                                       cv=5,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                      early_stopping=False,\n",
    "                                      time_budget_s=120)\n",
    "end = time.time()\n",
    "print(\"Grid Search Time:\", end - start)\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(tune_search.best_params_)\n",
    "\n",
    "print(f\"Internal CV Metrics score: {-1*(tune_search.best_score_):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to customize data and preprocessing steps you can do so by using `DataFrameLoader` and `PreProessor` classes. Please Check other Tutorials and detail documentations for these classes for more options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazing our 5 Fold CV MAE has reduced to 15700.401 within 2 minutes by HyperParamss tunning!\n",
    "\n",
    "If we can continue doing hyperparmas tunning, may be we can even do better, You can also try early_stopping, take that as challenge!\n",
    "\n",
    "###### Let's use our newly found params for k-fold training and update preprocessor and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update PreProcessor on MLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_params = tmlt.get_preprocessor_best_params(tune_search)\n",
    "\n",
    "# Update pipeline with updated preprocessor\n",
    "tmlt.update_preprocessor(**pp_params)\n",
    "tmlt.spl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Update Model on MLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = tmlt.get_model_best_params(tune_search)\n",
    "\n",
    "# create xgb ml model\n",
    "xgb_model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Update pipeline with xgb model\n",
    "tmlt.update_model(xgb_model)\n",
    "tmlt.spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold training\n",
    "xgb_model_k_fold, xgb_model_metrics_score = tmlt.do_k_fold_training(n_splits=10, metrics=mean_absolute_error)\n",
    "print(\"mean metrics score:\", np.mean(xgb_model_metrics_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yay, we have much better 10 K-Fold MAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test dataset which was given initially\n",
    "xgb_model_preds = tmlt.do_k_fold_prediction(k_fold=xgb_model_k_fold)\n",
    "print(xgb_model_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
