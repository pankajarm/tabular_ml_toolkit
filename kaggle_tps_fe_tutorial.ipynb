{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started Kaggle TPS Challenge with Tabular ML Toolkit\n",
    "\n",
    "> A Tutorial to showcase usage of tabular_ml_toolkit (tmlt) library on Kaggle TPS Challenge Nov 2021.\n",
    "\n",
    "> tabular_ml_toolkit is a helper library to jumpstart your machine learning project based on Tabular or Structured data.\n",
    "\n",
    "> It comes with model and data parallelism and cutting edge hyperparameter search techniques.\n",
    "\n",
    "> Under the hood TMLT uses modin, optuna, xgboost and scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install -U tabular_ml_toolkit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Best Use tabular_ml_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with your favorite model and then just simply create **tmlt** with one API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we are using XGBClassifier, on  [Kaggle TPS Challenge (Nov 2021) data](https://www.kaggle.com/c/tabular-playground-series-nov-2021/data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_ml_toolkit.tmlt import *\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# for visualizing pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "# just to measure fit performance\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset file names and Paths\n",
    "DIRECTORY_PATH = \"/Users/pamathur/kaggle_datasets/tps_nov_2021/\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "SAMPLE_SUB_FILE = \"sample_submission.csv\"\n",
    "OUTPUT_PATH = \"kaggle_tps_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY THIS using LOGISTIC Regression\n",
    "# https://www.kaggle.com/maximkazantsev/tps-11-21-eda-xgboost-optuna\n",
    "\n",
    "# ALSO TAKE OUT MODIN OR USE SOME FUNCTIONALITY TO USE BOTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_svm_model = LinearSVC(tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createm tmlt for xgb model\n",
    "tmlt = TMLT().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    #test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"id\",\n",
    "    target=\"target\",\n",
    "    model=skl_svm_model,\n",
    "    random_state=42,\n",
    "    problem_type=\"binary_classification\", nrows=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base xgb classifier model with your best guess params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    # your best guess params\n",
    "    'learning_rate':0.01,\n",
    "    'eval_metric':'auc',\n",
    "    # must for xgb classifier otherwise warning will be shown\n",
    "    'use_label_encoder':False,\n",
    "    # because 42 is the answer for all the randomness of this universe\n",
    "    'random_state':42,\n",
    "    #for GPU\n",
    "    #'tree_method': 'gpu_hist',\n",
    "    #'predictor': 'gpu_predictor',\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:07:20,402 INFO 12 cores found, model and data parallel processing should worked!\n",
      "UserWarning: Distributing <class 'int'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19469)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:07:39,201 INFO DataFrame Memory usage decreased to 0.80 Mb (74.4% reduction)\n",
      "2021-11-23 14:07:39,201 INFO No test_file_path given, so training will continue without it!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_func pid=19469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19470)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:07:49,152 INFO PreProcessing will include target(s) encoding!\n",
      "2021-11-23 14:07:49,153 INFO categorical columns are None, Preprocessing will done accordingly!\n"
     ]
    }
   ],
   "source": [
    "# createm tmlt for xgb model\n",
    "tmlt = TMLT().prepare_data_for_training(\n",
    "    train_file_path= DIRECTORY_PATH + TRAIN_FILE,\n",
    "    #test_file_path= DIRECTORY_PATH + TEST_FILE,\n",
    "    #make sure to use right index and target columns\n",
    "    idx_col=\"id\",\n",
    "    target=\"target\",\n",
    "    model=xgb_model,\n",
    "    random_state=42,\n",
    "    problem_type=\"classification\", nrows=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-ae5d1120-980e-4253-afa3-093df19cbf85 {color: black;background-color: white;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 pre{padding: 0;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-toggleable {background-color: white;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-estimator:hover {background-color: #d4ebff;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-item {z-index: 1;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-parallel-item:only-child::after {width: 0;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-ae5d1120-980e-4253-afa3-093df19cbf85 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-ae5d1120-980e-4253-afa3-093df19cbf85\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8153eb20-27f2-493d-afac-91098f7cf92a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8153eb20-27f2-493d-afac-91098f7cf92a\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num_cols',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['f0', 'f1', 'f2', 'f3', 'f4',\n",
       "                                                   'f5', 'f6', 'f7', 'f8', 'f9',\n",
       "                                                   'f10', 'f11', 'f12', 'f13',\n",
       "                                                   'f14', 'f15', 'f16', 'f17',\n",
       "                                                   'f18', 'f19', 'f20', 'f21',\n",
       "                                                   'f22', 'f23', 'f24', 'f25',\n",
       "                                                   'f26', 'f27', 'f28', 'f29', ...])])),\n",
       "                (...\n",
       "                               interaction_constraints=None, learning_rate=0.01,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=11, num_parallel_tree=None,\n",
       "                               predictor=None, random_state=42, reg_alpha=None,\n",
       "                               reg_lambda=None, scale_pos_weight=None,\n",
       "                               subsample=None, tree_method=None,\n",
       "                               use_label_encoder=False,\n",
       "                               validate_parameters=None, verbosity=None))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8f20cc2d-256e-4d34-8e88-a6c512070f2d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8f20cc2d-256e-4d34-8e88-a6c512070f2d\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('num_cols',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='constant')),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6',\n",
       "                                  'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13',\n",
       "                                  'f14', 'f15', 'f16', 'f17', 'f18', 'f19',\n",
       "                                  'f20', 'f21', 'f22', 'f23', 'f24', 'f25',\n",
       "                                  'f26', 'f27', 'f28', 'f29', ...])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a73b5ff0-b7fb-4cbe-8c22-f867294688f2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a73b5ff0-b7fb-4cbe-8c22-f867294688f2\">num_cols</label><div class=\"sk-toggleable__content\"><pre>['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7a1990e5-e74e-4988-9c59-cfa74c5d9e7c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7a1990e5-e74e-4988-9c59-cfa74c5d9e7c\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy='constant')</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f0ac3351-b796-4566-b630-9bdcc87288c2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f0ac3351-b796-4566-b630-9bdcc87288c2\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f3d3cb61-b3ac-4f4a-968e-988db4ae40a1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f3d3cb61-b3ac-4f4a-968e-988db4ae40a1\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None,\n",
       "              enable_categorical=False, eval_metric='auc', gamma=None,\n",
       "              gpu_id=None, importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=11, num_parallel_tree=None,\n",
       "              predictor=None, random_state=42, reg_alpha=None, reg_lambda=None,\n",
       "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "              use_label_encoder=False, validate_parameters=None,\n",
       "              verbosity=None)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num_cols',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['f0', 'f1', 'f2', 'f3', 'f4',\n",
       "                                                   'f5', 'f6', 'f7', 'f8', 'f9',\n",
       "                                                   'f10', 'f11', 'f12', 'f13',\n",
       "                                                   'f14', 'f15', 'f16', 'f17',\n",
       "                                                   'f18', 'f19', 'f20', 'f21',\n",
       "                                                   'f22', 'f23', 'f24', 'f25',\n",
       "                                                   'f26', 'f27', 'f28', 'f29', ...])])),\n",
       "                (...\n",
       "                               interaction_constraints=None, learning_rate=0.01,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=11, num_parallel_tree=None,\n",
       "                               predictor=None, random_state=42, reg_alpha=None,\n",
       "                               reg_lambda=None, scale_pos_weight=None,\n",
       "                               subsample=None, tree_method=None,\n",
       "                               use_label_encoder=False,\n",
       "                               validate_parameters=None, verbosity=None))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmlt.spl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do a quick round of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmlt.dfl.create_train_valid(valid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick check on dataframe shapes\n",
    "# print(f\"X_train shape is {tmlt.dfl.X_train.shape}\" )\n",
    "# print(f\"X_valid shape is {tmlt.dfl.X_valid.shape}\" )\n",
    "# print(f\"y_train shape is {tmlt.dfl.y_train.shape}\")\n",
    "# print(f\"y_valid shape is {tmlt.dfl.y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit\n",
    "# start = time.time()\n",
    "# # Now fit\n",
    "# tmlt.spl.fit(tmlt.dfl.X_train, tmlt.dfl.y_train)\n",
    "# end = time.time()\n",
    "# print(\"Fit Time:\", end - start)\n",
    "\n",
    "# #predict\n",
    "# preds = tmlt.spl.predict(tmlt.dfl.X_valid)\n",
    "# preds_probs = tmlt.spl.predict_proba(tmlt.dfl.X_valid)[:, 1]\n",
    "\n",
    "# # Metrics\n",
    "# auc = roc_auc_score(tmlt.dfl.y_valid, preds_probs)\n",
    "# acc = accuracy_score(tmlt.dfl.y_valid, preds)\n",
    "\n",
    "# print(f\"AUC is : {auc} while Accuracy is : {acc} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do Optuna based HyperParameter search to get best params for fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:25:46,244 INFO Optimization Direction is: minimize\n",
      "\u001b[32m[I 2021-11-23 14:25:46,274]\u001b[0m Using an existing study with name 'tmlt_autoxgb' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19464)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:25:49,535 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n",
      "[14:25:55] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:26:07,809 INFO Training Ended!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19471)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19466)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:26:29,793 INFO log_loss: 0.6051906617963687\n",
      "2021-11-23 14:26:29,794 INFO roc_auc_score: 0.7158958927251612\n",
      "2021-11-23 14:26:29,794 INFO accuracy_score: 0.695\n",
      "2021-11-23 14:26:29,795 INFO f1_score: 0.5836177474402731\n",
      "2021-11-23 14:26:29,796 INFO precision_score: 0.6151079136690647\n",
      "2021-11-23 14:26:29,797 INFO recall_score: 0.5551948051948052\n",
      "\u001b[32m[I 2021-11-23 14:26:29,834]\u001b[0m Trial 4 finished with value: 0.6051906617963687 and parameters: {'learning_rate': 0.07778202222863026, 'n_estimators': 20000, 'reg_lambda': 1.1572194721196033e-05, 'reg_alpha': 3.079971779735798e-08, 'subsample': 0.8566092401661841, 'colsample_bytree': 0.7338078050434531, 'max_depth': 1, 'tree_method': 'exact', 'booster': 'gblinear'}. Best is trial 3 with value: 0.6051832608412951.\u001b[0m\n",
      "2021-11-23 14:26:33,655 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19470)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:26:56,977 INFO Training Ended!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19466)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_func pid=19463)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19470)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19474)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19467)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:27:18,824 INFO log_loss: 1.3733108584548672\n",
      "2021-11-23 14:27:18,826 INFO roc_auc_score: 0.600121423292155\n",
      "2021-11-23 14:27:18,826 INFO accuracy_score: 0.59375\n",
      "2021-11-23 14:27:18,827 INFO f1_score: 0.4574290484140234\n",
      "2021-11-23 14:27:18,828 INFO precision_score: 0.47079037800687284\n",
      "2021-11-23 14:27:18,828 INFO recall_score: 0.4448051948051948\n",
      "\u001b[32m[I 2021-11-23 14:27:18,860]\u001b[0m Trial 5 finished with value: 1.3733108584548672 and parameters: {'learning_rate': 0.14680037130381238, 'n_estimators': 15000, 'reg_lambda': 4.211629467291321, 'reg_alpha': 5.624849648423015e-07, 'subsample': 0.11905642392373826, 'colsample_bytree': 0.7166667785774565, 'max_depth': 2, 'tree_method': 'hist', 'booster': 'gbtree', 'gamma': 2.401734572428494e-07, 'grow_policy': 'depthwise'}. Best is trial 3 with value: 0.6051832608412951.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19465)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:27:22,683 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19465)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_func pid=19473)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:27:50,993 INFO Training Ended!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19473)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19470)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19469)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19473)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:28:12,915 INFO log_loss: 0.7951592802736559\n",
      "2021-11-23 14:28:12,916 INFO roc_auc_score: 0.6458663287931581\n",
      "2021-11-23 14:28:12,916 INFO accuracy_score: 0.63875\n",
      "2021-11-23 14:28:12,917 INFO f1_score: 0.5025817555938038\n",
      "2021-11-23 14:28:12,918 INFO precision_score: 0.5347985347985348\n",
      "2021-11-23 14:28:12,919 INFO recall_score: 0.474025974025974\n",
      "\u001b[32m[I 2021-11-23 14:28:12,956]\u001b[0m Trial 6 finished with value: 0.7951592802736559 and parameters: {'learning_rate': 0.020919490032178578, 'n_estimators': 7000, 'reg_lambda': 1.0191685617159378e-06, 'reg_alpha': 2.7618753046849175e-08, 'subsample': 0.5364273278094787, 'colsample_bytree': 0.5406269791805867, 'max_depth': 3, 'tree_method': 'approx', 'booster': 'gbtree', 'gamma': 4.48740453005513e-06, 'grow_policy': 'depthwise'}. Best is trial 3 with value: 0.6051832608412951.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19466)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:28:16,633 INFO Training Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19464)\u001b[0m \n",
      "[14:28:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:28:35,484 INFO Training Ended!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19466)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19473)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19474)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19474)\u001b[0m \n",
      "\u001b[2m\u001b[36m(compute_sliced_len pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19473)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19474)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:28:56,523 INFO log_loss: 0.6051877725403756\n",
      "2021-11-23 14:28:56,524 INFO roc_auc_score: 0.7158958927251612\n",
      "2021-11-23 14:28:56,525 INFO accuracy_score: 0.695\n",
      "2021-11-23 14:28:56,526 INFO f1_score: 0.5836177474402731\n",
      "2021-11-23 14:28:56,527 INFO precision_score: 0.6151079136690647\n",
      "2021-11-23 14:28:56,527 INFO recall_score: 0.5551948051948052\n",
      "\u001b[32m[I 2021-11-23 14:28:56,560]\u001b[0m Trial 7 finished with value: 0.6051877725403756 and parameters: {'learning_rate': 0.04988928951526494, 'n_estimators': 20000, 'reg_lambda': 1.7238404326454513e-05, 'reg_alpha': 6.60629606464357e-07, 'subsample': 0.2750714764806063, 'colsample_bytree': 0.325291757151274, 'max_depth': 9, 'tree_method': 'exact', 'booster': 'gblinear'}. Best is trial 3 with value: 0.6051832608412951.\u001b[0m\n",
      "2021-11-23 14:29:00,226 INFO Training Started!\n",
      "2021-11-23 14:29:25,026 INFO Training Ended!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19474)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19470)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19464)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19468)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "study = tmlt.do_xgb_optuna_optimization(optuna_db_path=OUTPUT_PATH, opt_timeout=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now update the model with best params from study and then update the sklearn pipeline with new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params.update(study.best_trial.params)\n",
    "print(\"Final xgb_params:\", xgb_params)\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "tmlt.update_model(xgb_model)\n",
    "tmlt.spl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Use K-Fold Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19474)\u001b[0m \n",
      "[14:10:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19464)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:10:23,049 INFO fold: 1 log_loss : 0.6039021413773298\n",
      "2021-11-23 14:10:23,050 INFO fold: 1 roc_auc_score : 0.7272275950999355\n",
      "2021-11-23 14:10:23,051 INFO fold: 1 accuracy_score : 0.6925\n",
      "2021-11-23 14:10:23,051 INFO fold: 1 f1_score : 0.5758620689655173\n",
      "2021-11-23 14:10:23,052 INFO fold: 1 precision_score : 0.668\n",
      "2021-11-23 14:10:23,053 INFO fold: 1 recall_score : 0.5060606060606061\n",
      "2021-11-23 14:10:23,054 WARNING Trying to do Test Predictions but No Test Dataset Provided!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:10:27] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19471)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19471)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:10:49,192 INFO fold: 2 log_loss : 0.6287158783921041\n",
      "2021-11-23 14:10:49,193 INFO fold: 2 roc_auc_score : 0.694635718891038\n",
      "2021-11-23 14:10:49,194 INFO fold: 2 accuracy_score : 0.6825\n",
      "2021-11-23 14:10:49,195 INFO fold: 2 f1_score : 0.5876623376623378\n",
      "2021-11-23 14:10:49,196 INFO fold: 2 precision_score : 0.6328671328671329\n",
      "2021-11-23 14:10:49,197 INFO fold: 2 recall_score : 0.5484848484848485\n",
      "2021-11-23 14:10:49,198 WARNING Trying to do Test Predictions but No Test Dataset Provided!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:10:54] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:11:12,677 INFO fold: 3 log_loss : 0.65383338053427\n",
      "2021-11-23 14:11:12,678 INFO fold: 3 roc_auc_score : 0.6620309477756288\n",
      "2021-11-23 14:11:12,679 INFO fold: 3 accuracy_score : 0.6425\n",
      "2021-11-23 14:11:12,680 INFO fold: 3 f1_score : 0.5119453924914675\n",
      "2021-11-23 14:11:12,680 INFO fold: 3 precision_score : 0.5859375\n",
      "2021-11-23 14:11:12,681 INFO fold: 3 recall_score : 0.45454545454545453\n",
      "2021-11-23 14:11:12,682 WARNING Trying to do Test Predictions but No Test Dataset Provided!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:11:17] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19471)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19463)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19465)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:11:34,620 INFO fold: 4 log_loss : 0.6231675453111529\n",
      "2021-11-23 14:11:34,621 INFO fold: 4 roc_auc_score : 0.7080308427650268\n",
      "2021-11-23 14:11:34,621 INFO fold: 4 accuracy_score : 0.69125\n",
      "2021-11-23 14:11:34,622 INFO fold: 4 f1_score : 0.5957446808510638\n",
      "2021-11-23 14:11:34,623 INFO fold: 4 precision_score : 0.65\n",
      "2021-11-23 14:11:34,624 INFO fold: 4 recall_score : 0.5498489425981873\n",
      "2021-11-23 14:11:34,625 WARNING Trying to do Test Predictions but No Test Dataset Provided!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:11:39] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1634712680264/work/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19464)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19466)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19468)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19470)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19473)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 14:11:58,562 INFO fold: 5 log_loss : 0.6006386911938898\n",
      "2021-11-23 14:11:58,563 INFO fold: 5 roc_auc_score : 0.7226470152474571\n",
      "2021-11-23 14:11:58,564 INFO fold: 5 accuracy_score : 0.6775\n",
      "2021-11-23 14:11:58,565 INFO fold: 5 f1_score : 0.568561872909699\n",
      "2021-11-23 14:11:58,565 INFO fold: 5 precision_score : 0.6367041198501873\n",
      "2021-11-23 14:11:58,566 INFO fold: 5 recall_score : 0.513595166163142\n",
      "2021-11-23 14:11:58,567 WARNING Trying to do Test Predictions but No Test Dataset Provided!\n",
      "2021-11-23 14:11:58,568 INFO  Mean Metrics Results from all Folds are: {'log_loss': 0.6220515273617493, 'roc_auc_score': 0.7029144239558172, 'accuracy_score': 0.6772500000000001, 'f1_score': 0.5679552705760171, 'precision_score': 0.634701750543464, 'recall_score': 0.5145070035704478}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(apply_list_of_funcs pid=19464)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "# K-Fold fit and predict on test dataset\n",
    "xgb_model_mean_metrics_results, xgb_model_test_preds= tmlt.do_kfold_training(n_splits=5,\n",
    "                                                                            test_preds_metric=roc_auc_score)\n",
    "if xgb_model_test_preds is not None:\n",
    "    print(xgb_model_test_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take weighted average of both k-fold models predictions\n",
    "# final_preds = ((0.45 * sci_model_preds) + (0.55* xgb_model_test_preds)) / 2\n",
    "# print(final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(DIRECTORY_PATH + SAMPLE_SUB_FILE)\n",
    "# sub['target'] = final_preds\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev_env",
   "language": "python",
   "name": "nbdev_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
