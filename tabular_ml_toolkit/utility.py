# AUTOGENERATED! DO NOT EDIT! File to edit: utility.ipynb (unless otherwise specified).

__all__ = ['find_ideal_cpu_cores', 'check_has_n_jobs', 'needs_predict_proba', 'fetch_tabnet_params_for_problem_type',
           'fetch_xgb_params_for_problem_type', 'fetch_skl_params_for_problem_type', 'kfold_dict_mean', 'clip_splits']

# Cell
from .dataframeloader import *
from .preprocessor import *
from .logger import *
from .xgb_optuna_objective import *

# Cell
# hide

import pandas as pd
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score, precision_score, recall_score
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold

# for Optuna
import optuna

#for XGB
import xgboost

#for TabNet
from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor
from pytorch_tabnet.multitask import TabNetMultiTaskClassifier

# for finding n_jobs in all sklearn estimators
from sklearn.utils import all_estimators
import inspect

# Just to compare fit times
import time

# for os specific settings
import os

# Cell

#helper method to find ideal cpu cores
def find_ideal_cpu_cores():
    if os.cpu_count() > 2:
        ideal_cpu_cores = os.cpu_count()-1
        logger.info(f"{os.cpu_count()} cores found, model and data parallel processing could worked!")
    else:
        ideal_cpu_cores = None
        logger.info(f"{os.cpu_count()} cores found, model and data parallel processing may NOT worked!")
    return ideal_cpu_cores

#Helper method to find all sklearn estimators with support for parallelism aka n_jobs
def check_has_n_jobs():
    has_n_jobs = ['XGBRegressor', 'XGBClassifier']
    for est in all_estimators():
        s = inspect.signature(est[1])
        if 'n_jobs' in s.parameters:
            has_n_jobs.append(est[0])
    return has_n_jobs

# Cell

def needs_predict_proba(metric):
    if ("log_loss" in str(metric.__name__)) or ("roc_auc_score" in str(metric.__name__)):
        return True
    #     elif ("accuracy_score" in str(metric.__name__)):
    #         return False
    else:
        return False



def fetch_tabnet_params_for_problem_type(problem_type):
    if problem_type == "binary_classification":
        tabnet_model = TabNetClassifier
        eval_metric = "auc"
        oof_val_metric = roc_auc_score
        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = roc_auc_score
        #optuna optimization direction
        direction = "maximize"

    elif problem_type == "multi_label_classification":
        tabnet_model = TabNetClassifier
        eval_metric = "auc"
        oof_val_metric = log_loss
        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = accuracy_score
        direction = "maximize"


    elif problem_type == "multi_class_classification":
        tabnet_model = TabNetClassifier
        eval_metric = "logloss"
        oof_val_metric = log_loss
        #val_preds_metrics = [log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = accuracy_score
        direction = "maximize"

    elif problem_type == "regression":
        tabnet_model = TabNetRegression
        eval_metric = "rmse"
        oof_val_metric = mean_absolute_error
        #val_preds_metrics = [mean_absolute_error, mean_squared_error, r2_score]
        optuna_val_metric = mean_absolute_error
        direction = "minimize"
    else:
        raise NotImplementedError

    return tabnet_model, optuna_val_metric, eval_metric, direction, oof_val_metric

def fetch_xgb_params_for_problem_type(problem_type):
    if problem_type == "binary_classification":
        xgb_model = xgboost.XGBClassifier
        eval_metric = "auc"
        oof_val_metric = roc_auc_score
        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = roc_auc_score
        direction = "maximize"

    elif problem_type == "multi_label_classification":
        xgb_model = xgboost.XGBClassifier
        eval_metric = "auc"
        oof_val_metric = log_loss
        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = accuracy_score
        direction = "maximize"

    elif problem_type == "multi_class_classification":
        xgb_model = xgboost.XGBClassifier
        eval_metric = "mlogloss"
        oof_val_metric = log_loss
        #val_preds_metrics = [log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = accuracy_score
        direction = "maximize"

    elif problem_type == "regression":
        xgb_model = xgboost.XGBRegressor
        eval_metric = "rmse"
        oof_val_metric = mean_absolute_error
        #val_preds_metrics = [mean_absolute_error, mean_squared_error, r2_score]
        optuna_val_metric = mean_absolute_error
        direction = "minimize"

    else:
        raise NotImplementedError

    return xgb_model, optuna_val_metric, eval_metric, direction, oof_val_metric

def fetch_skl_params_for_problem_type(problem_type):
    if problem_type == "binary_classification":
        oof_val_metric = roc_auc_score
        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = roc_auc_score
        direction = "maximize"


    elif problem_type == "multi_label_classification":
        oof_val_metric = log_loss
        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = accuracy_score
        direction = "maximize"

    elif problem_type == "multi_class_classification":
        oof_val_metric = log_loss
        #val_preds_metrics = [log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score]
        optuna_val_metric = accuracy_score
        direction = "maximize"

    elif problem_type == "regression":
        oof_val_metric = mean_absolute_error
        #val_preds_metrics = [mean_absolute_error, mean_squared_error, r2_score]
        optuna_val_metric = mean_absolute_error
        direction = "minimize"
    else:
        raise NotImplementedError

    return optuna_val_metric, direction, oof_val_metric


def kfold_dict_mean(kfold_metrics_results):
    mean_metrics_results = {}
    for single_fold_metrics_results in kfold_metrics_results:
        for key in single_fold_metrics_results.keys():
            if key in mean_metrics_results:
                mean_metrics_results[key] += single_fold_metrics_results[key] / len(kfold_metrics_results)
            else:
                mean_metrics_results[key] = single_fold_metrics_results[key] / len(kfold_metrics_results)

    return mean_metrics_results

#utility functions which take care of startification problem in multi sclass imbalance
def clip_splits(train,val,temp):
    """
    Fixes the stratification problem of StratifiedKfold
    Finds outstanding target values in train, valid splits and removes indexes of
    those train and valid splits from consideration for further correct definition of
    X_train,X_val,y_train,y_val
    """
    tr_target_unique=temp.iloc[train].target.unique()
    val_target_unique=temp.iloc[val].target.unique()

    if len(tr_target_unique) > len(val_target_unique):
        outstanding_target_class = list(set(tr_target_unique)- set(val_target_unique))
        ix_to_remove_from_train = []

        for i in outstanding_target_class:
            ix_to_remove_from_train.extend(temp.iloc[train].
                                           reset_index(drop=True).
                                           index[temp.target.iloc[train] == i].tolist())
            train = list(train)

        for index in sorted(ix_to_remove_from_train, reverse=True):
            del train[index]

    elif len(tr_target_unique) < len(val_target_unique):

        outstanding_target_class = list(set(val_target_unique)- set(tr_target_unique))
        ix_to_remove_from_val = []

        for i in outstanding_target_class:
            ix_to_remove_from_val.extend(temp.iloc[val].reset_index(drop=True).
                                         index[temp.target.iloc[val] == i].tolist())
        val = list(val)

        for index in sorted(ix_to_remove_from_val, reverse=True):
            del val[index]

    return train, val