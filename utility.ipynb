{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331f568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef623126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3607f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21305/2363042074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabular_ml_toolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtabular_ml_toolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabular_ml_toolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxgb_optuna_objective\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/tabular_ml_toolkit/tabular_ml_toolkit/xgb_optuna_objective.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# for Optuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#for XGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from tabular_ml_toolkit.dataframeloader import *\n",
    "from tabular_ml_toolkit.preprocessor import *\n",
    "from tabular_ml_toolkit.logger import *\n",
    "from tabular_ml_toolkit.xgb_optuna_objective import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# hide\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "\n",
    "# for Optuna\n",
    "import optuna\n",
    "\n",
    "#for XGB\n",
    "import xgboost\n",
    "\n",
    "#for TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "# for finding n_jobs in all sklearn estimators\n",
    "from sklearn.utils import all_estimators\n",
    "import inspect\n",
    "\n",
    "# Just to compare fit times\n",
    "import time\n",
    "\n",
    "# for os specific settings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83226bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#helper method to find ideal cpu cores\n",
    "def find_ideal_cpu_cores():\n",
    "    if os.cpu_count() > 2:\n",
    "        ideal_cpu_cores = os.cpu_count()-1\n",
    "        logger.info(f\"{os.cpu_count()} cores found, model and data parallel processing should worked!\")\n",
    "    else:\n",
    "        ideal_cpu_cores = None\n",
    "        logger.info(f\"{os.cpu_count()} cores found, model and data parallel processing may NOT worked!\")\n",
    "    return ideal_cpu_cores\n",
    "\n",
    "#Helper method to find all sklearn estimators with support for parallelism aka n_jobs\n",
    "def check_has_n_jobs():\n",
    "    has_n_jobs = ['XGBRegressor', 'XGBClassifier']\n",
    "    for est in all_estimators():\n",
    "        s = inspect.signature(est[1])\n",
    "        if 'n_jobs' in s.parameters:\n",
    "            has_n_jobs.append(est[0])\n",
    "    return has_n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "\n",
    "# def use_modin():\n",
    "    \n",
    "#     #settings for modin\n",
    "#     import ray\n",
    "#     ray.init()\n",
    "#     import os\n",
    "#     os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "#     import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc153fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def fetch_tabnet_params_for_problem_type(problem_type):\n",
    "    if problem_type == \"binary_classification\":\n",
    "        tabnet_model = TabNetClassifier\n",
    "        direction = \"maximize\"\n",
    "        eval_metric = \"auc\"\n",
    "        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = roc_auc_score\n",
    "\n",
    "    elif problem_type == \"multi_label_classification\":\n",
    "        tabnet_model = TabNetClassifier\n",
    "        direction = \"maximize\"\n",
    "        eval_metric = \"auc\"\n",
    "        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = accuracy_score\n",
    "    \n",
    "    elif problem_type == \"multi_class_classification\":\n",
    "        tabnet_model = TabNetMultiTaskClassifier\n",
    "        direction = \"minimize\"\n",
    "        eval_metric = \"logloss\"\n",
    "        #val_preds_metrics = [log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = accuracy_score\n",
    "\n",
    "    elif problem_type == \"regression\":\n",
    "        tabnet_model = TabNetRegression\n",
    "        direction = \"minimize\"\n",
    "        eval_metric = \"rmse\"\n",
    "        #val_preds_metrics = [mean_absolute_error, mean_squared_error, r2_score]\n",
    "        val_preds_metrics = mean_absolute_error\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return tabnet_model, val_preds_metrics, eval_metric, direction\n",
    "\n",
    "def fetch_xgb_params_for_problem_type(problem_type):\n",
    "    if problem_type == \"binary_classification\":\n",
    "        xgb_model = xgboost.XGBClassifier\n",
    "        direction = \"maximize\"\n",
    "        eval_metric = \"auc\"\n",
    "        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = roc_auc_score\n",
    "\n",
    "    elif problem_type == \"multi_label_classification\":\n",
    "        xgb_model = xgboost.XGBClassifier\n",
    "        direction = \"maximize\"\n",
    "        eval_metric = \"auc\"\n",
    "        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = accuracy_score\n",
    "    \n",
    "    elif problem_type == \"multi_class_classification\":\n",
    "        xgb_model = xgboost.XGBClassifier\n",
    "        direction = \"minimize\"\n",
    "        eval_metric = \"mlogloss\"\n",
    "        #val_preds_metrics = [log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = accuracy_score\n",
    "\n",
    "    elif problem_type == \"regression\":\n",
    "        xgb_model = xgboost.XGBRegressor\n",
    "        direction = \"minimize\"\n",
    "        eval_metric = \"rmse\"\n",
    "        #val_preds_metrics = [mean_absolute_error, mean_squared_error, r2_score]\n",
    "        val_preds_metrics = mean_absolute_error\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return xgb_model, val_preds_metrics, eval_metric, direction\n",
    "\n",
    "def fetch_skl_params_for_problem_type(problem_type):\n",
    "    if problem_type == \"binary_classification\":\n",
    "        direction = \"maximize\"\n",
    "        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = roc_auc_score\n",
    "        \n",
    "\n",
    "    elif problem_type == \"multi_label_classification\":\n",
    "        direction = \"maximize\"\n",
    "        #val_preds_metrics = [roc_auc_score, log_loss, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = accuracy_score\n",
    "    \n",
    "    elif problem_type == \"multi_class_classification\":\n",
    "        direction = \"minimize\"\n",
    "        #val_preds_metrics = [log_loss, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score]\n",
    "        val_preds_metrics = accuracy_score\n",
    "\n",
    "    elif problem_type == \"regression\":\n",
    "        direction = \"minimize\"\n",
    "        #val_preds_metrics = [mean_absolute_error, mean_squared_error, r2_score]\n",
    "        val_preds_metrics = mean_absolute_error\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return val_preds_metrics, direction\n",
    "\n",
    "\n",
    "def kfold_dict_mean(kfold_metrics_results):\n",
    "    mean_metrics_results = {}\n",
    "    for single_fold_metrics_results in kfold_metrics_results:\n",
    "        for key in single_fold_metrics_results.keys():\n",
    "            if key in mean_metrics_results:\n",
    "                mean_metrics_results[key] += single_fold_metrics_results[key] / len(kfold_metrics_results)\n",
    "            else:\n",
    "                mean_metrics_results[key] = single_fold_metrics_results[key] / len(kfold_metrics_results)\n",
    "    \n",
    "    return mean_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper method for updating preprocessor in pipeline\n",
    "\n",
    "# # to create params value dict from grid_search object\n",
    "# def get_preprocessor_best_params_from_grid_search(grid_search_object:object):\n",
    "#     pp_best_params = {}\n",
    "#     for k in grid_search_object.best_params_:\n",
    "#         #print(k)\n",
    "#         if 'preprocessor' in k:\n",
    "#             key = k.split('__')[1] + \"__\" + k.split('__')[2] \n",
    "#             pp_best_params[key] = grid_search_object.best_params_[k]\n",
    "#     return pp_best_params\n",
    "\n",
    "# # helper method for update_model\n",
    "# def get_model_best_params_from_grid_search(grid_search_object:object):\n",
    "#     model_best_params = {}\n",
    "#     for k in grid_search_object.best_params_:\n",
    "#         #print(k)\n",
    "#         if 'model' in k:\n",
    "#             key = k.split('__')[1]\n",
    "#             model_best_params[key] = grid_search_object.best_params_[k]\n",
    "#     return model_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# run the script to build \n",
    "\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a55a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
